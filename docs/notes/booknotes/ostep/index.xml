<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Operating Systems: Three Easy Pieces | Yuyao Wang&#39;s Homepage</title>
    <link>https://kristoff-starling.github.io/notes/booknotes/ostep/</link>
      <atom:link href="https://kristoff-starling.github.io/notes/booknotes/ostep/index.xml" rel="self" type="application/rss+xml" />
    <description>Operating Systems: Three Easy Pieces</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 13 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Operating Systems: Three Easy Pieces</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/</link>
    </image>
    
    <item>
      <title>Chapter 02: Introduction to Operating Systems</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch02/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch02/</guid>
      <description>&lt;p&gt;操作系统：为上层多个应用程序的正常运行提供支持。操作系统实现这一目的的重要方法是虚拟化：将硬件资源抽象成简单易用的虚拟资源。为了方便上层应用程序使用这些资源，操作系统会提供数百个系统调用。从这个角度来看，操作系统也很像一个标准库。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;21-virtualizing-the-cpu&#34;&gt;2.1 Virtualizing The CPU&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2002%20-%20intro/cpu.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cpu.c&lt;/a&gt; 的作用是每过一秒钟输出一次字符串。&lt;code&gt;Spin(1)&lt;/code&gt; 表示等待一秒，其实现在 
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2002%20-%20intro/common.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;common.h&lt;/a&gt; 中，其中 GetTime() 是对系统调用 gettimeofday() 的进一步封装，返回系统启动以来的秒数。Spin() 会不断调用 GetTime()，直到间隔时间达到输入值 howlong。&lt;/p&gt;
&lt;p&gt;键入命令 &lt;code&gt;./cpu A &amp;amp; ; ./cpu B &amp;amp; ; ./cpu C &amp;amp; ; ./cpu D &amp;amp;&lt;/code&gt;，可以看到A,B,C,D 交替输出，仿佛各自独占了 CPU。操作系统通过虚拟化 CPU 的方式，给上层应用程序一种系统中有很多很多个 CPU 的假象。至于多个进程谁在真正的物理 CPU 上运行，这取决于操作系统的调度策略。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;为什么程序会不停输出，按 &lt;code&gt;Ctrl+C&lt;/code&gt; 也无法停止？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上文中，用分号隔开各个命令表示这些命令都要执行。它和 &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; 的区别在于：&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; 要求第一个命令执行成功才会执行第二个命令，而 &lt;code&gt;;&lt;/code&gt; 不论第一个命令是否成功都会执行第二个命令 (可以用 &lt;code&gt;gcc notexist.c ; ls&lt;/code&gt; 和 &lt;code&gt;gcc notexist.c &amp;amp;&amp;amp; ls&lt;/code&gt; 做实验验证)。&lt;/p&gt;
&lt;p&gt;在命令最后加上 &lt;code&gt;&amp;amp;&lt;/code&gt; 表示将这个进程放到后台执行， &lt;code&gt;Ctrl+C&lt;/code&gt; 的意义是中断所有正在运行的前台进程，因此该组合键无法终止后台运行的进程。如果某个命令所需要的执行时间很长，可以用 &lt;code&gt;&amp;amp;&lt;/code&gt; 将其放在后台执行，从而终端界面仍然可以继续操作。在使用 &lt;code&gt;&amp;amp;&lt;/code&gt; 时，bash 会提示分配给该任务的进程号。如果想要结束后台任务，可以使用命令 &lt;code&gt;kill pid&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;这里我们使用 &lt;code&gt;&amp;amp;&lt;/code&gt; 的原因在于：用 &lt;code&gt;;&lt;/code&gt; 分隔的若干命令总是会依次执行，即第一个执行结束才会执行第二个。而  cpu 程序中有一个死循环，为了观测“并发”，我们必须让四个任务同时运行起来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;22-virtualizing-memory&#34;&gt;2.2 Virtualizing Memory&lt;/h2&gt;
&lt;p&gt;物理内存本身没有任何特殊之处，就是一个大数组，每个位置有一个物理地址。&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2002%20-%20intro/mem.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mem.c&lt;/a&gt; 会调用 malloc() 分配一个地址，并不断向该地址写入内容。如果我们使用 &lt;code&gt;./mem 1 &amp;amp; ; ./mem 1 &amp;amp;&lt;/code&gt;  命令，我们会发现两个进程分配的地址是一样的，然而两个进程都在完好地运行。这是因为每个进程都有自己的虚拟地址空间，输出的地址是进程的虚拟地址，不同进程的相同虚拟地址会指向不同的物理地址。操作系统负责虚拟化内存，保证每个进程只能访问自己的地址空间。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;为什么我使用上述命令无法复现，两个地址不一样？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linux 默认使用了地址空间随机化 (Address Space Layout Randomization, ASLR) 技术。ASLR 是一种针对缓冲区溢出攻击的安全保护技术，通过对堆、栈、共享库映射等布局的随机化增加攻击者预测目的地址的难度，关于利用固定地址进行攻击的方法可以见
&lt;a href=&#34;https://kristoff-starling.github.io/2022/02/28/Stack-smashing%20Attack%20-%20An%20Introduction/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这篇文章&lt;/a&gt;。在 &lt;code&gt;/proc/sys/kernel/randomize_va_space&lt;/code&gt; 文件中，我们可以查看当前 ASLR 是否打开：0 表示关闭，1 表示对于部分函数打开，2 表示完全打开。&lt;/p&gt;
&lt;p&gt;为了暂时关闭 ASLR 以复现上述情境，我们可以使用命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;setarch `uname -m` -R /bin/zsh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;uname -m&lt;/code&gt; 命令会输出机器架构，该参数可以省略；&lt;code&gt;-R&lt;/code&gt; 参数 (对应长参数 &lt;code&gt;--addr-no-randomize&lt;/code&gt;) 表示关闭 ASLR。执行该命令会暂时打开一个新的 shell (默认 &lt;code&gt;/bin/sh&lt;/code&gt;，可以通过最后一个参数指定其他 shell)，该 shell 和其子进程会在关闭 ASLR 的情况下执行命令 (退出该 shell 后，一切恢复)。&lt;/p&gt;
&lt;p&gt;另外一种方法是使用命令&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sysctl -w kernel.randomize_va_space=0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;机器重启后该改变会消失。如果想要永久性的关闭 ASLR，可以将 &lt;code&gt;kernel.randomize_va_space=0&lt;/code&gt; 写到 &lt;code&gt;/etc/sysctl.conf&lt;/code&gt; 中。&lt;/p&gt;
&lt;p&gt;如果使用 GDB 调试，可以通过命令 &lt;code&gt;set disable-randomization off&lt;/code&gt; 关闭 ASLR。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;23-concurrency&#34;&gt;2.3 Concurrency&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2002%20-%20intro/threads.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;threads.c&lt;/a&gt; 创建了两个线程，创建线程调用的函数是 Pthread_create()，它在 &lt;code&gt;common_threads.h&lt;/code&gt; 中定义，是对 POSIX 线程库的简单封装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#define Pthread_create(thread, attr, start_routine, arg) assert(pthread_create(thread, attr, start_routine, arg) == 0);
#define Pthread_join(thread, value_ptr)                  assert(pthread_join(thread, value_ptr) == 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;让两个线程各执行 N 次 +1 操作，可以观测到结果不是 2N。这主要是因为核心语句 &lt;code&gt;counter++&lt;/code&gt; 会被编译成三条汇编语句：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;mov &amp;amp;counter, register
add $1, register
mov register, &amp;amp;counter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于原子性的丧失，可能出现 race condition。&lt;/p&gt;
&lt;h2 id=&#34;24-persistence&#34;&gt;2.4 Persistence&lt;/h2&gt;
&lt;p&gt;数据的持久性是一个重要的话题。内存中存储的数据是不稳定的——机器一旦断点，DRAM 中的信息就会丢失。我们需要硬盘来存储长期数据，硬盘在现代系统中被看作 I/O 设备的一部分，管理硬盘信息的软件称为文件系统。&lt;/p&gt;
&lt;p&gt;操作系统对于硬盘的抽象和 CPU，内存不同。我们针对 CPU 抽象出了进程，对于内存抽象出了虚拟地址空间，其目的都是为了让某一个程序“独占”整个计算机资源。而硬盘上的数据理应让各个程序共享，比如编辑器创建了一个文件，然后编译器负责编译它，接着加载器负责加载、运行这个程序。操作系统通过系统调用来抽象硬盘：将复杂的硬件读写操作封装成简单的接口。&lt;/p&gt;
&lt;p&gt;在 
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2002%20-%20intro/io.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;io.c&lt;/a&gt; 中我们使用了 open() 和 write() 系统调用来创建，写入文件。这些系统调用底层的实现非常复杂 (比如为了性能的提升，读写都有缓冲区)，但上层使用这些 API 非常简便。OS 在这里充当了标准库的角色。&lt;/p&gt;
&lt;h2 id=&#34;25-design-goals&#34;&gt;2.5 Design Goals&lt;/h2&gt;
&lt;p&gt;操作系统应当努力追求的目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高性能；&lt;/li&gt;
&lt;li&gt;保护/隔离：应用程序之间不能互相“伤害“；&lt;/li&gt;
&lt;li&gt;可靠：操作系统一旦崩溃，在其上运行的所有应用程序都无法使用，因此操作系统必须是非常可靠的软件。&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 04: The Abstraction: The Process</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch04/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch04/</guid>
      <description>&lt;p&gt;进程就是正在运行的程序。程序本身只是存储在硬盘上的指令和数据的集合，只有 OS 将其加载到内存中运行起来时程序才会发挥作用。&lt;/p&gt;
&lt;p&gt;通常操作系统上会运行多个程序 - 比 CPU 核个数更多的程序。但每个程序并不需要关心 CPU 当前是否可用，这是因为 OS 为进程虚拟化了 CPU。OS 的基本操作是运行一个进程，然后暂停它去运行一会儿另外一个进程，这是分是运行的基本思想。&lt;!-- more --&gt;&lt;/p&gt;
&lt;p&gt;为了实现 CPU 的虚拟化，OS 既需要底层的机制，也需要上层的智慧。所谓&lt;strong&gt;机制 (mechanism)&lt;/strong&gt; 指的是实现某个功能所需要的一些底层的方法或协议。比如操作系统为了实现进程的切换，必须要有上下文切换的机制。分时运行也是一种机制，被所有的现代操作系统采用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Time Sharing 和 Space Sharing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Time Sharing 是 OS 共享资源的一种基本方式，即每个人用一会儿，然后把资源交给下一个人。Time sharing 不仅仅用在多个进程/线程共享 CPU 上，共享的资源都可以使用这种机制，比如网络连接。&lt;/p&gt;
&lt;p&gt;与 time sharing 互为补充的是 space sharing。磁盘就是一种 space sharing 的模型。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在机制之上是策略。&lt;strong&gt;策略 (policy)&lt;/strong&gt; 是操作系统做出决定的一些算法，比如如何调度各个进程就需要一个 scheduling policy，这里有多种可能性，比如根据优先级，根据历史运行时长等等 (如果说机制是 OS 必须的东西，那么策略影响的其实只是效率)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;区分机制和策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们可以这样区分机制和策略：机制回答的是一个 how question，比如“如何实现上下文切换？” 而策略回答的是一个 which question，比如“下一个用 CPU 的应该是哪个进程？”&lt;/p&gt;
&lt;p&gt;区分好机制和策略，我们在修改策略时就不必关注底层的机制，更利于模块化编程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;41-the-abstraction-a-process&#34;&gt;4.1 The Abstraction: A Process&lt;/h2&gt;
&lt;p&gt;OS 为正在运行的程序提供的抽象是进程。进程中包括的内容当然是一个正在运行的程序所拥有的机器状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory：每一个程序都有自己可以读写的内存区域，称为地址空间 (address space)。&lt;/li&gt;
&lt;li&gt;Regitsters：进程中应该有各个寄存器的信息，尤其是几个比较重要的寄存器：PC，stack pointer，frame pointer 等。&lt;/li&gt;
&lt;li&gt;I/O Information：程序要和可持久化存储设备打交道，因此进程会存储一些 I/O 相关信息，比如当前打开了哪些文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;42-process-api&#34;&gt;4.2 Process API&lt;/h2&gt;
&lt;p&gt;从抽象模型的角度，我们可以提出如下几类进程相关的 API：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create：创建一个新的进程。&lt;/li&gt;
&lt;li&gt;destroy：虽然一个运行的程序结束时会自动退出，但我们仍然应该有 API 可以强制杀死一个进程。&lt;/li&gt;
&lt;li&gt;wait：等待一个进程结束。&lt;/li&gt;
&lt;li&gt;Miscellaneous Control：比如暂时挂起一个进程，或让一个进程继续运行。&lt;/li&gt;
&lt;li&gt;Status：访问一个进程的信息，比如总运行时长，现在处于什么状态 (running, suspended etc.) 等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;43-process-creation-a-little-more-detail&#34;&gt;4.3 Process Creation: A Little More Detail&lt;/h2&gt;
&lt;p&gt;创建一个进程 (让一个程序跑起来) 通常要经过以下步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;将程序的代码和静态数据 (比如初始化过的全局变量) 加载到内存中。程序本身是以某种可执行文件的格式存放在硬盘上的 (比如 Linux 中默认使用 ELF)，文件会告诉 OS 应该把哪些代码加载到内存的哪些位置。&lt;/p&gt;
&lt;p&gt;早期的操作系统会 eagerly 地完成加载这个动作。但现代操作系统通常使用延迟加载：当程序真正要用到某一部分的代码/数据时再将数据从交换分区复制进来。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为该程序准备一个 run-time stack。以 C 程序为例，程序通常在栈上存储局部变量，此外传给 main() 的参数也保存在栈上。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OS 准备一些空间用作该程序的堆区。堆区负责为程序中的动态内存申请提供支持，即 malloc()/free()。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;做一些和 I/O 相关的准备工作，比如 UNIX 系统中默认为程序打开 stdin, stdout, stderr 三个文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;让 PC 跳转到该程序的入口地址，开始运行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;44-process-state&#34;&gt;4.4 Process State&lt;/h2&gt;
&lt;p&gt;进程的状态通常包括以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running：进程正在 CPU 上运行。&lt;/li&gt;
&lt;li&gt;Ready：程序可以运行，但当前不在 CPU 上。&lt;/li&gt;
&lt;li&gt;Blocked：程序因为做了某种操作使得当前还不能运行 (比如执行 I/O，正在等待数据返回)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面的图很好地反映了三种状态的切换：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
Running --&amp;gt; Ready: Descheduled
Ready --&amp;gt; Running: Scheduled
Running --&amp;gt; Blocked: I/O Initiate
Blocked --&amp;gt; Ready: I/O Done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OS 的调度器有很多事情要决定：比如 process 1 因为 I/O 操作被阻塞时，是否需要将其他进程 process 2切换上来？如果 process 2 运行时 I/O 操作完成，process 1 从 blocked 变成 ready，那么 OS 是将 process 1 立刻请回来还是先做 process 2？……&lt;/p&gt;
&lt;h2 id=&#34;45-data-structure&#34;&gt;4.5 Data Structure&lt;/h2&gt;
&lt;p&gt;OS 中有很多维护信息的数据结构，比如对于进程我们应该有一个 process list，链表中的每个节点存储一个进程的相关信息，通常被称为 process control block (PCB)，再比如我们应该有上下文结构体用于 context switching。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// the registers xv6 will save and restore
// to stop and subsequently restart a process
struct context {
  int eip;
  int esp;
  int ebx;
  int ecx;
  int edx;
  int esi;
  int edi;
  int ebp;
};
// the different states a process can be in
enum proc_state { UNUSED, EMBRYO, SLEEPING,
RUNNABLE, RUNNING, ZOMBIE };
// the information xv6 tracks about each process
// including its register context and state
struct proc {
  char *mem;                  // Start of process memory
  uint sz;                    // Size of process memory
  char *kstack;               // Bottom of kernel stack for this process
  enum proc_state state;      // Process state
  int pid;                    // Process ID
  struct proc *parent;        // Parent process
  void *chan;                 // If non-zero, sleeping on chan
  int killed;                 // If non-zero, have been killed
  struct file *ofile[NOFILE]; // Open files
  struct inode *cwd;          // Current directory
  struct context context;     // Switch here to run process
  struct trapframe *tf;       // Trap frame for the current interrupt
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这是 xv6-x86 中的上下文结构体和 PCB。可以看到它存储了比之前提到的更多的一些信息，比如每个进程的内核栈地址，父进程，如果睡眠睡在了哪个地址上，kernel trap 的页面地址等。&lt;/p&gt;
&lt;p&gt;进程的状态也比之间提到的 ready, running, blocked 要多。一些很有用的状态包括 zombie，它表示一个程序已经运行结束，但相关的信息还没有被清空。这种状态可以让调用 wait() 的父进程去检查子进程的返回值是否符合要求。&lt;/p&gt;
&lt;h2 id=&#34;46-summary&#34;&gt;4.6 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 05: Interlude: Process API</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch05/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch05/</guid>
      <description>&lt;h2 id=&#34;51-the-fork-system-call&#34;&gt;5.1 The &lt;code&gt;fork()&lt;/code&gt; System Call&lt;/h2&gt;
&lt;p&gt;fork() 系统调用会创建一个和当前进程完全相同的子进程，这两个进程除了 fork() 的返回值不同，其他完全相同。fork() 在父进程中返回子进程的进程号，在子进程中返回 0，这可以用于区分两个进程。&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2005%20-%20cpu_api/p1.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p1.c&lt;/a&gt; 展示了一个使用 fork() 的例子，其中 getpid() 函数可以获得当前进程的进程号。注意这个程序的运行结果是 non-deterministic 的：父进程和子进程谁会先输出，这取决于 OS 的调度器。现代操作系统的调度策略极其复杂，可以看
&lt;a href=&#34;https://linux.cn/article-7325-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这篇文章&lt;/a&gt; 有一个大概的了解。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;52-the-wait-system-call&#34;&gt;5.2 The &lt;code&gt;wait()&lt;/code&gt; System Call&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2005%20-%20cpu_api/p2.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p2.c&lt;/a&gt; 展示了一个使用 wait() 的例子，wait() 提供一种同步机制，保证了父进程在子进程结束之后再执行。如果父进程先被调度器选中，那么它执行 wait() 会被阻塞，直到子进程结束；如果子进程先被调度器选中，那么父进程执行 wait() 时看到子进程已经退出则会立刻返回。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Zombie Process&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个子进程如果已经退出但还没有被它的父进程 wait()，那它就是一个僵尸进程。内核保存了僵尸进程的一份 minimal 的信息，包括进程号、退出状态等，这样以后如果父进程 wait() 了，父进程就可以获取子进程的信息。&lt;/p&gt;
&lt;p&gt;被 wait() 过的僵尸进程会被从 process table 中移除。一个 zombie process 如果不被 wait，就会一直待在 process table 中，一旦内核的 process table 满了，就不能再创建新的进程。因此父进程应该及时清理自己的僵尸子进程。如果父进程退出了，那么它的僵尸子进程会被 init 进程 (或其他某个指定的进程) wait 掉。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;53-finally-the-exec-system-call&#34;&gt;5.3 Finally, The &lt;code&gt;exec()&lt;/code&gt; System Call&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2005%20-%20cpu_api/p3.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p3.c&lt;/a&gt; 展示了一个使用 exec() 的例子。exec() 传入一个可执行程序的文件名，它会将该文件的代码和数据加载到内存中覆盖当前的代码和数据，重新初始化堆区和栈，并跳转到新程序的入口开始执行。exec() 不创建新的进程，它只是重启当前的状态机。exec() 如果执行成功就不会返回。&lt;/p&gt;
&lt;h2 id=&#34;54-why-motivating-the-api&#34;&gt;5.4 Why? Motivating the API&lt;/h2&gt;
&lt;p&gt;将 fork() 和 exec() 分开的设计似乎有一些反人类：我们创建一个新进程运行新程序需要两个系统调用。这样设计的真正目的是让我们有机会在 fork() 和 exec() 之间做一些有意思的事情。&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2005%20-%20cpu_api/p4.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p4.c&lt;/a&gt; 展示了一个重定向的例子。我们在 fork() 之后 exec() 之前关闭 stdout，再打开一个新的文件，这样新打开的文件的描述符就是 1 (stdout)，这是再 exec()，我们就实现了将标准输出重定向到文件。除了重定向，UNIX 的管道机制也是通过在 fork() 和 exec() 中间操作实现的。&lt;/p&gt;
&lt;h2 id=&#34;55-process-control-and-users&#34;&gt;5.5 Process Control And Users&lt;/h2&gt;
&lt;p&gt;除了 fork(), wait() 和 exec()，UNIX 系统中还有很多其他控制进程的 API，比如 kill() 用于给进程发送信号。信号机制可以向进程通知外部事件的发生，常见的信号有 SIGINT (interrupt，通常用于结束程序)，SIGTSTP (暂时挂起程序)，SIGSEGV (段错误) 等。&lt;/p&gt;
&lt;p&gt;既然信号的能力这么强，那么肯定不能让任意用户随便给别人的进程发送 SIGINT。你需要通过密码登录获取管理员权限，或者你本身是 root 用户，才能执行这些系统资源相关的操作。在用户态，你只能管理你自己的进程。&lt;/p&gt;
&lt;h2 id=&#34;56-useful-tools&#34;&gt;5.6 Useful Tools&lt;/h2&gt;
&lt;p&gt;ps 命令可以查看当前活跃的进程。top 命令可以查看所有进程的详细信息。&lt;/p&gt;
&lt;h2 id=&#34;57-summary&#34;&gt;5.7 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 13: The Abstraction: The Address Space</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch13/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch13/</guid>
      <description>&lt;h2 id=&#34;131-early-systems&#34;&gt;13.1 Early Systems&lt;/h2&gt;
&lt;p&gt;早期的系统非常简单，操作系统没有为程序提供任何的抽象，整个计算机中只有一个物理地址空间，操作系统代码以及操作系统为程序提供的一些库函数存放在内存中，此外还有一个运行的程序，它可以使用剩下的内存。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;132-multiprogramming-and-time-sharing&#34;&gt;13.2 Multiprogramming and Time Sharing&lt;/h2&gt;
&lt;p&gt;随着时代发展，人们对计算机的需求激增，于是一个计算机上需要运行多个程序，由操作系统负责调度程序的运行，time sharing 的概念也被提了出来。一个最简单的想法是：先将第一个程序加载到内存中，让它拥有全部的地址空间，它的时间片用完之后，将整个内存状态保存到磁盘上，然后加载下一个程序。这个做法因为效率太低而被淘汰。&lt;/p&gt;
&lt;p&gt;人们希望在进程切换的时候，被切的程序的状态可以仍然保留在内存中，等待下次被切回来的时候继续使用。这样 OS 调度就会更有效率。将各个进程的状态同时保存在内存中就必然涉及到保护问题，我们不希望一个进程可以干扰另一个进程的状态。&lt;/p&gt;
&lt;h2 id=&#34;133-the-address-space&#34;&gt;13.3 The Address Space&lt;/h2&gt;
&lt;p&gt;地址空间是操作系统提供给运行中的程序的内存的模样。运行中的程序看到的内存包括三个部分：代码区，栈区和堆区。代码区的内容是固定不变的，通常放在地址空间底部。栈区和堆区的大小都是会动态变化的，因此他们通常一个在底部一个在顶部，两者往相反的方向生长。&lt;/p&gt;
&lt;p&gt;需要注意的是，虽然在进程眼中地址空间是这种简洁的结构，但这是操作系统提供给进程的 illusion，在实际的物理内存中，一个进程的地址空间可能会被存放在任意位置。操作系统通过虚拟化内存的方式来为进程提供这种 illusion：进程眼中的地址都是虚拟地址，操作系统/MMU硬件负责将虚拟地址映射到正确的物理地址。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The Principle Of Isolation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;隔离是构建一个可靠系统的必要条件。在有效隔离的情况下，一个程序的崩溃不会影响别的程序的运行。操作系统通过虚拟内存等各种手段保证隔离，一些现代的内核通过将内核各个模块拆开的方式做到了更强的隔离，这种微内核设计比传统的宏内核更安全。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;134-goals&#34;&gt;13.4 Goals&lt;/h2&gt;
&lt;p&gt;虚拟内存的目标如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;透明 (transparency)：程序不应该感受到它们获得的内存是虚拟的，每个程序都应该觉得自己在独占整个物理内存。&lt;/li&gt;
&lt;li&gt;效率 (efficiency)：一方面虚拟内存机制不能太慢，另一方面存储映射表不能耗费太多空间。为了达到这一点，一方面操作系统需要设计精巧的数据结构 (页表)，一方面也需要硬件的帮助 (比如 TLB 作为页表的 cache)。&lt;/li&gt;
&lt;li&gt;保护 (protection)：任何程序的行为都不能影响别的程序。不同的程序之间应当形成隔离。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Every Address You See Is Virtual&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们程序员在用户态能看到的所有地址都是虚拟地址。不论是代码段的地址，malloc() 获得的堆区地址，还是任意一个指针的值，都是虚拟地址。只有操作系统 (和硬件) 知道真正的物理地址。&lt;/p&gt;
&lt;p&gt;下面的程序 &lt;code&gt;va.c&lt;/code&gt; 打印了一个代码地址、一个堆区地址和一个栈上的地址。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
int main(int argc, char *argv[]) {
    printf(&amp;quot;location of code : %p\n&amp;quot;, (void *) main);
    printf(&amp;quot;location of heap : %p\n&amp;quot;, (void *) malloc(1));
    int x = 3;
    printf(&amp;quot;location of stack : %p\n&amp;quot;, (void *) &amp;amp;x);
    return x;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 Linux 下，该程序输出结果为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;location of code : 0x55aa61491189
location of heap : 0x55aa62a0b6b0
location of stack : 0x7ffd13ff4374
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到虚拟地址空间中，代码段在底部，堆区在代码段之上，栈在地址空间的另一头。不过这整个结构都是操作系统为我们虚拟出来的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;135-summary&#34;&gt;13.5 Summary&lt;/h2&gt;
&lt;p&gt;虚拟内存系统的任务是为每个进程提供一个大的，私有的地址空间，让程序存储其所有的代码和相关数据。操作系统和相关的硬件会在背后负责虚拟地址到物理地址的翻译。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 14: Interlude: Memory API</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch14/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch14/</guid>
      <description>&lt;h2 id=&#34;141-types-of-memory&#34;&gt;14.1 Types of Memory&lt;/h2&gt;
&lt;p&gt;在一个 C 程序中，程序使用的内存有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;栈内存，这部分内存的分配和回收由编译器隐式地完成，因此也被称为“自动内存”。在 C 程序中声明使用栈内存是非常简单的，比如 &lt;!-- more --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void func() {
    int x; // declares an integer on the stack
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编译器会自动在栈上为变量 x 分配空间，且函数 func() 结束时编译器会自动回收这部分内存。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;堆内存：如果我们想要一些长生存周期 (不只存活在函数内部) 的内存，我们就要使用堆内存。堆内存是由程序员在程序中显式申请的，分配和释放都由程序员负责。一个例子如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void func() {
    int *x = (int *)malloc(sizeof(int));
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;malloc() 函数负责在堆区申请了一个 int 大小的内存，而这个内存的地址被保存在了指针变量 x 中，这个地址是存储在栈上的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;142-the-malloc-call&#34;&gt;14.2 The &lt;code&gt;malloc()&lt;/code&gt; Call&lt;/h2&gt;
&lt;p&gt;malloc() 的声明如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdlib.h&amp;gt;
void *malloc(size_t size);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;传入需要分配的大小，malloc() 要么返回 NULL 表示分配失败，要么返回一个地址表示分配区域的起始位置。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;小心 sizeof() 的行为！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面两段代码的输出行为是不同的！&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int x[10];
printf(&amp;quot;%d\n&amp;quot;, sizeof(x));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里 sizeof() 会返回整个数组的大小 40。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int *x = malloc(10 * sizeof(int));
printf(&amp;quot;%d\n&amp;quot;, sizeof(x));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里 sizeof() 会认为你只是想知道 int 指针的大小，因此会返回 4 (32/64bit machine)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;143-the-free-call&#34;&gt;14.3 The &lt;code&gt;free()&lt;/code&gt; Call&lt;/h2&gt;
&lt;p&gt;free() API 的参数只有一个，只需要给他传某个 malloc() 返回的首地址指针即可。分配区域的大小是由内存分配库自己记录的。&lt;/p&gt;
&lt;h2 id=&#34;144-common-errors&#34;&gt;14.4 Common Errors&lt;/h2&gt;
&lt;p&gt;程序员自己管理 malloc() 和 free() 总是会出现各种细微的错误，因此很多现代的编程语言支持自动内存分配，即某些场景下即使程序员只管分配不管回收，垃圾收集器 (garbage collector) 也会帮你把回收的脏活干完。&lt;/p&gt;
&lt;h3 id=&#34;forgetting-to-allocate-memory&#34;&gt;Forgetting To Allocate Memory&lt;/h3&gt;
&lt;p&gt;错误的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;char *src = &amp;quot;Hello&amp;quot;;
char *dst;
strcpy(dst, src);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正确的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;char *src = &amp;quot;Hello&amp;quot;;
char *dst = (char *)malloc(strlen(src) + 1); // 小心！别忘了+1,&#39;\0&#39;也要被复制过来
strcpy(dst, src);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;事实上 glibc 还提供了 strdup() 函数，我们只需要指定需要复制的函数，它就会自动帮我们 malloc() 空间，复制字符串，并返回指向复制字符串的指针。&lt;/p&gt;
&lt;h3 id=&#34;not-allocating-enough-memory&#34;&gt;Not Allocating Enough Memory&lt;/h3&gt;
&lt;p&gt;分配的内存不够不一定会暴露问题，但一旦有别的变量被覆盖就会导致严重的后果。&lt;/p&gt;
&lt;h3 id=&#34;forgetting-to-initialize-allocated-memory&#34;&gt;Forgetting to Initialize Allocated Memory&lt;/h3&gt;
&lt;p&gt;对 malloc() 得到的内存中的初始值作出任何假设都属于 undefined behavior。&lt;/p&gt;
&lt;h3 id=&#34;forgetting-to-free-memory&#34;&gt;Forgetting to Free Memory&lt;/h3&gt;
&lt;p&gt;内存泄漏是一种常见的错误。如果程序员总是忘了释放内存，那么长时间运行的软件就有可能将堆区消耗殆尽，最终只能重启机器。需要注意的是，即使你使用的是带有 garbage collector 的语言，你仍然可能无法避免这种错误：如果你还有任何一个指针指向某段内存，即使你将来不再用这段内存，garbage collector 也不会将其回收。&lt;/p&gt;
&lt;p&gt;在某些情况下不使用 free() 可能是无害的，比如你写一个短生命周期的程序 (OJ程序)，那么进程退出的时候操作系统会把该进程使用的所有资源释放。但只分配不释放是很坏的编程习惯。&lt;/p&gt;
&lt;h3 id=&#34;freeing-memory-before-you-are-done-with-it&#34;&gt;Freeing Memory Before You Are Done With It&lt;/h3&gt;
&lt;p&gt;在释放了一段内存后仍然使用其中的内容是危险的，这种指针被称为 dangling pointer。use-after-free 属于 undefined behavior，可能导致严重后果：比如如果内存分配器将这段内存又分配给了别人，就会出现两人同时使用一段内存的情况。&lt;/p&gt;
&lt;h3 id=&#34;freeing-memory-repeatly&#34;&gt;Freeing Memory Repeatly&lt;/h3&gt;
&lt;p&gt;double free 也是 undefined behavior，可能导致内存分配器崩溃。&lt;/p&gt;
&lt;h3 id=&#34;calling-free-incorrectly&#34;&gt;Calling &lt;code&gt;free()&lt;/code&gt; Incorrectly&lt;/h3&gt;
&lt;p&gt;给 free() 传递错误的指针 (不是之前某个 malloc() 的返回值) 也可能造成严重的后果。&lt;/p&gt;
&lt;h2 id=&#34;145-underlying-os-support&#34;&gt;14.5 Underlying OS Support&lt;/h2&gt;
&lt;p&gt;操作系统提供 brk() 和 sbrk() 两个系统调用，用于修改进程的 break：break 指向进程堆区的顶部。需要注意的是 malloc() 和 free() 这两个库函数会使用 OS 的系统调用，在程序员层面我们不要跨级去使用系统调用，否则可能导致意想不到的结果，在用户层面我们只要使用库函数即可。&lt;/p&gt;
&lt;p&gt;另外一个可以获得内存的系统调用是 mmap()，传入正确的参数后，mmap() 可以返回一段匿名的内存区域，这个区域并不属于任何一个文件，而是磁盘上交换空间 (swap space) 的一部分。对于应用程序来说，这样一段内存可以被当作堆区内存使用。&lt;/p&gt;
&lt;h2 id=&#34;146-other-calls&#34;&gt;14.6 Other Calls&lt;/h2&gt;
&lt;p&gt;一些其他有用的库函数包括但不限于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;calloc()：先 malloc() 再将分配的内存清零。&lt;/li&gt;
&lt;li&gt;realloc()：当你调用 malloc() 分配了一段内存，但发现大小不足时，可以调用 realloc()，它会分配一段更大的内存，将旧内存中的内容复制到新内存区域中，并返回新内存的指针。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;147-summary&#34;&gt;14.7 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 17: Free Space Management</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch17/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch17/</guid>
      <description>&lt;p&gt;动态大小的内存分配是比较困难的：随着反复的分配和释放，空闲内存会被切分成很多碎片，此时即使空闲空间的总量大于某个分配需求，分配也可能因为剩余空间过于碎片化而失败。因此，我们在管理空闲空间时要兼顾效率、空闲空间连续性、内存消耗等多方面因素。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;171-assumptions&#34;&gt;17.1 Assumptions&lt;/h2&gt;
&lt;p&gt;这里我们假设内存申请和释放的接口和 C 库中的 malloc()/free() 相同：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *malloc(size_t size);
void free(void *ptr);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：释放空间时，调用者只传入起始地址，不传入空间的 size，因此我们在分配时就要想办法记录每块分配出去的空间的大小。&lt;/p&gt;
&lt;p&gt;分配空间的过程中严禁出现 double-alloc 的情况。&lt;/p&gt;
&lt;h2 id=&#34;172-low-level-mechanisms&#34;&gt;17.2 Low-level Mechanisms&lt;/h2&gt;
&lt;h3 id=&#34;splitting-and-coalescing&#34;&gt;Splitting and Coalescing&lt;/h3&gt;
&lt;p&gt;我们用一个链表维护所有的空闲内存区间，链表的每个节点代表一段连续的空闲内存，有起始地址，块大小等字段。当我们要分配一段内存出去，且分配大小小于当前链表节点大小时，我们可以将链表节点的前半部分切出去给调用者，后半部分保留在链表中。这就是 splitting。&lt;/p&gt;
&lt;p&gt;当 free() 将一段内存空间释放时，如果我们不加任何操作地将其放到链表头，时间久了就会出现完整的空闲内存被我们人为地切成了若干个首尾相接的段的情况。一个好的维护方法是：链表中的所有节点按照顺序排列，每当新插入节点时，将其与前后相邻的节点比较，如果地址连在了一起就合并节点。这就是 coalescing。&lt;/p&gt;
&lt;h3 id=&#34;tracking-the-size-of-allocated-regions&#34;&gt;Tracking The Size Of Allocated Regions&lt;/h3&gt;
&lt;p&gt;free() 函数传入的只有待释放内存空间的起始地址，我们必须在分配的时候想办法记录分配出去的空间大小。一种方法是在分配区域首地址的前面记录一个 header：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct __header_t {
    int size;
    int magic;
}header_t;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当 &lt;code&gt;free(void *ptr)&lt;/code&gt; 来到时，我们首先在 ptr 前方的 header 处检查魔数是否正确以判定这是否是一个合法的可释放地址。如果魔数检查通过，我们便可以取出 size。注意最后加入空闲空间链表的总大小应为 &lt;code&gt;size + sizeof(header_t)&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;embedding-a-free-list&#34;&gt;Embedding A Free List&lt;/h3&gt;
&lt;p&gt;我们需要空间来保存链表，但我们作为 memory allocator 本身，当然不能调用 malloc() 去申请内存。我们应当将链表节点直接保存在空闲的内存空间里。&lt;/p&gt;
&lt;p&gt;当一个 alloc() 需求来临时，选择一个合法的节点 split 出需要的空间。当一个 free() 来临时，将新的空闲空间节点加入链表，并在可以时与相邻节点合并。&lt;/p&gt;
&lt;h3 id=&#34;growing-the-heap&#34;&gt;Growing The Heap&lt;/h3&gt;
&lt;p&gt;当扫描完整个空闲空间链表但仍然无法分配出所需大小的空间时，返回 NULL 表示无法分配是完全合情合理的。对于应用层的 memory allocator 来说，如果内存耗尽，它会用系统调用 (如 sbrk()) 向操作系统申请更多的内存并将其加入到空闲空间链表中。&lt;/p&gt;
&lt;h2 id=&#34;173-basic-strategies&#34;&gt;17.3 Basic Strategies&lt;/h2&gt;
&lt;p&gt;由于申请内存和释放内存的行为是完全由上层程序决定的，所以任何策略都会在针对性的 input 下把内存搞得很破碎。我们无法给出完美的解决方案，但至少可以提出一些比较有可能较优的解决方案。&lt;/p&gt;
&lt;h3 id=&#34;best-fit-smallest-fit&#34;&gt;Best Fit (Smallest Fit)&lt;/h3&gt;
&lt;p&gt;Best fit 策略是：alloc() 需求来临时，根据大小从链表中找出可以满足要求且大小最小的链表节点 split 出一块。该策略可以比较好地减少破碎的内存块，但每次 alloc() 都完整地遍历链表代价太大。&lt;/p&gt;
&lt;h3 id=&#34;worst-fit&#34;&gt;Worst Fit&lt;/h3&gt;
&lt;p&gt;Worst Fit 每次挑最大的内存块切割，这样可以避免出现很多破碎的小内存块，但它也要遍历整个链表，而且实际情况下效果不好。&lt;/p&gt;
&lt;h3 id=&#34;first-fit&#34;&gt;First Fit&lt;/h3&gt;
&lt;p&gt;First Fit 总是寻找第一个足够切割的内存块 alloc()，这样不需要遍历整个链表。但做久了以后链表的头部会充斥比较多的内存碎片。&lt;/p&gt;
&lt;h3 id=&#34;next-fit&#34;&gt;Next Fit&lt;/h3&gt;
&lt;p&gt;Next Fit 的做法是保存上一次分配的位置，下一次要分配时从上一次的位置开始 First Fit。实践中这种方法的表现和 First Fit 差不多。&lt;/p&gt;
&lt;h2 id=&#34;174-other-approaches&#34;&gt;17.4 Other Approaches&lt;/h2&gt;
&lt;h3 id=&#34;segregated-list&#34;&gt;Segregated List&lt;/h3&gt;
&lt;p&gt;slab 的基本思路时：对于一些比较常见的分配大小 (比如 4B, 2B, page size) 等，可以准备一个链表专门存储这种大小的块块，这样分配的时候直接从链表中取一个节点即可，不需要之前所说的繁琐的切分、合并等步骤。slab 有点像全局 memory manager 的一个 cache，如果某个时刻 slab 里的节点用完了，它会从全局的大链表中再批发一些节点。&lt;/p&gt;
&lt;h3 id=&#34;buddy-allocation&#34;&gt;Buddy Allocation&lt;/h3&gt;
&lt;p&gt;Buddy Allocation 比较像“线段树”：它把一个长度为 $2^N$ 的区间分成左右各 $2^{N-1}$ 的，每个节点再一分为二，依次类推。这样我们不需要仔细地维护拆分、合并相关的问题。Buddy Allocation 的弱点在于：为了对齐我们只能分配 2 的幂次大小的块，会造成一些 internal fragmentation。&lt;/p&gt;
&lt;h2 id=&#34;175-summary&#34;&gt;17.5 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 26: Concurrency: An Introduction</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch26/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch26/</guid>
      <description>&lt;p&gt;一个多线程程序会有多个执行流，即有好几个 PC 并发/并行地取指执行，我们可以将其理解为多个进程，不过它们共享同一个地址空间。每个线程有自己的 PC 和一套寄存器，因此类似于进程切换，我们也需要线程切换。不过由于线程共享地址空间，所以线程切换时无需切换页表。&lt;/p&gt;
&lt;p&gt;虽然线程可以共享地址空间，但每个线程要有自己独立的栈。不同于单线程地址空间，多线程程序的地址空间中有多个栈，每个线程一个，这个线程栈也被称为 thread-local storage。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;261-why-use-threads&#34;&gt;26.1 Why Use Threads?&lt;/h2&gt;
&lt;p&gt;使用线程至少有以下两点好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提高并行度。在一个多核处理器上，我们可以让多个线程每个占据一个 CPU 核，分摊一部分工作，从而达到并行提速的效果。&lt;/li&gt;
&lt;li&gt;避免 I/O 操作阻塞程序。如果一个线程在等待 I/O 操作，CPU 可以通过线程切换让别的线程上 CPU 工作，这样就实现了等 I/O 和做其他事情同时进行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;262-an-example-thread-creation&#34;&gt;26.2 An Example: Thread Creation&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;&#34;&gt;t0.c&lt;/a&gt; 中创建了两个进程并让它们打印不同的内容。在多线程程序中，各个线程执行的顺序是不确定的：先被创建的进程可能会后执行；一个线程被创建了之后可能在 wait 它时才执行，也可能会立即执行，然后 wait 时立即返回 etc.&lt;/p&gt;
&lt;h2 id=&#34;263-why-it-gets-worse-shared-data&#34;&gt;26.3 Why It Gets Worse: Shared Data&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;&#34;&gt;t1.c&lt;/a&gt; 中创建了两个线程，两个线程都对共享变量进行 N 次 +1，在 N 较大时可以观测到共享变量的结果小于 2N。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Thread-local Variables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在线程调用的函数里定义的变量都会是 thread local 的变量。&lt;code&gt;t1.c&lt;/code&gt; 中打印了局部变量 i 的地址，可以看到不同的线程打印出的地址不一样 (在各自的线程栈上)。&lt;/p&gt;
&lt;p&gt;在函数体外，用 &lt;code&gt;__thread&lt;/code&gt; 修饰的变量也是 thread local 的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Valgrind&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Valgrind 中的 memchecker 是强大的内存检测工具。编译好一个文件 &lt;code&gt;proc.c&lt;/code&gt; 后用 &lt;code&gt;valgrind ./proc&lt;/code&gt; 执行，可以检测 memory leak, use after free 等问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;264-the-heart-of-the-problem-uncontrolled-scheduling&#34;&gt;26.4 The Heart Of The Problem: Uncontrolled Scheduling&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;counter ++&lt;/code&gt; 这条语句在汇编层面是三条指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;mov (addr), %eax
add $0x1, %eax
mov %eax, (addr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;addr&lt;/code&gt; 是 counter 变量的内存地址。如果两个线程轮流执行汇编语句，那么它们各执行一次 +1 后，事实上 counter 只加了 1,并没有 +2。&lt;/p&gt;
&lt;p&gt;我们称这两个线程触发了竞争条件 (race condition)，准确说是一次数据竞争。导致这一竞争的代码称为临界区域 (critical section)。我们希望实现一种互斥机制，使得一个线程执行临界区域代码时不会被另一个线程打断。&lt;/p&gt;
&lt;h2 id=&#34;265-the-wish-for-atomicity&#34;&gt;26.5 The Wish For Atomicity&lt;/h2&gt;
&lt;p&gt;我们希望有这样的一条原子指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;momory-add (addr), 0x1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以一下子帮我们完成 +1 操作。这里的原子性指的是：操作在执行过程中不会被中途打断。这个操作是一个最小的单元，它要么被完全执行了，要么没有被执行  (all or none)，不会在某时刻处于执行了一半的状态。&lt;/p&gt;
&lt;p&gt;将若干个操作打包成一个原子指令的过程我们称为一个交易 (transaction)。但硬件不能允许我们无限制地添加原子指令。我们要实现的是一种同步机制，它可以保证多个线程以一种可控的方式进入临界区域，从而使得执行正确。&lt;/p&gt;
&lt;h2 id=&#34;266-one-more-problem-waiting-for-another&#34;&gt;26.6 One More Problem: Waiting For Another&lt;/h2&gt;
&lt;p&gt;在某些情境下，一个线程需要等待某件事情做好了才能继续进行，比如等待 I/O 操作完成。操作系统不仅要支持线程之间的同步，还要支持在某些条件下挂起/唤醒线程，这会通过条件变量 (condition variables) 来实现。&lt;/p&gt;
&lt;h2 id=&#34;267-summary-why-in-os-class&#34;&gt;26.7 Summary: Why in OS Class?&lt;/h2&gt;
&lt;p&gt;操作系统本身也是一个并发程序。如果有多个程序使用 write() 系统调用，那么操作系统就必须非常小心地处理内核中和 write() 有关的并发数据结构。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 27: Interlude: Thread API</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch27/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch27/</guid>
      <description>&lt;h2 id=&#34;271-thread-creation&#34;&gt;27.1 Thread Creation&lt;/h2&gt;
&lt;p&gt;创建线程的 API 为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_create(      pthread_t      *thread, 
                   const pthread_attr_t *attr,
                         void *         (*start_routine)(void*),
                         void *         arg);
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;thread 是指向 pthread_t 类型变量的指针，我们将来要通过这个结构体来控制这个线程，所以现在需要对其进行初始化。&lt;/li&gt;
&lt;li&gt;attr 表明了希望该线程拥有的属性，大多数情况下可以传 NULL，表示按照默认属性创建。&lt;/li&gt;
&lt;li&gt;start_routine 是一个指向函数的指针，新创建的线程会从这个函数开始执行。&lt;/li&gt;
&lt;li&gt;arg 是传给 start_routine() 的参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- more --&gt;
&lt;p&gt;这里参数的类型和返回值类型都是 &lt;code&gt;void *&lt;/code&gt; 类型的指针 (相当于只要求地址，不要求对地址类型的解读)，这是为了使得函数支持任意类型的参数和返回值。&lt;/p&gt;
&lt;h2 id=&#34;272-thread-completion&#34;&gt;27.2 Thread Completion&lt;/h2&gt;
&lt;p&gt;等待线程完成的 API 为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_join(pthread_t thread,
                 void      **value_ptr);
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;thread 表示要等待运行完成的线程结构体。&lt;/li&gt;
&lt;li&gt;value_ptr 是一个指向 &lt;code&gt;void *&lt;/code&gt; 类型指针的指针，pthread_join() 返回时， value_ptr 会指向线程创建时的 start_routine() 函数的返回值。如果我们不在乎这个返回值，可以直接传入 NULL。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很多时候，我们需要的线程函数返回值只是一个数 (如 0 表示成功，1 表示失败)，这时我们有比较简单的写法：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *mythread(void *arg) {
    return (void *)(arg + 1);
}

int main () {
    int r; pthread_t p;
    pthread_create(&amp;amp;p, NULL, mythread, (void *)100);
    pthread_join(p, (void **)&amp;amp;r);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在线程函数中将返回值转换成 &lt;code&gt;void *&lt;/code&gt; 类型，在主函数中我们只要将整型变量 r 的地址转换成 &lt;code&gt;void **&lt;/code&gt; 类型，就可以直接把返回值存到 r 里面。&lt;/p&gt;
&lt;p&gt;使用线程返回时要格外注意：返回值的实体不能在线程栈上，因为线程返回时线程栈会被释放。比如&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *mythread(void *arg) {
    myret_t r; r = {10, 20};
    return (void *)&amp;amp;r;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个写法是不合理的，返回时 r 已经被释放，返回结果是 UB。将结构体定义在堆区可以避免这个问题：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *mythread(void *arg) {
    myret_t *r = malloc(sizeof(myret_t));
    *r = {10, 20};
    return (void *)r;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;273-locks&#34;&gt;27.3 Locks&lt;/h2&gt;
&lt;p&gt;用于获得互斥锁和释放互斥锁的 API 为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个函数会尝试获得锁，直到获得了之后返回 (互斥锁不会轮询锁的状态，在得不到时会进入睡眠状态)。第二个第二个函数用于释放锁。&lt;/p&gt;
&lt;p&gt;注：这两个函数是有返回值的。正常情况下它们应该返回 0。一个好的编程习惯是在调用这些 API 时随手检查返回值，比如封装成这样：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void Pthread_mutex_lock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_lock(mutex);
    assert(rc == 0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;互斥锁在使用之前需要先初始化。初始化有两种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在定义互斥锁变量时直接使用 INITIALIZER，它会按照默认属性初始化锁：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在运行过程中初始化：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void Pthread_mutex_init(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_init(mutex, NULL);
    assert(rc == 0);
}
int main () {
    pthread_mutex_t lock;
    Pthread_mutex_init(&amp;amp;lock);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pthread_mutex_init() 的第二个参数用于指定初始化锁的属性，通常情况下使用 NULL 即可。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;锁用完之后，应当使用和初始化函数相对应的 pthread_mutex_destroy() 函数来销毁一个锁。&lt;/p&gt;
&lt;p&gt;另有两个 API：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_mutex_trylock(pthread_mutex_t *mutex);
int pthread_mutex_timedlock(pthread_mutex_t *mutex,
                            struct timespec *abs_timeout);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;trylock() 会尝试获得锁，如果锁正在被占用则直接返回。timedlock() 会在一个指定的时间范围内尝试获得锁，如果这个时间段内未能获得锁就返回。trylock() 可以理解为 timedlock() 的 0 秒版本；之前的 lock() 可以理解为 timedlock() 的无限长时间版本。&lt;/p&gt;
&lt;p&gt;这两个 API 不常用，但在一些情境下可以用于避免死锁。&lt;/p&gt;
&lt;h2 id=&#34;274-condition-variables&#34;&gt;27.4 Condition Variables&lt;/h2&gt;
&lt;p&gt;条件变量的两个主要 API 为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;调用 cond_wait() 时，当前线程必须已经拥有互斥锁 mutex。cond_wait() 会释放 mutex 并将当前线程睡眠在条件变量 cond 上。当另外的某个线程调用 cond_signal() 唤醒了该线程时，该线程会重新尝试获得互斥锁 mutex，获得了之后从 cond_wait() 函数返回。&lt;/p&gt;
&lt;p&gt;使用条件变量之前要先对条件变量初始化，其方法和互斥锁是类似的：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;pthread_cond_t cond = PTREAD_COND_INITALIZER;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;275-compiling-and-running&#34;&gt;27.5 Compiling and Running&lt;/h2&gt;
&lt;p&gt;要使用上述的 POSIX 线程库中的函数，我们需要在源代码中 &lt;code&gt;#include &amp;lt;pthread.h&amp;gt;&lt;/code&gt; ，并在编译时加入 &lt;code&gt;-lpthread&lt;/code&gt; 选项。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 29: Lock-based Concurrent Data Structures</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch29/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch29/</guid>
      <description>&lt;h2 id=&#34;291-concurrent-counters&#34;&gt;29.1 Concurrent Counters&lt;/h2&gt;
&lt;p&gt;一个最简单的 thread-safe 的计数器实现如下 (用锁保护每次读写)：&lt;/p&gt;
&lt;!-- more --&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct __counter_t {
    int value;
    pthread_mutex_t lock;
} counter_t;
void init(counter_t *c) {
    c-&amp;gt;value = 0;
    Pthread_mutex_init(&amp;amp;c-&amp;gt;lock, NULL);
}
void increment(counter_t *c) {
    Pthread_mutex_lock(&amp;amp;c-&amp;gt;lock);
    c-&amp;gt;value++;
    Pthread_mutex_unlock(&amp;amp;c-&amp;gt;lock);
}
void decrement(counter_t *c) {
    Pthread_mutex_lock(&amp;amp;c-&amp;gt;lock);
    c-&amp;gt;value--;
    Pthread_mutex_unlock(&amp;amp;c-&amp;gt;lock);
}
int get(counter_t *c) {
    Pthread_mutex_lock(&amp;amp;c-&amp;gt;lock);
    int rc = c-&amp;gt;value;
    Pthread_mutex_unlock(&amp;amp;c-&amp;gt;lock);
    return rc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但这种实现的 scalability 很差：如果给 $n$ 个核分配等量的任务，最终所消耗的时间几乎是单个任务时长的 $n$ 倍。大部分时间 CPU 核之间在互相等锁，我们没有利用多处理器达到并行的效率。&lt;/p&gt;
&lt;p&gt;如果愿意牺牲一部分精确性，我们可以设计一种 approximate counter 来提升效率。approximate counter 中，每个 CPU 核有一个单独的 counter，此外还有一个全局的 counter。全局和本地的 counter 都有锁保护。修改操作中 (假设只有 increment)，CPU 核直接修改本地的 counter，因此各个核可以并行。当本地 counter 的值达到一个阈值时，本地 counter 会和全局 counter 做一次同步，将本地值加到全局值上并将本地值清零。查询操作中直接返回全局 counter 的值即可。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct __counter_t {
    int             global;
    pthread_mutex_t glock;
    int             local[NUMCPUS];
    pthread_mutex_t llock[NUMCPUS];
    int             threshold;
}counter_t;
void init(counter_t *c, int threshold) {
    c-&amp;gt;threshold = threshold;
    c-&amp;gt;global = 0;
    Pthread_mutex_init(&amp;amp;c-&amp;gt;glock, NULL);
    for (int i = 0; i &amp;lt; NUMCPUS; ++i) {
        c-&amp;gt;local[i] = 0;
        Pthread_mutex_init(&amp;amp;c-&amp;gt;llock[i], NULL);
    }
}
void update(counter_t *c, int threadID, int amt) {
    int cpu = threadID % NUMCPUS;
    Pthread_mutex_lock(&amp;amp;c-&amp;gt;llock[cpu]);
    c-&amp;gt;local[cpu] += amt;
    if (c-&amp;gt;local[cpu] &amp;gt;= c-&amp;gt;threshold) {
        Pthread_mutex_lock(&amp;amp;c-&amp;gt;glock);
        c-&amp;gt;glocal += c-&amp;gt;local[cpu];
        Pthread_mutex_unlock(&amp;amp;c-&amp;gt;glock);
        c-&amp;gt;local[cpu] = 0;
    }
    Pthread_mutex_unlock(&amp;amp;c-&amp;gt;llock[cpu]);
}
int get(counter_t *c)
{
    Pthread_mutex_lock(&amp;amp;c-&amp;gt;glock);
    int rt = c-&amp;gt;global;
    Pthread_mutex_unlock(&amp;amp;c-&amp;gt;glock);
    return rt;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在该实现版本中，每个线程并不会去确认自己所处的核，而是直接随机 (进程号取模) 分配一个核对应的本地计数器。这样做和之前描述的算法无异。&lt;/p&gt;
&lt;p&gt;上述算法中，如果阈值是 0 则与精确的计数器无异。随着阈值的提高，计数器的并行效率会越来越高 (大致呈反比例函数)，但返回的结果会越来越不精确。&lt;/p&gt;
&lt;h2 id=&#34;292-concurrent-linked-list&#34;&gt;29.2 Concurrent Linked List&lt;/h2&gt;
&lt;p&gt;一个朴素的做法是用一把大锁保护整个链表的修改和查询。一种很容易出错的情形是：如果函数中间有多处 return，则每处 return 前都要记得释放锁。为了避免这种 bug，书写代码时可以考虑多用 break 替代 return 减少 return 分支数，或者另写一个 wrapper 调用真正的函数，在 wrapper 中维护锁。&lt;/p&gt;
&lt;p&gt;一种增加链表访问并行度的方案是所谓的 hand-over-hand locking (lock coupling)。在查询链表中是否有某个元素时，我们平常的做法是用一个大锁保护整个链表，然后依次扫描链表节点。hand-over-hand locking 的思想是为每个链表节点创建一把锁，每次准备访问下一个节点时，先获得下一个节点的锁，再释放本节点的锁。这样多个线程就可以并行地查询链表。&lt;/p&gt;
&lt;p&gt;这个方案在理论上可行，但实际操作中，频繁地获得和释放锁会带来昂贵的代价。一个更可行的方案是每 $n$ 个节点用一把锁保护，准备进入下一个“区域”时做一次 overhead acquiring and releasing。&lt;/p&gt;
&lt;h2 id=&#34;293-concurrent-queues&#34;&gt;29.3 Concurrent Queues&lt;/h2&gt;
&lt;p&gt;我们可以在队头和队尾分别维护一把锁，这样从队头取元素和向队尾添加元素的操作可以并行地执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct __node_t {
    int              val;
    struct __node_t *next;
}node_t;
typedef struct __queue_t {
    node_t          *head;
    node_t          *tail;
    pthread_mutex_t  headlock;
    pthread_mutex_t  taillock;
}queue_t;
void queue_init(queue_t *q) {
    node *tmp = malloc(sizeof(node_t));
    tmp-&amp;gt;next = NULL;
    q-&amp;gt;head = q-&amp;gt;tail = tmp;
    pthread_mutex_init(&amp;amp;q-&amp;gt;headlock, NULL);
    pthread_mutex_init(&amp;amp;q-&amp;gt;taillock, NULL);
}
int queue_enqueue(queue_t *q, int val) {
    node *tmp = malloc(sizeof(node_t));
    if (tmp == NULL) return -1;
    tmp-&amp;gt;val = val; tmp-&amp;gt;next = NULL;
    pthread_mutex_lock(&amp;amp;q-&amp;gt;taillock);
    q-&amp;gt;tail-&amp;gt;next = tmp;
    q-&amp;gt;tail = tmp;
    pthread_mutex_unlock(&amp;amp;q-&amp;gt;taillock);
}
int queue_dequeue(queue_t *q, int *val) {
    pthread_mutex_lock(&amp;amp;q-&amp;gt;headlock);
    node *tmp = q-&amp;gt;head;
    node *newhead = tmp-&amp;gt;next;
    if (newhead == NULL) {
        pthread_mutex_unlock(&amp;amp;q-&amp;gt;headlock);
        return -1; // queue was empty
    }
    *val = tmp-&amp;gt;val;
    q-&amp;gt;head = newhead;
    pthread_mutex_unlock(&amp;amp;q-&amp;gt;headlock);
    free(tmp);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里的一个重要的技巧是：我们在初始化队列时创建了一个 dummy node，并让 &lt;code&gt;q-&amp;gt;head&lt;/code&gt; 和 &lt;code&gt;q-&amp;gt;tail&lt;/code&gt; 都指向它。这个 dummy node 是永远不会出队的。这样我们避免了队列在形状上完全为空的情况，从而保证 headlock 和 taillock 做的工作永远是没有交叉的。&lt;/p&gt;
&lt;h2 id=&#34;294-concurrent-hash-table&#34;&gt;29.4 Concurrent Hash Table&lt;/h2&gt;
&lt;p&gt;一个支持并发访问的 Hash table 实现很简单：使用多个之前提到的链表即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#define BUCKETS (101)

typedef struct __hash_t {
    list_t lists[BUCKETS];
}hash_t;

void Hash_Init(hash_t *H) {
    for (int i = 0; i &amp;lt; BUCKETS; i++)
        List_Init(&amp;amp;H-&amp;gt;lists[i]);
}

int Hash_Insert(hash_t *H, int key) {
    int bucket = key % BUCKETS;
    return List_Insert(&amp;amp;H-&amp;gt;lists[bucket], key);
}

int Hash_Lookup(hash_t *H, int key) {
    int bucket = key % BUCKETS;
    return List_Lookup(&amp;amp;H-&amp;gt;lists[bucket], key);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个实现在实际中效率不低的原因是：我们并没有用一把大锁保护整个哈希表，而是对每个链表单独用一把所保护。多个线程同时访问哈希表的一个 bucket 的几率较低，这使得 lock contention 发生次数较少。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip: Avoid Premature Optimization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Premature Optimization is the root of all evil.&amp;rdquo; - Knuth&lt;/p&gt;
&lt;p&gt;很多操作系统内核开发时，都是先使用一个大内核锁 (big kernel lock, BKL)，先保证正确性，再考虑如何把锁拆开提升效率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;295-summary&#34;&gt;29.5 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 30: Condition Variables</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch30/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch30/</guid>
      <description>&lt;p&gt;很多时候，我们有一个任务等待另一个任务的需求，比如主线程等到子线程结束后再继续执行 (这个函数通常被称为 join() )。一个朴素的想法是利用一个共享变量实现：&lt;!-- more --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;volatile int done = 0;

void *child(void *arg) {
    do_something();
    done = 1;
    return NULL;
}

int main () {
    pthread_t c;
    Pthread_create(&amp;amp;c, NULL, child, NULL);
    while (done == 0)
        ;
    do_something_else();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但这样做主线程会在 done 变量上不停地自旋，占 CPU 不干活。我们希望有更高效的实现方法。&lt;/p&gt;
&lt;h2 id=&#34;301-definition-and-routines&#34;&gt;30.1 Definition and Routines&lt;/h2&gt;
&lt;p&gt;为了让一个线程等待一个条件成立，我们通常使用条件变量 (condition variable)。条件变量可以理解为一个显式的队列，线程可以将自己加入到这个队列中挂起，当其他线程的操作使得条件变化时，它会唤醒这个“队列”中的线程。我们提供两个 API：&lt;code&gt;wait()&lt;/code&gt; 可以让线程将自己加入到队列中等待；&lt;code&gt;signal()&lt;/code&gt; 可以让别的线程改变了条件状态时向队列中的线程“发送信号”。&lt;/p&gt;
&lt;p&gt;POSIX 提供的条件变量 API 为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int pthread_cond_wait(pthread_cont_t *c, pthread_mutex_t *m);
int pthread_cond_signal(pthread_cond_t *c);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意到 wait() 的参数中除了条件变量 c 还有一个自旋锁 m。wait() 的语义要求调用时线程必须拥有自旋锁 m，wait() 会负责释放这个自旋锁并使当前线程进入睡眠状态 (这两步是原子的)；当 signal() 使当前线程被唤醒后，wait() 会负责让线程重新获得自旋锁 m，然后返回。&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2030%20-%20threads_cv/join.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;join.c&lt;/a&gt; 提供了一个主线程等待子线程结束的正确实现。事实上这个过程是很容易实现错的，比如如下一些写法：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void thr_exit() {
    Pthread_mutex_lock(&amp;amp;m);
    Pthread_cond_signal(&amp;amp;c);
    Pthread_mutex_unlock(&amp;amp;m);
}
void thr_join() {
    Pthread_mutex_lock(&amp;amp;m);
    Pthread_cond_wait(&amp;amp;c, &amp;amp;m);
    Pthread_mutex_unlock(&amp;amp;m);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个写法只有在 thr_join() 在 thr_exit() 之前执行的情况下才是正确的。如果颠倒了顺序，thr_join() 将永远不会被唤醒 (
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2030%20-%20threads_cv/join_no_state_var.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;join_no_state_var.c&lt;/a&gt; 中，主线程里使用了 sleep() 以精确复现这一情形)。因此我们可以看到，使用条件变量时理应有一个条件状态的判断。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void thr_exit() {
    done = 1;
    Pthread_cond_signal(&amp;amp;c);
}
void thr_join() {
    while (done == 0)
        Pthread_cond_wait(&amp;amp;c);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个写法没有用锁保护状态变量 done 的读写，缺失了原子性。这里的原子性缺失指的是：虽然在 while 判断时 &lt;code&gt;done == 0&lt;/code&gt;，但在 wait() 时 done 可能已经不等于 0 了。考虑这样一种执行流：thr_join() 做完 while 判断进入循环，还没来得及 wait() 时，执行流被打断，thr_exit() 执行并 signal()，但此时队列中并没有线程。接着 thr_join() 继续执行陷入睡眠，那么它将永远不会被唤醒 (
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2030%20-%20threads_cv/join_no_lock.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;join_no_lock()&lt;/a&gt; 中在主线程使用了比子线程时间更长的 sleep() 以精确复现这一情况)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip: Always Hold The Lock While Signaling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;POSIX 的 wait() 函数的语义已经要求了我们在调用 wait() 时必须要拥有自旋锁；虽然少数情况下我们可以无锁地调用 signal()，但为了安全和简单，我们应当在 signal() 时也用自旋锁保护。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;302-the-producerconsumer-bounded-buffer-problem&#34;&gt;30.2 The Producer/Consumer (Bounded Buffer) Problem&lt;/h2&gt;
&lt;p&gt;生产者-消费者问题在系统中很常见，比如下面的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;grep foo file.txt | wc -l
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;grep 找到的内容通过管道传送给 wc 统计个数。管道在内核中就是一个缓冲区，因此这里 grep 是一个生产者，wc 是一个消费者，wc 不能在 buffer 为空的时候从管道里读东西，grep 也不能在缓冲区满了的时候再往里填。&lt;/p&gt;
&lt;h3 id=&#34;a-broken-solution&#34;&gt;A Broken Solution&lt;/h3&gt;
&lt;p&gt;一个比较自然但却错误的实现如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *producer(void *arg) {
    for (int i = 0; i &amp;lt; LOOP; i++) {
        lock(&amp;amp;lk);
        if (count == 1) wait(&amp;amp;cond, &amp;amp;lk);
        put(i); // count becomes 1
        signal(&amp;amp;cond);
        unlock(&amp;amp;lk);
    }
}
void *consumer(void *arg) {
    while (1) {
        lock(&amp;amp;lk);
        if (count == 0) wait(&amp;amp;cond, &amp;amp;lk);
        int rt = get(); // count becomes 0
        signal(&amp;amp;cond);
        unlock(&amp;amp;lk);
        printf(&amp;quot;%d\n&amp;quot;, rt);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果只有一个生产者线程和一个消费者线程，这个实现是正确的。但如果有多个，比如一个生产者和两个消费者，就会有并发 bug。考虑如下执行过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_{c_1}$ 运行，发现 &lt;code&gt;count == 0&lt;/code&gt;，挂起；&lt;/li&gt;
&lt;li&gt;$T_p$ 运行，往 buffer 里放了一个数，然后通过 signal() 唤醒 $T_{c_1}$。&lt;/li&gt;
&lt;li&gt;$T_{c_1}$ 刚被唤醒，还没来得及获得自旋锁返回的时候，$T_{c_2}$ 运行，从 buffer 里取走了这个数打印；&lt;/li&gt;
&lt;li&gt;这时返回到 $T_{c_1}$，获得自旋锁返回后，由于之前的判断是 if 语句，它无法发现 count 此时又变成 0 了，于时再次从 buffer 中取数，触发 assertion fail。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;并发 bug 产生的原因是：从消费者线程被唤醒到消费者线程真正获得自旋锁开始工作这段时间内，buffer 的状态改变了。&lt;strong&gt;signal() 的语义只是通知一个正在等待的线程：世界的状态改变了，但它不保证世界的状态被改变成了调用 signal() 之前那一瞬的状态。&lt;/strong&gt; 这种 signal() 的语义被称为 &lt;strong&gt;Mesa semantics&lt;/strong&gt;。与之相对的是 &lt;strong&gt;Hoare Semantics&lt;/strong&gt;，它保证调用 signal() 之后一个线程被唤醒并立即被执行 (原子性)。但后者难实现的多，现在绝大部分系统的 signal() 使用的都是 Mesa semantics。&lt;/p&gt;
&lt;h3 id=&#34;better-but-still-broken-while-not-if&#34;&gt;Better, But Still Broken: While, Not If&lt;/h3&gt;
&lt;p&gt;Mesa semantics 是比较容易克服的：我们只要将上面程序中对状态变量做判断的 if 换成 while 即可。这样即使被唤醒后状态又被改变，线程获得自旋锁后会再次检查状态，发现不对后可以再次进入睡眠。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Always use while loops when working with condition variables!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但即使将 if 改成 while，我们的程序仍然有并发 bug。考虑如下执行过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_{c_1}$ 运行，发现 &lt;code&gt;count == 0&lt;/code&gt;，挂起；&lt;/li&gt;
&lt;li&gt;$T_{c_2}$ 运行，发现 &lt;code&gt;count == 0&lt;/code&gt;，挂起；&lt;/li&gt;
&lt;li&gt;$T_p$ 运行，往 buffer 里添加了一个数字，然后通过 signal() 唤醒了 $T_{c_1}$。唤醒后 $T_{c_1}$ 并未立刻执行，而是 $T_p$ 继续运行，第二次循环时 $T_p$ 发现 &lt;code&gt;count == 1&lt;/code&gt;，挂起；&lt;/li&gt;
&lt;li&gt;$T_{c_1}$ 运行，从 buffer 里读取了一个数字 (count 变为 0) 然后通过 signal() 唤醒一个线程。注意此时睡眠在条件变量上的线程有 $T_p$ 和 $T_{c_2}$ 两个。$T_{c_1}$ 选择唤醒 $T_{c_2}$。&lt;/li&gt;
&lt;li&gt;$T_{c_1}$ 和 $T_{c_2}$ 都发现没有数据可读，挂起。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;到这里，三个线程全部陷入睡眠，程序停滞。从这个例子中我们可以看出：消费者线程消费完后应该唤醒生产者线程；生产者线程生产完后应该唤醒消费者线程，我们应该有两个条件变量。&lt;/p&gt;
&lt;h3 id=&#34;the-correct-producerconsumer-solution&#34;&gt;The Correct Producer/Consumer Solution&lt;/h3&gt;
&lt;p&gt;pc.c 提供了一个完整、正确的生产者-消费者实现，其中的要点如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用两个条件变量，保证消费者只能唤醒生产者，生产者只能唤醒消费者。&lt;/li&gt;
&lt;li&gt;判断状态变量使用 while 语句。&lt;/li&gt;
&lt;li&gt;使用一个循环数组作为 buffer，count 的上限不再是 1 而是一个指定的 max，这使得 buffer 有了一定的容量。&lt;/li&gt;
&lt;li&gt;这是一个单生产者-多消费者的程序，生产者最后为每个消费者准备了一个 -1，以保证消费者线程能够全部退出。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip: Use While (Not If) for Conditions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用 while 总是对的，在少数场合下使用 if 也可以达到目的，但为了安全性最好通通使用 while。&lt;/p&gt;
&lt;p&gt;我们推荐使用 while 而不是 if 的另一个原因是：一些实现的有问题的 signal() 可能会唤醒不止一个等待的线程。这种情况下，while 仍然能帮我们保证正确。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;303-covering-conditions&#34;&gt;30.3 Covering Conditions&lt;/h2&gt;
&lt;p&gt;假设我们希望用条件变量实现一个内存分配器，alloc() 时如果剩余内存不足则挂起，一段伪代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int bytesLeft = MAX_HEAP_SIZE;
void *allocate(int size) {
    lock(&amp;amp;lk);
    while (bytesLeft &amp;lt; size)
        wait(&amp;amp;cond, &amp;amp;lk);
    void *ptr = GetMemoryFromHeap(size);
    bytesLeft -= size;
    unlock(&amp;amp;lk);
    return ptr;
}
void free(void *ptr, int size) {
    lock(&amp;amp;lk);
    bytesLeft += size;
    FreeMemoryToHeap(ptr, size);
    signal(&amp;amp;cond);
    unlock(&amp;amp;lk);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假设我们有一个 &lt;code&gt;alloc(100)&lt;/code&gt; 和 &lt;code&gt;alloc(10)&lt;/code&gt; 处于睡眠状态，现在来了一个 &lt;code&gt;free(50)&lt;/code&gt;，问题出现了：如果我们随意挑一个线程唤醒，假如挑了 &lt;code&gt;alloc(100)&lt;/code&gt; 的那个，分配还是会失败，且我们错过了让 &lt;code&gt;alloc(10)&lt;/code&gt; 成功的机会。一种解决方案是：用一个 broadcast() 函数替代 signal() 函数，它可以唤醒睡眠在条件变量上的所有线程。这种方式被称为 covering conditions。&lt;/p&gt;
&lt;p&gt;broadcast() 相较于 signal() 的坏处在于：并不是所有的线程都能在被唤醒后运行下去，比如现在 buffer 里只有一个字符，而 broadcast() 唤醒了 100 个消费者，那么只有一个消费者能得到字符。运行不下去的字符会释放锁并再次进入睡眠，这种无谓打搅了睡眠线程的方式对性能影响很大。&lt;/p&gt;
&lt;p&gt;正因如此，虽然我们在之前的生产者-消费者问题中可以使用一个条件变量+broadcast 的方式解决问题，但那时我们显然有很简单的 2-条件变量解决方案，所以不考虑这种效率较低的方式。不过在内存分配的场景中，broadcast() 几乎是唯一的选择。&lt;/p&gt;
&lt;h2 id=&#34;304-summary&#34;&gt;30.4 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 31: Semaphores</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch31/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch31/</guid>
      <description>&lt;p&gt;信号量的概念最早由 Dijkstra 提出，它既可以作为锁使用也可以作为条件变量使用。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;311-semaphores-a-definition&#34;&gt;31.1 Semaphores: A Definition&lt;/h2&gt;
&lt;p&gt;使用一个信号量之前我们要定义其初始值：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;semaphore.h&amp;gt;
sem_t s;
sem_init(&amp;amp;s, 0, 1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sem_init() 的第一个参数是信号量，第三个参数是初始值，第二个参数为 0 表示该信号量在同进程下的所有线程之间共享 (如果想让信号量在不同进程之间共享，可以设置为其他数值)。&lt;/p&gt;
&lt;p&gt;POSIX 标准提供了信号量操作相关的 API：&lt;code&gt;sem_wait()&lt;/code&gt; 和 &lt;code&gt;sem_post()&lt;/code&gt;。相较于条件变量，信号量使用起来非常简单：我们不需要关注什么时候获得锁，用 while 还是 if 等问题，在用户层面我们可以认为这些 API 都是原子的。它们的语义可以用如下伪代码描述：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void sem_wait(sem_t *s) {
    decrement the value of s by one;
    wait if value of s is negative;
}
void sem_post(sem_t *s) {
    increment the value of s by one;
    if there are one or more threads waiting, wake one;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当 s 的值为正时，其意义是剩余的资源量；当 s 的值为负时，其意义是当前正在等待的线程数量。&lt;/p&gt;
&lt;p&gt;(注：不同于锁，sem_post() 并不需要一个线程曾经调用过 sem_wait()，这相当于线程可以凭空“创造”一份资源。因此使用时一定要格外小心，注意 sem_wait() 和 sem_post() 的对应。)&lt;/p&gt;
&lt;h2 id=&#34;312-binary-semaphores-locks&#34;&gt;31.2 Binary Semaphores (Locks)&lt;/h2&gt;
&lt;p&gt;信号量可以当作锁来使用，这种信号量被称为 binary semaphore。
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/binary.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binary.c&lt;/a&gt; 展示了具体的实现。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;sem_t m;
sem_init(&amp;amp;m, 0, 1);

sem_wait(&amp;amp;m);
// critical section
sem_post(&amp;amp;m);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;信号量的初始值设置为 1，我们可以把它想象成一个单位的资源 (任何时刻只能上一把锁)。考虑一个 lock contention 的情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程 1 调用 sem_wait()，m 的值变为 0，线程 1 成功进入临界区域。&lt;/li&gt;
&lt;li&gt;线程 2 调用 sem_wait()，m 的值变为 -1，该线程进入等待序列。&lt;/li&gt;
&lt;li&gt;线程 3 调用 sem_wait()，m 的值变为 -2，该线程进入等待序列。&lt;/li&gt;
&lt;li&gt;线程 1 离开临界区域，调用 sem_post()，m 的值变为 -1，线程 1 唤醒一个等待序列里的线程 (假设是线程 2) 继续执行。线程 2 成功进入临界区域。&lt;/li&gt;
&lt;li&gt;线程 2 离开临界区域，调用 sem_post()，m 的值变为 0，线程 2 唤醒线程 3，线程 3 成功进入临界区域。&lt;/li&gt;
&lt;li&gt;线程 3 离开临界区域，调用 sem_post()，m 的值恢复为 1 (锁当前空闲)，线程 3 没有唤醒任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用信号量实现的锁是 sleep lock 而不是 spinlock。&lt;/p&gt;
&lt;h2 id=&#34;313-semaphores-for-ordering&#34;&gt;31.3 Semaphores For Ordering&lt;/h2&gt;
&lt;p&gt;信号量可以用来实现等待 (类似于 join() 的功能)。这时信号量的初始值应当被设为 0。在等待的线程中使用 sem_wait()，在被等待的线程结尾使用 sem_post()。
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/join.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;join.c&lt;/a&gt; 给出了具体的实现。&lt;/p&gt;
&lt;p&gt;我们分析两种可能的情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主线程在子线程结束之前调用 sem_wait()，此时信号量的值变为 -1，主线程被挂起。待子进程执行结束后，调用 sem_post()，信号量的值恢复为 0，并唤醒等待的主线程继续执行。&lt;/li&gt;
&lt;li&gt;主线程在子线程结束之后调用 sem_wait()，那么子线程结束时调用 sem_post() 后，信号量的值变为 1。主线程调用 sem_wait()，信号量的值恢复为 0，因为非负，所以主线程可以直接继续执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;314-the-producerconsumer-bounded-buffer-problem&#34;&gt;31.4 The Producer/Consumer (Bounded Buffer) Problem&lt;/h2&gt;
&lt;p&gt;最直接的想法是将之前双条件变量的生产者-消费者模型直接套用过来：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void put(int val) {buffer[fill] = val; fill = (fill + 1) % MAX;}
int get() {int rt = buffer[use]; use = (use + 1) % MAX; return rt;}

void *producer(void *arg) {
    for (int i = 0; i &amp;lt; LOOP; i++) {
        sem_wait(&amp;amp;empty);
        put(i);
        sem_post(&amp;amp;full);
    }
}
void *consumer(void *arg) {
    while (1) {
        sem_wait(&amp;amp;full);
        printf(&amp;quot;%d\n&amp;quot;, get());
        sem_post(&amp;amp;empty);
    }
}
sem_init(&amp;amp;empty, 0, MAX);
sem_init(&amp;amp;full, 0, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个做法在 &lt;code&gt;MAX=1&lt;/code&gt; 时是可以正确执行的，但 MAX 大于等于 2 时会出现并发 bug。考虑两个生产者线程并发执行的过程：T1 执行完 sem_wait() 之后进入函数 put()，刚将 val 放进 buffer，还没有来得及给 fill +1 时，T1 被打断，T2 开始执行，因为 MAX&amp;gt;=2，这时 empty 不为 0，T2 可以顺利进入 put()，这时发生了问题：T1 放进 buffer 的内容被 T2 的内容覆盖了。&lt;/p&gt;
&lt;p&gt;可以看到出现并发 bug 的根本原因在于两个进程同时进入了临界区域，发生了数据竞争。联想条件变量的写法，线程在 wait() 中被唤醒后有一个尝试重新获得自旋锁的过程，而在这里的信号量实现中我们没有用锁把临界区域保护起来。因此我们只要加锁即可。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;真的随便加一个锁就可以了吗？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;考虑如下加锁后的实现：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *producer(void *arg) {
 for (int i = 0; i &amp;lt; LOOP; i++) {
     lock(&amp;amp;lk);
     sem_wait(&amp;amp;empty);
     put(i);
     sem_post(&amp;amp;full);
     unlock(&amp;amp;lk);
 }
}
void *consumer(void *arg) {
 while (1) {
     lock(&amp;amp;lk);
     sem_wait(&amp;amp;full);
     printf(&amp;quot;%d\n&amp;quot;, get());
     sem_post(&amp;amp;empty);
     unlock(&amp;amp;lk);
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假设一个消费者线程先运行，它获得了锁之后，调用 sem_wait()，发现 &lt;code&gt;full == -1&lt;/code&gt;，于是带着锁陷入了睡眠。这样别的线程再要尝试获得锁时就会陷入死锁：消费者线程拥有锁，在等待 full 信号；生产者线程可以 signal，但得先获得锁，这是一个标准的死锁。&lt;/p&gt;
&lt;p&gt;联想条件变量的实现，cond_wait() 和 sem_wait() 的不同在于，cond_wait() 接收一个锁的参数，会原子地完成线程睡眠和锁释放，而 sme_wait() 没有帮我们释放锁的功能。&lt;/p&gt;
&lt;p&gt;事实上，条件变量要把锁加在前面是因为我们要保证对全局的状态变量的访问 ( &lt;code&gt;while (!cond)&lt;/code&gt; ) 是原子的。但信号量本身已经保证原子性了。我们只要把锁加在里面就是正确的实现：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *producer(void *arg) {
 for (int i = 0; i &amp;lt; LOOP; i++) {
     sem_wait(&amp;amp;empty);
     lock(&amp;amp;lk);
     put(i);
     unlock(&amp;amp;lk);
     sem_post(&amp;amp;full);
 }
}
void *consumer(void *arg) {
 while (1) {
     sem_wait(&amp;amp;full);
     lock(&amp;amp;lk);
     printf(&amp;quot;%d\n&amp;quot;, get());
     unlock(&amp;amp;lk);
     sem_post(&amp;amp;empty);
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/producer_consumer_works.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;produer_consumer_works.c&lt;/a&gt; 提供了完整的实现。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;315-reader-writer-locks&#34;&gt;31.5 Reader-Writer Locks&lt;/h2&gt;
&lt;p&gt;动机：对于并发数据结构，我们会用锁保护与之相关的函数。我们用锁保护读函数是为了避免写的过程中读取函数执行导致奇怪的问题，但这也导致了如果当前只有很多个线程读取而没有写操作，我们的读取无法并行。Reader-Writer Locks 致力于解决这个问题 (事实上以下方法中的信号量都在模拟锁的行为，可以只使用自旋锁完成)。&lt;/p&gt;
&lt;p&gt;解决这个问题的基本思路是：写操作每次要获得一个写锁，完成任务后要释放写锁。读操作的行为有所不同：我们维护一个变量 reader 记录当前正在读的线程的个数，当 &lt;code&gt;reader == 1&lt;/code&gt; 时，读线程要获得写锁，这样就屏蔽了写操作，后续更多的读操作可以加入进来。当 &lt;code&gt;reader == 0&lt;/code&gt; 时，读线程要释放写锁。(当然，读线程修改 reader 变量的过程还需要一个锁保护)。大致的伪代码实现如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct _rwlock_t {
    sem_t reader_lock;
    sem_t writelock;
    int readers;
}rwlock_t;
void rwlock_init(rwlock_t *rw) {
    rw-&amp;gt;readers = 0;
    sem_init(&amp;amp;rw-&amp;gt;reader_lock, 0, 1); // 信号量作为睡眠锁
    sem_init(&amp;amp;rw-&amp;gt;writelock, 0, 1);   // 信号量作为睡眠锁
}
void rwlock_acquire_readlock(rwlock_t *rw) {
    sem_wait(&amp;amp;rw-&amp;gt;reader_lock);
    reader++;
    if (reader == 1)
        sem_wait(&amp;amp;rw-&amp;gt;writelock);
    sem_post(&amp;amp;rw-&amp;gt;reader_lock);
}
void rwlock_release_readlock(rwlock_t *rw) {
    sem_wait(&amp;amp;rw-&amp;gt;reader_lock);
    reader--;
    if (reader == 0)
        sem_post(&amp;amp;rw-&amp;gt;writelock);
    sem_post(&amp;amp;rw-&amp;gt;reader_lock);
}
void rwlock_acquire_writelock(rwlock_t *rw) {sem_wait(&amp;amp;rw-&amp;gt;writelock);}
void rwlock_release_writelock(rwlock_t *rw) {sem_post(&amp;amp;rw-&amp;gt;writelock);}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/rwlock.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rwlock.c&lt;/a&gt; 提供了完整的实现。&lt;/p&gt;
&lt;p&gt;该算法存在一定的公平性隐患：如果读线程比较多，源源不断地加入，写线程可能会一直调度不上。(一个可能的维护公平性的方法是为 reader 设置一个上限，如果 &lt;code&gt;reader == MAX_READER&lt;/code&gt;，则把读线程挂起。)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hill&amp;rsquo;s Law&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hill&amp;rsquo;s Law 大致的意思是在很多情况下，那些看上去简单笨拙的实现反而是好的。在这里，reader-writer lock  虽然看起来很 fancy，但实际使用时未必好过简单的自旋锁，这是因为维护精巧的结构总需要更多的开销。因此，抛开 workload 谈优化就是耍流氓。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;316-the-dining-philosophers&#34;&gt;31.6 The Dining Philosophers&lt;/h2&gt;
&lt;p&gt;一个哲学家的行为可以用如下函数描述：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void *philosopher(void *arg) {
    while (1) {
        think();
        getforks();
        eat();
        putforks();
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果我们简单地为每把叉子建立一个信号量，然后这样构建 getforks() 和 putforks()：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int left(int p) {return p;}
int right(int p) {return (p + 1) % PHI_NUM;}
void getforks(int p) {
    sem_wait(forks[left(p)]);
    sem_wait(forks[right(p)]);
}
void putforks(int p) {
    sem_post(forks[left(p)]);
    sem_post(forks[right(p)]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;容易发现这个程序有死锁风险：如果每个哲学家都拿起了自己左手的叉子，那么所有人都会等待自己右手边的叉子，而又没有任何人会放下自己左手上的叉子——死锁。
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/dining_philosophers_deadlock_print.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dining_philosophers_deadlock_print.c&lt;/a&gt; 打印出了死锁的局面。&lt;/p&gt;
&lt;p&gt;如果锁的需求链出现了环，就会发生死锁，Dijkstra 用一个简单的方法解决了这个问题：只要安排某一个哲学家先拿右手叉子再拿左手叉子，就可以破这个局：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void getforks(int p) {
    if (p == PHI_NUM) {
        sem_wait(forks[right(p)]);
        sem_wait(forks[left(p)]);
    }
    else {
        sem_wait(forks[left(p)]);
        sem_wait(forks[right(p)]);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/dining_philosophers_no_deadlock.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dining_philosophers_no_deadlock.c&lt;/a&gt; 提供了完整的实现。&lt;/p&gt;
&lt;h2 id=&#34;317-how-to-implement-semaphores&#34;&gt;31.7 How To Implement Semaphores&lt;/h2&gt;
&lt;p&gt;我们在 31.1 已经建立了信号量的语义，因此用条件变量实现信号量 (不妨称之为 zemaphore) 非常简单：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct _zem_t {
    int val;
    pthread_cond_t cond;
    pthread_mutex_t lock;
}zem_t;
void zem_init(zem_t *z, int val) {
    z-&amp;gt;val = val;
    Cond_init(&amp;amp;z-&amp;gt;cond); // 带返回值检查的 pthread_cond_init()
    Mutex_init(&amp;amp;z-&amp;gt;lock);
}
void zem_wait(zem_t *z) {
    Mutex_lock(&amp;amp;z-&amp;gt;lock);
    while (z-&amp;gt;val &amp;lt;= 0)
        Cond_wait(&amp;amp;z-&amp;gt;cond, &amp;amp;z-&amp;gt;lock);
    z-&amp;gt;val--;
    Mutex_unlock(&amp;amp;z-&amp;gt;lock);
}
void zem_post(zem_t *z) {
    Mutex_lock(&amp;amp;z-&amp;gt;lock);
    z-&amp;gt;val++;
    Cond_signal(&amp;amp;z-&amp;gt;cond);
    Mutex_unlock(&amp;amp;z-&amp;gt;lock);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/zemaphore.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zemaphore.h&lt;/a&gt; 提供了完整的实现，
&lt;a href=&#34;https://github.com/Kristoff-starling/OSTEP/blob/master/bookcode/Chapter%2031%20-%20threads_sema/zemaphore.c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zemaphore.c&lt;/a&gt; 提供了一个使用 zemaphores 的例子。&lt;/p&gt;
&lt;h2 id=&#34;318-summary&#34;&gt;31.8 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 36: I/O Devices</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch36/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch36/</guid>
      <description>&lt;p&gt;输入输出对于计算机来说非常重要：如果没有输入，计算机每次输出的都是相同的结果；如果没有输出，那我们让计算机运行程序的目的是什么呢？因此我们的问题是如何将 Input/Output 融入计算机系统。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;361-system-architecture&#34;&gt;36.1 System Architecture&lt;/h2&gt;
&lt;p&gt;一个经典的计算机系统的结构布局如下图所示：&lt;/p&gt;
&lt;img src=&#34;https://kristoff-starling.github.io/img/OSTEP-36.1.png&#34; alt=&#34;36.1&#34; style=&#34;zoom: 33%;&#34; /&gt;
&lt;p&gt;CPU 和内存通过内存总线连接。有一些设备通过通用 I/O 总线 (在现代系统中通常是 PCI 总线) 连入系统，这类设备通常是一些高速设备，比如显卡。再往下一层，一些比较低速的设备会通过外围总线 (peripheral bus，比如 SATA, USB 等) 连入系统，包括鼠标，键盘等。&lt;/p&gt;
&lt;p&gt;使用这种 hierarchical 主要是处于性价比的考量。通常来说一条总线速度越快，它的单价就越贵，它的长度也越短，上面可以插的设备数量就越少。因此系统设计者采用这种层级结构，让速度更快的设备更靠近 CPU，下方的低速总线上则可以插很多设备。&lt;/p&gt;
&lt;p&gt;现代的系统更多地采用芯片组和快速的点对点互连来提升效率。Intel Z270 芯片组的结构如下图所示：&lt;/p&gt;
&lt;img src=&#34;https://kristoff-starling.github.io/img/OSTEP-36.2.png&#34; alt=&#34;36.2&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;在这种结构中，CPU 和内存之间的连接很近，此外有一条高速的通道直接连接 CPU 和显卡，从而使 graphic-intensive 的应用有更流畅的使用体验。CPU 和 I/O 芯片之间通过一个专门的 DMI (Direct Media Interface) 连接，剩下的设备都通过 I/O 芯片与 CPU 交互。I/O 芯片提供了多种类型的接口：右侧有很多的硬盘驱动器通过 eSATA 接口与 I/O 芯片连接。下方有很多的 USB 接口，通常用于连接键盘、鼠标等低速设备。左侧的 PCIe (Peripheral Component Interconnect Express) 接口可以接一些高速设备，比如高速网卡等。&lt;/p&gt;
&lt;h2 id=&#34;362-a-canonical-device&#34;&gt;36.2 A Canonical Device&lt;/h2&gt;
&lt;p&gt;一个典型的设备通常由两个部分组成：第一个部分是它暴露给系统的硬件接口，系统可以根据设备的协议，通过这些硬件接口来操纵设备。第二部分是设备的内部结构，用于实现设备提供给系统的抽象。简单的设备可能只有一个或几个很小的芯片，但也有比较复杂的设备，里面甚至会有一个简单的 CPU，比如现代的 RAID 控制器中包含了几千行固件代码。&lt;/p&gt;
&lt;h2 id=&#34;363-the-canonical-protocol&#34;&gt;36.3 The Canonical Protocol&lt;/h2&gt;
&lt;p&gt;一个抽象的设备向外暴露的接口通常有以下部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个状态寄存器，操作系统读取它的值可以获知设备的状态；&lt;/li&gt;
&lt;li&gt;一个指令寄存器，操作系统向它写入命令可以控制设备的行为；&lt;/li&gt;
&lt;li&gt;一个数据寄存器，用于操作系统和设备的数据交换。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个典型的 OS 和设备交互的协议如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;while (STATUS == BUSY)
	;
Write data to DATA register;
Write command to COMMAND register;
	(Doing so starts the device and executes the command)
while (STATUS == busy)
    ; // wait until device is done with your request
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;整个过程分为四步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先 OS 等待直到设备的状态寄存器表示设备已经准备就绪，这个过程称为 OS 轮询 (polling) 设备。&lt;/li&gt;
&lt;li&gt;OS 将数据通过数据寄存器传送给设备，如果这个数据迁移的过程是由主 CPU 完成的，我们称这种方式为 programmed I/O (PIO)。&lt;/li&gt;
&lt;li&gt;OS 向指令寄存器写入命令，这样相当于隐式地告诉设备数据也已经传送完成，于是设备会开始工作执行这条命令。&lt;/li&gt;
&lt;li&gt;OS 再次通过轮询的方式等待设备工作完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该协议的最大优点是简单，但它的效率比较低。一个比较明显的缺点是 CPU 轮询设备状态会浪费大量的时间。&lt;/p&gt;
&lt;h2 id=&#34;364-lowering-cpu-overhead-with-interrupts&#34;&gt;36.4 Lowering CPU Overhead With Interrupts&lt;/h2&gt;
&lt;p&gt;解决轮询浪费时间问题的一个方法是使用中断。当 OS 发现当前设备正忙时，它可以将当前进程睡眠。当设备准备就绪时，设备可以生成一个硬件中断发送给操作系统，OS 中的中断处理程序会根据情况进行进程切换。&lt;/p&gt;
&lt;p&gt;中断 I/O 的方式可以在设备工作时让 OS 做别的事情，从而达到更好的并行度。但中断并不是在任何情况下都比轮询来得好。中断本身是有一定的代价的 (比如处理中断，上下文切换等)，因此对于一些工作的很快的高速设备，使用轮询反而可以获得更好的效率。对于一些工作得时快时慢的设备，OS 可以采取综合的方式：先轮询一小段时间，如果设备没有准备就绪，则走中断流程。这种两阶段的方法对于快慢两种情况都可以有不错的效果。&lt;/p&gt;
&lt;p&gt;另一个不能滥用中断的原因在网络领域。如果有大量到来的网络数据包，每个都会生成中断，那么我们的服务器就会陷入一个 livelock 的状态：OS 一直在中断处理程序中处理中断，很长时间内都没有用户台进程有进展。在这种情况下，时不时地使用一些轮询可以更好地控制当前系统中正在发生的事情：即使有大量的数据包到来，服务器也可以选择处理一部分 request，然后再去响应数据包。&lt;/p&gt;
&lt;p&gt;另一种基于中断的优化叫做 coalescing (合并)。当设备需要发出中断时，它先不急着发送而是等一小会儿，这段时间可能又有别的事件发生 (比如一个其他任务完成了)，这样我们就可以把多个事件放在一个中断里发送，提高了效率。但等待的事件如果过长中断的时效性就降低了，因此这其中也有 trade-off。&lt;/p&gt;
&lt;h2 id=&#34;365-more-efficient-data-movement-with-dma&#34;&gt;36.5 More Efficient Data Movement With DMA&lt;/h2&gt;
&lt;p&gt;Canonical Protocol 的另一个问题在于：当我们使用 programmed I/O 的方式来搬运大量数据时，大量的 CPU 时间被浪费在了这种非常重复和简单的事情上。我们希望把这部分 CPU 时间节省下来用来处理进程相关的事件。&lt;/p&gt;
&lt;p&gt;这个问题的处理方法称为 Direct Memory Access (DMA)。DMA 可以理解为一个专门用来搬运数据的设备。当 OS 需要搬运数据的时候，它会发送信号给 DMA，告诉它数据在内存中的地址，数据的长度，以及目标设备，然后 DMA 便会帮 CPU 搬数据，与此同时 CPU 就可以并行地做其他事情。&lt;/p&gt;
&lt;h2 id=&#34;366-methods-of-device-interaction&#34;&gt;36.6 Methods Of Device Interaction&lt;/h2&gt;
&lt;p&gt;OS 和设备到底是如何交互的？历史上主要有两种方法。第一种是在 ISA 中专门设计一组显式的 I/O 交互指令 (in/out)。该指令可以让 OS 指定将什么数据发送到哪个设备的哪个寄存器。这样的指令一般都是特权指令，即只有操作系统内核可以执行这样的操作，否则用户态的恶意程序很容易扰乱机器状态。第二种方法是使用所谓的内存映射 I/O (memory-mapped I/O)。这种方式下设备寄存器被绑定到特定的内存单元，操作系统可以使用普通的访存指令 (load/store) 通过特定的地址来访问设备，硬件 (MMU) 会将该访问路由到正确的设备上。&lt;/p&gt;
&lt;p&gt;这两种方法没有明显的优劣，虽然内存映射 I/O 因为没有引入新的指令看上去更简洁，但两种方法事实上现在都有在使用。&lt;/p&gt;
&lt;h2 id=&#34;367-fitting-into-the-os-the-device-driver&#34;&gt;36.7 Fitting Into The OS: The Device Driver&lt;/h2&gt;
&lt;p&gt;这个章节我们关注的问题是：不同的设备有各异的接口，我们如何让 OS 可以以一个比较统一的方式来访问设备呢？比如，文件系统可能建立在 SCSI 磁盘、IDE 磁盘、USB 设备上等等，我们希望有一个东西能帮我们抽象掉这些设备底层的细节差异，提供统一的接口 (比如 read/write)，让文件系统可以以一个统一的方式读写数据。&lt;/p&gt;
&lt;p&gt;设备驱动就是帮助我们抽象掉设备的底层细节，向上提供统一接口的系统软件。当然，盲目的抽象也有其弊端：如果一个设备有丰富的功能，那么为了使其适配简单统一的接口，设备驱动可能就不得不丢弃一些信息。比如 SCSI 磁盘有丰富的报错信息，但由于其他的块设备的 error handling 都非常简单，所以上层软件一般都只接受一个 error code，这导致 SCSI 提供的信息无法进入到文件系统。&lt;/p&gt;
&lt;p&gt;设备驱动代码占据了操作系统内核代码的一大部分，也是操作系统内核 bug 的重灾区。&lt;/p&gt;
&lt;h2 id=&#34;368-case-study-a-simple-ide-disk-driver&#34;&gt;36.8 Case Study: A Simple IDE Disk Driver&lt;/h2&gt;
&lt;p&gt;这一章节主要关注一个实际的例子：IDE 磁盘的驱动程序。IDE 磁盘提供的协议如下：&lt;/p&gt;
&lt;img src=&#34;http://kristoff-starling.github.io/img/OSTEP-36.5.png&#34; alt=&#34;36.5&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;下面是 Xv6 中关于 IDE 磁盘驱动的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static int ide_wait_ready() {
    while ((int r = inb(0x1f7)) &amp;amp; IDE_BSY || !(r &amp;amp; IDE_READY))
        ;                                       // 轮询设备的当前状态，直到设备准备就绪
}

static void ide_start_request(struct buf *b) {
    ide_wait_ready();
    outb(0x3f6, 0);                             // 生成中断信号
    outb(0x1f2, 1);                             // 要读/写的sector的个数
    outb(0x1f3, b-&amp;gt;sector &amp;amp; 0xff);              // 目标sector的LBA
    outb(0x1f4, (b-&amp;gt;sector &amp;gt;&amp;gt; 8) &amp;amp; 0xff);
    outb(0x1f5, (b-&amp;gt;sector &amp;gt;&amp;gt; 16) &amp;amp; 0xff);
    outb(0x1f6, 0xe0 | ((b-&amp;gt;dev&amp;amp;1)&amp;lt;&amp;lt;4) | ((b-&amp;gt;sector&amp;gt;&amp;gt;24)&amp;amp;0xff));
    if (b-&amp;gt;flags &amp;amp; B_DIRTY) {                   // buf的B_DIRTY位为1说明这是一次写入操作
        outb(0x1f7, IDE_CMD_WRITE);             // 向0x1f7传入写的command
        outsl(0x1f0, b-&amp;gt;data, 512/4);           // 传输要写入的数据
    } else {
        outb(0x1f7, IDE_CMD_READ);              // 向0x1f7传入读的command
    }
}

void ide_rw(struct buf *b) {
    acquire(&amp;amp;ide_lock);
    for (struct buf **pp = &amp;amp;ide_queue; *pp; pp = &amp;amp;(*pp)-&amp;gt;qnext)
        ;
    *pp = b;                                    // 将任务放到队列末尾
    if (ide_queue == b) ide_start_request(b);   // 如果该任务是当前的唯一任务，立刻开始做
    while ((b-&amp;gt;flags &amp;amp; (B_VALID|B_DIRTY)) != B_VALID)
        sleep(b, &amp;amp;ide_lock);                    // 等待任务完成时中断处理程序唤醒该进程
    release(&amp;amp;ide_lock);
}

void ide_intr() {
    struct buf *b;
    acquire(&amp;amp;ide_lock);
    if (!(b-&amp;gt;flags &amp;amp; B_DIRTY) &amp;amp;&amp;amp; ide_wait_ready() &amp;gt;= 0)
        insl(0x1f0, b-&amp;gt;data, 512/4);            // 如果是读取任务的数据准备好，则将数据读进来
   	b-&amp;gt;flags |= B_VALID;
    b-&amp;gt;flags &amp;amp;= B_DIRTY;
    wakeup(b);                                  // 唤醒对应进程，通知它任务已完成
    if ((ide_queue = b-&amp;gt;qnext) != 0)
        ide_start_request(ide_queue);           // 当前任务做完，继续做下一个任务
    release(&amp;amp;ide_lock);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;369-historical-notes&#34;&gt;36.9 Historical Notes&lt;/h2&gt;
&lt;p&gt;计算机系统的进步中没有 revolution，只有 evolution。中断、DMA 等想法都是为了提高系统性能可以自然而然想到的。&lt;/p&gt;
&lt;h2 id=&#34;3610-summary&#34;&gt;36.10 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 38: Redundant Arrays of Inexpensive Disks (RAIDs)</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch38/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch38/</guid>
      <description>&lt;p&gt;我们对磁盘有三个维度的需求：我们希望它读写速度快 (I/O 操作速度慢，因而成为整个系统的速度瓶颈)，我们希望它容量大，我们还希望它可靠 (如果发生磁盘损坏，仍然能恢复数据)。&lt;!-- more --&gt;&lt;/p&gt;
&lt;p&gt;本章节主要介绍 Redundant Arrays of Inexpensive Disks (RAIDs) 技术，它的核心思想是用多块物理磁盘去构建一个大容量的，高速的，可靠性高的磁盘。对外来看，RAID 虚拟出了一块可读可写的磁盘；在其内部，RAID 由一个非常复杂的系统构成，包含了若干个物理磁盘和一个或多个用于控制的芯片，可以说 RAID 内部就是一个小型的计算机系统。&lt;/p&gt;
&lt;p&gt;RAID 的优点在于：它可以通过多块磁盘并行的读写来提供很好的 performance；通过叠加磁盘的数量来获得 capacity；通过存储一部分的冗余数据来保证数据的 reliability。更重要的是，RAID 可以透明地提供这些优势，这里透明的意思是对于其他硬件/OS来说，RAID 看上去就像是一整块普通的磁盘。整个系统的其他部分不需要做任何修改就可以兼容 RAID。这一点极大地提升了 RAID 的可部署性 (deployability)。用户可以放心地将自己现有的磁盘更换成 RAID，不需要担心兼容问题。&lt;/p&gt;
&lt;h2 id=&#34;381-interface-and-raid-internals&#34;&gt;38.1 Interface And RAID Internals&lt;/h2&gt;
&lt;p&gt;对于上层的文件系统来说，RAID deng |看上去就像一个磁盘。和其他单块磁盘的抽象一样，RAID 对外暴露成一个线性的 block array，每个块都可读可写。&lt;/p&gt;
&lt;p&gt;当文件系统向 RAID 发送一个 logical I/O 请求时，RAID 内部需要搞清楚这个逻辑地址对应的块究竟在哪些盘的什么位置，并通过一次或多次 physical I/O 请求来完成这次任务。RAID 的内部结构相当复杂，通常会有一个 microcontroller 执行固件代码来控制 RAID 的行为；会有 DRAM 来作为数据块的 buffer cache；有时还会有一些非易失性的存储用来进行校验计算等。RAID 有一个完整计算机系统的大部分设施 (处理器，内存，磁盘等等)，但它是一个专门运行磁盘管理程序的专用系统。&lt;/p&gt;
&lt;h2 id=&#34;382-fault-model&#34;&gt;38.2 Fault Model&lt;/h2&gt;
&lt;p&gt;我们在这里考虑的错误模型是一个比较简单的错误模型，称为 fail-stop。在这个模型中，每块磁盘只会处于 working 或 failed 的状态。在 working 状态，磁盘一切正常，可读可写；在 failed 状态，磁盘损坏，可以理解为该磁盘内部的数据永久丢失。&lt;/p&gt;
&lt;p&gt;在 fail-stop 模型中，我们认为我们总是可以立刻检测到磁盘的损坏，即当某块磁盘从 working 变为 failed 时，我们可以迅速发现。因此我们不用考虑一些非常微妙的 silent failure (虽然这是实际中更可能发生的错误)。&lt;/p&gt;
&lt;h2 id=&#34;383-how-to-evaluate-a-raid&#34;&gt;38.3 How To Evaluate A RAID&lt;/h2&gt;
&lt;p&gt;我们主要从三个尺度来衡量 RAID：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容量 (capacity)：假设 RAID 中有 $N$ 块磁盘，每块磁盘中有 $B$ 个 block，那么理论上的最大存储量是 $N\cdot B$ 个 block，但由于我们需要存储一些必要的冗余数据来保证可靠性，实际上的容量达不到这个最大值。&lt;/li&gt;
&lt;li&gt;可靠性 (reliability)：我们的 RAID 系统至多可以容忍多少个磁盘同时损坏？(容忍指损坏后可以恢复数据)&lt;/li&gt;
&lt;li&gt;性能 (performance)：性能是比较难衡量的一个尺度，因为它和 workload 息息相关。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;384-raid-level-0-striping&#34;&gt;38.4 RAID Level 0: Striping&lt;/h2&gt;
&lt;p&gt;最简单的一种设计方法是将 block 以条带状放在各个物理磁盘上，下面展示了 4 块物理磁盘时的摆放顺序：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Disk 0&lt;/th&gt;
&lt;th&gt;Disk 1&lt;/th&gt;
&lt;th&gt;Disk 2&lt;/th&gt;
&lt;th&gt;Disk 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;block 以一种 round-robin 的方式放在各个磁盘上，我们称一行中的 block 为一个 stripe。这种方式的好处是我们在读写一串连续的 block 时可以达到最佳的并行性能。上面的摆放方式是 chunk size = 1 block 的方式，即每次放放一个 block 就转到下一个 disk。我们可以调整 chunk size，比如下面展示了 chunk size = 2 block 的摆放方式&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Disk 0&lt;/th&gt;
&lt;th&gt;Disk 1&lt;/th&gt;
&lt;th&gt;Disk 2&lt;/th&gt;
&lt;th&gt;Disk 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The RAID Mapping Problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在 RAID 中我们总是要处理的一个问题就是 mapping problem：给定一个逻辑块号，我们要确定它在哪一个物理磁盘上，以及在物理磁盘上的偏移量。对于 chunk size = 1 block 的 RAID 0，mapping problem 是容易的：假设逻辑块号为 $A$，则&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;Disk   = A % number_of_disks;
Offset = A / numbre_of_disks;
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;chunk-size&#34;&gt;Chunk Size&lt;/h3&gt;
&lt;p&gt;chunk size 主要影响读写的性能。小的 chunk size 可以带来更好的并行性，但每个物理磁盘上的 chunk 的个数会比较多，定位 chunk 也是需要时间代价的。&lt;/p&gt;
&lt;p&gt;下面的讨论中默认 chunk size = 1 block。&lt;/p&gt;
&lt;h3 id=&#34;back-to-raid-0-analysis&#34;&gt;Back To RAID-0 Analysis&lt;/h3&gt;
&lt;p&gt;RAID-0 的 capacity 是完美的：物理磁盘的每个 block 都被用来存储信息了；RAID-0 的 reliability 是糟糕的：任何一个磁盘的任何一个部分损坏都会导致数据丢失；RAID-0 的 performance 是比较好的，基本可以达到百分之百的带宽。&lt;/p&gt;
&lt;h3 id=&#34;evaluating-raid-performance&#34;&gt;Evaluating RAID Performance&lt;/h3&gt;
&lt;p&gt;我们衡量 RAID 的性能时通常从两种尺度出发：一种是单次请求的延迟，一种是稳定状态下的吞吐率。在衡量稳定状态下的吞吐率时，我们对两种 wordload 比较感兴趣：一种是 sequential 的，即一次性读取连续的多个 block；另一种是 random 的，即多次读取少量的 block。可以预见地，因为物理磁盘有寻道时间和旋转延迟，所以 random 的 workload 下磁盘要不停地重新定位，吞吐率会比 sequential 的情况低很多。下面的分析中，我们令 $S$ 表示 sequential 的一段长的数据的平均读取速度，$R$ 表示 random 的一块短的数据的平均读取速度。&lt;/p&gt;
&lt;h3 id=&#34;back-to-raid-0-analysis-again&#34;&gt;Back To RAID-0 Analysis, Again&lt;/h3&gt;
&lt;p&gt;从单次请求延迟的角度来看， RAID-0 的延迟基本等于一块物理磁盘的延迟，因为一个 single-block request 会被 RAID 的系统定向到某一块物理磁盘上。&lt;/p&gt;
&lt;p&gt;从稳定状态下的吞吐率来看，RAID-0 可以达到最大的带宽，因为不论是 sequential 还是 random，当读取的 block 个数足够多时，期望情况下所有的物理磁盘都在满负荷运作。即 sequential 的吞吐率为 $N\cdot S\text{ MB/s}$，random 的吞吐率为 $N\cdot R\text{ MB/s}$。&lt;/p&gt;
&lt;h2 id=&#34;385-raid-level-1-mirroring&#34;&gt;38.5 RAID Level 1: Mirroring&lt;/h2&gt;
&lt;p&gt;RAID 1 的基本思想是镜像：为每个 block 保存副本，这样我们就可以忍受 disk failure。下面是一个例子：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Disk 0&lt;/th&gt;
&lt;th&gt;Disk 1&lt;/th&gt;
&lt;th&gt;Disk 2&lt;/th&gt;
&lt;th&gt;Disk 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这种方式也被称为 RAID-10 (RAID-1+0)，因为它是先复制再按照 RAID-0 的方式排开。相应地也有 RAID-01 (RAID-0+1)，先排开再做镜像。我们所说的 RAID-1 通常指 RAID-10。&lt;/p&gt;
&lt;p&gt;RAID-1 在读取一个块时有两个选择，但 RAID-1 的写入操作必须同时修改两个副本。不过 RAID-1 的设计中一个块的两个副本位于不同的物理磁盘上，对两个副本的修改可以并行进行。&lt;/p&gt;
&lt;h3 id=&#34;raid-1-analysis&#34;&gt;RAID-1 Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;从 capacity 的角度，RAID-1 比较糟糕：只有一半的容量真正存储了信息，剩下的一半都是副本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 reliability 的角度，RAID-1 很可靠，因为任何一个物理磁盘损坏都不会导致数据丢失。甚至在上述例子中，如果 disk 0 和 disk 2 同时损坏，RAID-1 也不会丢失数据。RAID-1 reliability 的下限是 1 个磁盘，运气好的情况下甚至可以做到 $N/2$ 块磁盘。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 performance 的角度，首先考虑 latency：对于读操作来说延迟就是一块物理磁盘的延迟；写操作则有一点不同，写操作需要更新两个副本，虽然这两个写入在不同的物理磁盘上可以并行，但两块磁盘中寻道+旋转时间较长的那块磁盘将决定写操作的延迟，因此写操作延迟略高于一块物理磁盘的期望延迟。&lt;/p&gt;
&lt;p&gt;接下来考虑吞吐率。首先是 sequential 的 workload。写入方面，因为每个逻辑块的写入都要同时写入两个副本，所以吞吐率为 $\frac{N\cdot S}{2}$。一个有点反直觉的结论是：读取的吞吐率也是 $\frac{N\cdot S}{2}$。虽然我们每个块有两个副本，但精心安排读取顺序并不能给我们带来效率提升。比如我们要读取 0, 1, 2, 3, 4, 5, 6, 7，如果我们只使用 Disk 0 和 Disk 2，吞吐率显然为 $\frac{N\cdot S}{2}$；我们可能会想出这样的安排方案：Disk 0 和 Disk 2 读 0 和 1，Disk 1 和 Disk 3 读 2 和 3，后面依次类推，这样是不是可以用满带宽呢？实际上不然。我们考虑一个 Disk 接受到的任务，比如 Disk 0：它要读取 0, 4, 8&amp;hellip;&amp;hellip; 这些 block，这些 block 在 Disk 0 上不是连续存放的，所以磁盘在旋转的时候每读取一个块，就要等待一个块，磁盘划过不需要的块的时候并没有向用户输出有效的带宽，因此它实际上只输出了一半的效率。&lt;/p&gt;
&lt;p&gt;在 random 的 workload 下，RAID-1 的读取是完美的：Disk 2 和 Disk 4 作为 Disk 0 和 Disk 3 的副本也可以为用户提供带宽，因此聪明的选择当前空闲的磁盘可以使吞吐率达到 $N\cdot R$。写入方面，因为要同时修改两个磁盘中的副本，所以吞吐率是 $\frac{N\cdot R}{2}$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The RAID Consistent-Update Problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RAID-1 中涉及 mirroring，在写入操作时要同时更新两个副本，因此存在所谓的 consistent-update 问题：如果我们无脑地修改第一个部分再修改第二个副本，那么加入修改完第一个副本后系统掉电了，那么重启后整个磁盘将处于一个 inconsistent 的状态——两个副本的内容不一致。因此修改两个副本的操作应当是原子的。&lt;/p&gt;
&lt;p&gt;在文件系统中我们用 journaling 的方法来解决原子性的问题，在 RAID 中该方法同样适用。我们只要保存一个修改操作的 logging，意外掉电重启后就可以根据 logging 的内容来做恢复。值得一提的是，每次写入操作都在 disk 中做 logging 的代价太大无法忍受，因此 RAID 中一般会有一小块 non-volatile 的 RAM 用于写入 logging。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;386-raid-level-4-saving-space-with-parity&#34;&gt;38.6 RAID Level 4: Saving Space With Parity&lt;/h2&gt;
&lt;p&gt;镜像所需要的额外存储空间太多。事实上，我们可以使用校验码的思路来为数据保存副本，这就是 RAID-4。RAID-4 的一个例子如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Disk 0&lt;/th&gt;
&lt;th&gt;Disk 1&lt;/th&gt;
&lt;th&gt;Disk 2&lt;/th&gt;
&lt;th&gt;Disk 3&lt;/th&gt;
&lt;th&gt;Disk 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;P0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;P1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;P2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;P3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;其中 Disk 4 上存储的都是前 4 个 disk 的校验码：$P0 = 0\oplus 1\oplus 2\oplus 3$，依此类推。这样如果有任何一个磁盘损坏，将其他 4 个磁盘的数据异或起来就可以恢复该磁盘的数据。&lt;/p&gt;
&lt;h3 id=&#34;raid-4-analysis&#34;&gt;RAID-4 Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;从 capacity 的角度：它比 RAID-1 好很多：$\frac{N-1}{N}$ 的空间都可以用来存储有效的信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 reliability 的角度：RAID-4 可以容忍一个磁盘的损坏。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 performance 的角度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先考虑吞吐率：sequential 的情况是比较容易分析的：读取的时候，只有 $N-1$ 个磁盘中存储的是有效数据，所以带宽为 $(N-1)\cdot S$。写入的时候，每一个 stripe ($N-1$ 个 block) 的数据可以由 $N$ 个磁盘并行写入，因此带宽也是 $(N-1)\cdot S$。random 的读取也容易分析：只有 $N-1$ 个存储了有效数据的磁盘会工作，因此带宽为 $(N-1)\cdot R$。&lt;/p&gt;
&lt;p&gt;困难的是 random 写入的分析。我们在修改一个 block 的时候同时也要修改它所在的 stripe 的校验 block。我们有两种思路来计算新的校验值：一是 addictive parity，即把该 stripe 中其他的 block 都读出来，然后计算新校验值并写入，该方法的问题是其代价随着 RAID 中磁盘的上升而上升 (要做很多异或)；二是 subtractive parity，即读出旧的校验值，异或掉旧的 block，再异或上新的 block 以获得新的校验值。写成公式就是 $P_{new}=P_{old}\oplus(C_{old}\oplus C_{new})$，该方法需要对校验 block 和数据 block 各做一次读和一次写。通常来说我们选择 subtractive parity，但该方法的关键问题在于不管我们修改哪个物理磁盘上的数据，都要读写校验磁盘，因此即使数据磁盘的修改可以并行，校验磁盘仍然会将整个过程变成串行，它成为了整个系统的瓶颈。这个问题被成为 small-write problem。RAID-4 在 random 写入下的带宽是 $(R/2)$，这非常糟糕，因为它不随着 RAID 规模的增大而提高。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接着分析 latency：读一个 block 的延迟和一块物理磁盘上的延迟相同；写一个 block 的延迟则复杂一些：根据之前的 subtractive parity 的方法，我们要读一次写一次数据磁盘和校验磁盘，因此延迟大约是一块物理磁盘延迟的两倍。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;387-raid-level-5-rotating-parity&#34;&gt;38.7 RAID Level 5: Rotating Parity&lt;/h2&gt;
&lt;p&gt;RAID-5 针对 RAID-4 的 small-write problem 做出了改进，把校验 block 分散到了各个磁盘当中：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Disk 0&lt;/th&gt;
&lt;th&gt;Disk 1&lt;/th&gt;
&lt;th&gt;Disk 2&lt;/th&gt;
&lt;th&gt;Disk 3&lt;/th&gt;
&lt;th&gt;Disk 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;P0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;P1&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;P2&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;P3&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;P4&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;raid-5-analysis&#34;&gt;RAID-5 Analysis&lt;/h3&gt;
&lt;p&gt;RAID-5 很多方面的参数和 RAID-4 是差不多的。这里主要关注 RAID-5 在 random workload 下的吞吐率：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于读取操作，由于现在 $N$ 个磁盘上都存储了数据，所以带宽可以达到 $N\cdot R$。&lt;/li&gt;
&lt;li&gt;对于写入操作，RAID-5 相比较于 RAID-4 有很大改善。我们可以认为在 random request 足够多的情况下，所有的物理磁盘都在满负荷运转，因此带宽可以达到 $\frac{N\cdot R}{4}$，这里要除以 4 是因为一个数据块的写入要涉及 4 次 I/O 操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在绝大多数场合下，RAID-5 已经完全取代了 RAID-4。除了某些特殊的场景，使用者确定不会出现大量的随机读写，这时使用 RAID-4 会使磁盘在结构上简单一些。&lt;/p&gt;
&lt;h2 id=&#34;388-raid-comparison-a-summary&#34;&gt;38.8 RAID Comparison: A Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;RAID-0&lt;/th&gt;
&lt;th&gt;RAID-1&lt;/th&gt;
&lt;th&gt;RAID-4&lt;/th&gt;
&lt;th&gt;RAID-5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Capacity&lt;/td&gt;
&lt;td&gt;$N\cdot B$&lt;/td&gt;
&lt;td&gt;$(N\cdot B)/2$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot B$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot B$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reliability&lt;/td&gt;
&lt;td&gt;$0$&lt;/td&gt;
&lt;td&gt;$1$ (for sure)&lt;br&gt;$N/2$ (if lucky)&lt;/td&gt;
&lt;td&gt;$1$&lt;/td&gt;
&lt;td&gt;$1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Throughput&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sequential Read&lt;/td&gt;
&lt;td&gt;$N\cdot S$&lt;/td&gt;
&lt;td&gt;$(N\cdot S)/2$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot S$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot S$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sequential Write&lt;/td&gt;
&lt;td&gt;$N\cdot S$&lt;/td&gt;
&lt;td&gt;$(N\cdot S)/2$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot S$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot S$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Read&lt;/td&gt;
&lt;td&gt;$N\cdot R$&lt;/td&gt;
&lt;td&gt;$N\cdot R$&lt;/td&gt;
&lt;td&gt;$(N-1)\cdot R$&lt;/td&gt;
&lt;td&gt;$N\cdot R$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Write&lt;/td&gt;
&lt;td&gt;$N\cdot R$&lt;/td&gt;
&lt;td&gt;$(N\cdot R)/2$&lt;/td&gt;
&lt;td&gt;$R/2$&lt;/td&gt;
&lt;td&gt;$(N\cdot R)/4$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Latency&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Read&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Write&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;$2T$&lt;/td&gt;
&lt;td&gt;$2T$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如果你想要极致的性能，不在乎数据的可靠性，那么就选 RAID-0；如果你在乎 random I/O 的效率且需要可靠性，那么就选 RAID-1 (代价是容量)；如果 reliability 和 capacity 你都在乎，那么就选 RAID-5 (代价是 small-write performance)。&lt;/p&gt;
&lt;h2 id=&#34;389-other-interesting-raid-issues&#34;&gt;38.9 Other Interesting RAID Issues&lt;/h2&gt;
&lt;p&gt;关于 RAID 还有很多有意思的问题可以研究，比如在发生 failure 的时候系统会经历怎样的运作过程，这时候的性能会有什么变化等等。此外，我们也可以提出一些更贴合实际的 fault model，以考虑到 block corruption，latent sector error 等等。甚至有人将 RAID system 放到软件层面。&lt;/p&gt;
&lt;h2 id=&#34;3810-summary&#34;&gt;38.10 Summary&lt;/h2&gt;
&lt;p&gt;RAID 的核心思想是将许多块不太可靠的物理磁盘组合在一起形成一个又大又快又可靠的磁盘。RAID 的选择与使用场景的 workload 息息相关，并不是说 RAID-5 就一定比 RAID-1 来的好。因此选择合适的 RAID 模型并为其调整合适的参数 (比如 chunk size，物理磁盘个数等) 是一门艺术。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 40: File System Implementation</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/ch40/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/ch40/</guid>
      <description>&lt;p&gt;文件系统是一个纯粹的软件，因此在本章节中我们不考虑加入任何的硬件 feature 使文件系统工作得更好 (当然我们还是会注意块设备本身的特性)。文件系统设计本身有很大的弹性，因此现存的文件系统很多，它们使用不同的数据结构，在各方面的表现也各有千秋。&lt;!-- more --&gt;&lt;/p&gt;
&lt;h2 id=&#34;401-the-way-to-think&#34;&gt;40.1 The Way To Think&lt;/h2&gt;
&lt;p&gt;设计文件系统我们通常考虑两件事情：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据结构：我们准备使用什么数据结构来组织磁盘上的数据和元数据？在简单的文件系统实现中我们通常使用简单的块链表，在一些比较精密的文件系统实现中也有使用树状结构的。&lt;/li&gt;
&lt;li&gt;访问方式：进程使用的 open(), read(), write() 等函数该如何对应到文件系统的结构上？对于一个特定的系统调用，哪些数据需要被读写？每一步的效率如何？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;402-overall-organization&#34;&gt;40.2 Overall Organization&lt;/h2&gt;
&lt;p&gt;vsfs (very simple file system，一个 UNIX 文件系统的精简版本) 的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://kristoff-starling.github.io/img/OSTEP-vsfs.png&#34; alt=&#34;vsfs&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;整个磁盘被划分成 64 个 block，每个 block 的大小是 4KB。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后面的 56 个 block 是 data region，用来存储用户数据。&lt;/li&gt;
&lt;li&gt;3~7 这 5 个 block 是 inode table 区，存储了一个 inode 数组，注意到一个 inode 通常没有一个 block 那么大——只有 128 或 256 字节，这里假设 256 字节，则 5 个 block 可以存储 80 个 inode，即我们的文件系统中最多可以有 80 个文件 (在更大的磁盘上，我们的 inode table 可以更大，从而可以存储更多的文件)。&lt;/li&gt;
&lt;li&gt;block 1 是 inode bitmap，block 2 是 data bitmap，bitmap 存储了每个 inode/data block 处于空闲状态还是正在使用的状态。&lt;/li&gt;
&lt;li&gt;block 0 是 superblock，存储了文件系统的元数据，比如 inode 的个数，磁盘的大小，inode table 的起始位置，文件系统的魔数等等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;403-file-organization-the-inode&#34;&gt;40.3 File Organization: The Inode&lt;/h2&gt;
&lt;p&gt;几乎所有的文件系统都有类似于 inode 的结构：它保存了一个文件的元数据，比如大小、权限等等。inode 的全称是 index node，这是因为一般 inode 被整齐地存放在一个数组里面，因此给定一个下标，我们很容易索引到一个对应的 inode。以我们的 vsfs 为例，inode table 的起始地址 &lt;code&gt;inodeStartAddr = 12KB&lt;/code&gt;，每个 inode 的大小是 256B，因此给定一个 inumber，我们有如下的计算公式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;blk = (inumber * sizeof(inode_t)) / blockSize;
sector = ((blk * blockSize) + inodeStartAddr) / sectorSize;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(注：磁盘不是 byte addressable 的，所以我们只算出精确的地址没有用，而要算出它在哪个 sector 中，把整个 sector 读出来，再根据 offset 定位 inode。)&lt;/p&gt;
&lt;p&gt;inode 中有关于一个文件的所有信息：比如它的类型 (文件/文件夹/设备 etc.)，它的大小，它包含的 block 数目，它的访问权限，它的创建/修改日期等。这些信息通常被称为文件的元数据 (metadata) (通常文件系统中和用户数据无关的其他数据都被称为元数据)。下面展示了一个简化的 ext2 文件系统的 inode 中存储的元数据：&lt;/p&gt;
&lt;img src=&#34;https://kristoff-starling.github.io/img/OSTEP-40.1.png&#34; alt=&#34;40.1&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;inode 中最重要的部分就是指示 data block 位置的部分。一种简单的方法是在 inode 中存储若干个 direct pointers，每个指针保存一个 data block 的地址。这种方法的局限性在于无法保存大文件的所有 data block。&lt;/p&gt;
&lt;h3 id=&#34;the-multi-level-index&#34;&gt;The Multi-Level Index&lt;/h3&gt;
&lt;p&gt;解决大文件存储的方法通常是所谓的 indirect pointer：我们从数据区分配一个 block，让 inode 中的 indirect pointer 指向这个 block，然后这个 block 里存放一堆 direct pointer。假设一个地址占 4B，那么一个 block 里可以存放 1024 个 direct pointer，这意味着只需要一个 indirect pointer 就可以记录一个 1024*4 KB 的文件的所有 data block 的位置。&lt;/p&gt;
&lt;p&gt;这个思想有点像页表。正如页表可以有多级，文件系统中我们也可以有 double indirect pointer 和 triple indirect pointer，这样我们用树状结构存下了很多 data block 的地址。值得一提的是，通常 inode 里会有 12 个左右的 direct pointer 和一个 indirect pointer，存放如此多的 direct pointer 是因为从统计规律上，大部分的文件都非常小，用 direct pointer 直接存地址可以略去遍历“页表”的过程，更加高效。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Linked-Based Approaches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;inode 的另一种常见的设计方案是使用链表。在这种设计下我们不需要在 inode 中存储所有 data block 的地址，而只要存储第一个 data block 的地址。每个 data block 自己有一个 next 指针存储下一个 data block 的地址，这样也可以存储大文件。&lt;/p&gt;
&lt;p&gt;朴素的想法是将每个 data block 的 next 指针存储在 data block 内部。但这样文件系统在 random access 的 workload 下表现得会很坏：注意到磁盘是一个块设备，如果我们要访问一个文件尾部的内容，我们就必须顺着链表往后找，而每次我们要获得一个 next 指针都要把整个 data block 读出来，大量的磁盘读取会使得访问非常慢。&lt;/p&gt;
&lt;p&gt;一个优化是把所有的 next 指针放在一起做成一张表集中存储。这样我们只要将这个 next 指针表从磁盘读进内存，就可以定位一个大文件的任何一个 data block 的位置，再去磁盘中抓取对应的 data block 即可。&lt;/p&gt;
&lt;p&gt;这就是 FAT 的主要思想。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;404-directory-organization&#34;&gt;40.4 Directory Organization&lt;/h2&gt;
&lt;p&gt;在很多文件系统中，目录文件的内容就是一系列的 &lt;code&gt;(文件名, inode)&lt;/code&gt; 键值对。举一个例子，比如一个 inode 为 5 的目录文件 dir 中包含了文件 foo, bar 和 foobar_is_a_pretty_longname，那么 dir 文件的内容就大约长这样：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;inum&lt;/th&gt;
&lt;th&gt;reclen&lt;/th&gt;
&lt;th&gt;strlen&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;..&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;foo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;bar&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;foobar_is_a_pretty_longname&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这里的 strlen 表示文件名的实际长度 (包括末尾的 &lt;code&gt;\0&lt;/code&gt;)，reclen 则表示当前分配给这个目录项文件名的长度 (名字的长度和可能存在的若干空闲空间)。每个目录文件中都有两个文件：&lt;code&gt;.&lt;/code&gt; 表示当前目录，&lt;code&gt;..&lt;/code&gt; 表示上一级目录。&lt;/p&gt;
&lt;p&gt;如果我们在一个目录下删去一个文件，那么它对应的目录项就会被挖空。通常我们会使用 inode 0 来表示该目录项是空闲的。删除也是我们留有 reclen 字段的原因：比如一个长名文件被删除后，一个短名文件被加进目录，那么新文件可以复用旧文件的目录项，包括之前分配的长文件名的空间，因此文件名后可能会有空格。&lt;/p&gt;
&lt;p&gt;目录文件也是一种文件，因此在文件系统中它也有对应的 inode，上面描述的这些键值对就存在该文件的 data block 中。值得一提的是并不是所有文件系统都使用这种顺序列表的方式存储键值对——数据结构有很多的选择空间，比如有的文件系统使用 B 树存储键值对，这样它们在搜索文件名时就可以避免穷举遍历。&lt;/p&gt;
&lt;h2 id=&#34;405-free-space-management&#34;&gt;40.5 Free Space Management&lt;/h2&gt;
&lt;p&gt;在 vsfs 中，我们为 inode table 和 data region 各准备了一个 bitmap 来记录每个 inode/data block 是否处于空闲状态。分配的时候，我们遍历 bitmap 寻找空闲的 inode/data block 即可。&lt;/p&gt;
&lt;p&gt;分配中也有一些“花活”可以玩，比如 ext2/ext3 文件系统中，当一个新文件被创建且需要数据块时，文件系统会去寻找空闲的连续数据块 (8 个或更多)，这样文件在文件系统中存储的更加连续，有助于提升性能。&lt;/p&gt;
&lt;h2 id=&#34;406-access-paths-reading-and-writing&#34;&gt;40.6 Access Paths: Reading and Writing&lt;/h2&gt;
&lt;h3 id=&#34;reading-a-file-from-disk&#34;&gt;Reading A File From Disk&lt;/h3&gt;
&lt;p&gt;我们首先考虑这样一个系统调用：&lt;code&gt;open(&amp;quot;/foo/bar&amp;quot;, O_RDONLY)&lt;/code&gt;。我们的目标是找到 bar 文件的 inode，但这是无法直接做到的，因此我们必须得通过文件名一层一层地去找。一个文件的 inode 编号存放在它的父目录的文件内容中，因此我们需要去读取 &lt;code&gt;/foo&lt;/code&gt; 目录文件的内容，从而需要其 inode 编号，再向上我们需要 &lt;code&gt;/&lt;/code&gt; 目录文件的内容，以及 &lt;code&gt;/&lt;/code&gt; 的 inode 编号。&lt;/p&gt;
&lt;p&gt;根目录没有父目录，因此根目录文件的 inode 编号必须被显式或隐式地规定。在绝大多数的 UNIX 文件系统中，根目录文件的 inode 编号都是 2。因此整个过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读取根目录的 inode 2，根据 inode 中的指针找到根目录文件内容，从目录项中找到 &lt;code&gt;/foo&lt;/code&gt; 文件的 inode 号。&lt;/li&gt;
&lt;li&gt;读取 &lt;code&gt;/foo&lt;/code&gt; 的 inode，根据 inode 中的指针找到 &lt;code&gt;/foo&lt;/code&gt; 文件内容，从目录项中找到 &lt;code&gt;/foo/bar&lt;/code&gt; 的 inode 号。&lt;/li&gt;
&lt;li&gt;读取 &lt;code&gt;/foo/bar&lt;/code&gt; 的 inode，进行一系列权限检查，返回给当前进程一个指向 &lt;code&gt;/foo/bar&lt;/code&gt; 的文件描述符。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;打开文件后，程序就可以通过 read() 系统调用来读取文件内容。对于一次 read() 系统调用，文件系统首先要读取 inode，根据 read 的 offset 找到对应 data block 的地址，然后读取对应的 data block，最后还要修改文件的 inode，更新最近访问时间等字段。在文件系统之外，文件描述符的 offset 也要更新。&lt;/p&gt;
&lt;p&gt;在某个时刻，该文件会被关闭。关闭文件需要释放掉对应的文件描述符，不过这不是文件系统层面的动作，close() 的时候没有任何的 disk I/O 操作。&lt;/p&gt;
&lt;p&gt;下图展示了整个过程各部分数据的读写情况：&lt;/p&gt;
&lt;img src=&#34;https://kristoff-starling.github.io/img/OSTEP-40.3.png&#34; alt=&#34;40.3&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;h3 id=&#34;writing-a-file-to-disk&#34;&gt;Writing A File To Disk&lt;/h3&gt;
&lt;p&gt;写入的过程和读取的过程基本类似，但写入更麻烦的地方在于我们有时候要分配新的 inode/data block，因此还要读写 bitmap。通常来说在文件已经打开的情况下，一次 write 操作要对应 5 次磁盘 I/O 操作：一次读取数据区的 bitmap (寻找可用数据块)，一次写入数据区的 bitmap (更新使用情况)，读取和写入该文件的 inode，以及对数据块的写入。&lt;/p&gt;
&lt;p&gt;如果我们考虑文件的创建，涉及到的 disk I/O 次数则更多：创建文件时我们要为文件创建 inode，因此我们需要读写 inode 的 bitmap；我们要初始化新分配的 inode，因此需要写入新文件的 inode；我们需要更新该文件的父目录的键值对，因此我们要修改父目录的 data block，从而还要读写父目录文件的 inode。如果父目录文件的 data block 容量不足，我们还要为父目录文件分配新的 data block，这又要涉及数据区 bitmap 的读写……&lt;/p&gt;
&lt;p&gt;由此我们可以看出，文件系统 disk I/O 的次数非常多，负担很重，我们需要想办法让频繁的 I/O 更加高效。&lt;/p&gt;
&lt;h2 id=&#34;407-caching-and-buffering&#34;&gt;40.7 Caching and Buffering&lt;/h2&gt;
&lt;p&gt;为了缓解文件系统大量的磁盘 I/O 开销，大多数文件系统使用内存作为磁盘的 cache，在内存中保存一些常用的/重要的 block。早期的文件系统使用固定大小的 cache，通常是 DRAM 的 10%，这种静态分区的做法不太高效——当前空闲的文件系统 cache slot 无法作为其他东西使用。因此现代的文件系统采用 dynamic partitioning 的方法，把虚拟内存的页面和文件系统的页面放在一起统一管理。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Understand Static VS. Dynamic Partitioning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当我们将资源分成若干种不同用途的时候，我们通常有静态和动态两种方法。静态方法提前将资源划分成固定的比例，每种用途取一份；动态方法则根据当前的 workload 动态调整每种用途的使用量。静态实现简单，效率更稳定；而动态可以达到更好的资源利用率，但实现起来比较复杂。两者各有千秋。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简单分析 caching 对于文件系统读写的好处：在读取方面，如果我们遍历一个目录下的所有文件，那么目录文件以及其 inode 就可以一直放在 cache 中供读取，节省了很多 I/O 操作；在写入方面，write buffering 带来的延迟写入可以将多次操作 batch 在一起，减少 I/O 次数 (比如创建文件再删除=什么都不用做)，系统还可以调度多次写入的顺序以获得更好的效率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Durability/Performance Trade-Off&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;存储系统通常面临 durability/performance trade-off。如果用户希望写入的数据能立刻变得持久，那么文件系统就必须将新数据立刻落盘，但这样会很慢；如果用户可以忍受小量的数据丢失，那么文件系统就可以将数据在内存中放一会儿，每隔一段时间落盘一次，这样效率会有明显提升。至于如何在 trade-off 中选择，这与用户需求息息相关。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一些应用 (比如数据库) 不喜欢这种 trade-off，因此它们会通过使用 fsync()，或者跳过文件系统层直接使用磁盘 I/O API 等方式来强制落盘。它们牺牲了效率但获得了稳定性。&lt;/p&gt;
&lt;h2 id=&#34;408-summary&#34;&gt;40.8 Summary&lt;/h2&gt;
&lt;p&gt;略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Homework 26</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/hw26/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/hw26/</guid>
      <description>&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Let’s examine a simple program, “loop.s”. First, just read and understand it. Then, run it with these arguments (&lt;code&gt;./x86.py -p loop.s -t 1 -i 100 -R dx&lt;/code&gt;) This specifies a single thread, an interrupt every 100 instructions, and tracing of register %dx. What will %dx be during the run? Use the -c flag to check your answers; the answers, on the left, show the value of the register (or memory value) after the instruction on the right has run. &lt;!-- more --&gt;&lt;/li&gt;
&lt;li&gt;Same code, different flags: (&lt;code&gt;./x86.py -p loop.s -t 2 -i 100 -a dx=3,dx=3 -R dx&lt;/code&gt;) This specifies two threads, and initializes each %dx to 3. What values will %dx see? Run with -c to check. Does the presence of multiple threads affect your calculations? Is there a race in this code?&lt;/li&gt;
&lt;li&gt;Run this: &lt;code&gt;./x86.py -p loop.s -t 2 -i 3 -r -a dx=3,dx=3 -R dx&lt;/code&gt; This makes the interrupt interval small/random; use different seeds (-s) to see different interleavings. Does the interrupt frequency change anything?&lt;/li&gt;
&lt;li&gt;Now, a different program, looping-race-nolock.s, which accesses a shared variable located at address 2000; we’ll call this variable value. Run it with a single thread to confirm your understanding: &lt;code&gt;./x86.py -p looping-race-nolock.s -t 1 -M 2000 &lt;/code&gt; What is value (i.e., at memory address 2000) throughout the run? Use -c to check.&lt;/li&gt;
&lt;li&gt;Run with multiple iterations/threads: &lt;code&gt;./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000&lt;/code&gt; Why does each thread loop three times? What is final value of value?&lt;/li&gt;
&lt;li&gt;Run with random interrupt intervals: &lt;code&gt;./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0&lt;/code&gt; with different seeds (-s 1, -s 2, etc.) Can you tell by looking at the thread interleaving what the final value of value will be? Does the timing of the interrupt matter? Where can it safely occur? Where not? In other words, where is the critical section exactly?&lt;/li&gt;
&lt;li&gt;Now examine fixed interrupt intervals: &lt;code&gt;./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1&lt;/code&gt; What will the final value of the shared variable value be? What about when you change -i 2, -i 3, etc.? For which interrupt intervals does the program give the “correct” answer?&lt;/li&gt;
&lt;li&gt;Run the same for more loops (e.g., set -a bx=100). What interrupt intervals (-i) lead to a correct outcome? Which intervals are surprising?&lt;/li&gt;
&lt;li&gt;One last program: wait-for-me.s. Run: &lt;code&gt;./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000&lt;/code&gt; This sets the %ax register to 1 for thread 0, and 0 for thread 1, and watches %ax and memory location 2000. How should the code behave? How is the value at location 2000 being used by the threads? What will its final value be?&lt;/li&gt;
&lt;li&gt;Now switch the inputs: &lt;code&gt;./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000&lt;/code&gt; How do the threads behave? What is thread 0 doing? How would changing the interrupt interval (e.g., &lt;code&gt;-i 1000&lt;/code&gt;, or perhaps to use random intervals) change the trace outcome? Is the program efficiently using the CPU?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;%dx 寄存器初始值为 0，执行一次 sub 指令后变为 -1，随后跳出循环。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;%dx 从 3 开始每做一次循环 -1,直到变为负数后退出。两个线程的行为是完全一样的。多线程没有影响计算，这里没有出现竞争条件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中断的频率不会对线程行为产生任何影响。&lt;code&gt;loop.s&lt;/code&gt; 中只有针对寄存器的行为，不同线程的寄存器之间是互相独立的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;循环只做了一次，地址 2000 处最终数值为 1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;%bx 寄存器的初始值为 3，所以每个线程做三次循环，最终地址 2000 处的值为 6。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用 0 和 2 做种子可以得到正确的结果 2，因为两个线程临界区域的代码没有交叉：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;# seed = 0
 2000      bx          Thread 0                Thread 1         
    0       0   
    0       0   1000 mov 2000, %ax
    0       0   1001 add $1, %ax
    1       0   1002 mov %ax, 2000
    1      -1   1003 sub  $1, %bx
    1       0   ------ Interrupt ------  ------ Interrupt ------  
    1       0                            1000 mov 2000, %ax
    1       0                            1001 add $1, %ax
    2       0                            1002 mov %ax, 2000
    2      -1                            1003 sub  $1, %bx
    2      -1   ------ Interrupt ------  ------ Interrupt ------  
    2      -1   1004 test $0, %bx
    2      -1   1005 jgt .top
    2      -1   ------ Interrupt ------  ------ Interrupt ------  
    2      -1                            1004 test $0, %bx
    2      -1                            1005 jgt .top
    2      -1   ------ Interrupt ------  ------ Interrupt ------  
    2      -1   1006 halt
    2      -1   ----- Halt;Switch -----  ----- Halt;Switch -----  
    2      -1                            1006 halt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 1 做种子会得到错误的结果 1，因为两个线程的临界区域出现了交叉 (第一个线程做了第一条指令后切换到了第二个线程)。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt; 2000      bx          Thread 0                Thread 1         
    0       0   
    0       0   1000 mov 2000, %ax
    0       0   ------ Interrupt ------  ------ Interrupt ------  
    0       0                            1000 mov 2000, %ax
    0       0                            1001 add $1, %ax
    1       0                            1002 mov %ax, 2000
    1      -1                            1003 sub  $1, %bx
    1       0   ------ Interrupt ------  ------ Interrupt ------  
    1       0   1001 add $1, %ax
    1       0   1002 mov %ax, 2000
    1      -1   1003 sub  $1, %bx
    1      -1   1004 test $0, %bx
    1      -1   ------ Interrupt ------  ------ Interrupt ------  
    1      -1                            1004 test $0, %bx
    1      -1                            1005 jgt .top
    1      -1   ------ Interrupt ------  ------ Interrupt ------  
    1      -1   1005 jgt .top
    1      -1   1006 halt
    1      -1   ----- Halt;Switch -----  ----- Halt;Switch -----  
    1      -1   ------ Interrupt ------  ------ Interrupt ------  
    1      -1                            1006 halt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;只有两个线程在执行临界区域代码时保持原子性 (不被打断)，才能得到正确的结果。临界区域为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;.main
.top
# &amp;lt;-- critical section begin --&amp;gt;
mov 2000, %ax
add $1, %ax
mov %ax, 2000
# &amp;lt;--  critical section end  --&amp;gt;
sub $1, %bx
test $0, %bx
jgt .top
halt
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中断步长为 1 和 2 时无法获得正确的结果 (临界区域交织)。在只循环一次的情况下，中断步长大于等于 3 即可保证结果正确。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;保证执行结果正确有以下几种思路：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只要设置 &lt;code&gt;-i&lt;/code&gt; 参数足够大以使得两个线程串行地执行，就可以保证结果正确。&lt;/li&gt;
&lt;li&gt;该汇编代码执行一个循环一共有 6 条语句，其中临界区域有 3 条语句。因此只要设置 &lt;code&gt;-i&lt;/code&gt; 参数为 3 的倍数，就可以保证临界区域不会交叉。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于那些不是 3 的倍数的参数，在参数较小时尚可预估 (比如如果设置 &lt;code&gt;-i 1/2&lt;/code&gt;，则每个临界区域都会交织，最后答案为 N，&lt;code&gt;-i 4&lt;/code&gt; 最后的结果会是 3N/2 上取整)，参数较大时得到的结果就难以理解了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;wait-for-me.s&lt;/code&gt; 代码会根据 %ax 的值选择 waiter 或 signaller 身份。waiter 等待地址 2000 处值为 1结束，signaller 负责向 2000 处放一个 1。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;.main
test $1, %ax     # ax should be 1 (signaller) or 0 (waiter)
je .signaller

.waiter	
mov  2000, %cx
test $1, %cx
jne .waiter
halt

.signaller
mov  $1, 2000
halt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行指令后会看到第一个线程放 1 结束，第二个线程看到有 1 结束。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果反过来，则会看到线程 1 在轮询地址 2000 处的值，直到发生线程切换，线程 2 向 2000 处填写了 1，线程 1 才得以退出。这是自旋锁的雏形。如果将 &lt;code&gt;-i&lt;/code&gt; 调得很大，线程 1 将浪费 CPU cycle 进行大量轮询。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Homework 27</title>
      <link>https://kristoff-starling.github.io/notes/booknotes/ostep/hw27/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://kristoff-starling.github.io/notes/booknotes/ostep/hw27/</guid>
      <description>&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;First build main-race.c. Examine the code so you can see the (hopefully obvious) data race in the code. Now run helgrind (by typing valgrind &amp;ndash;tool=helgrind main-race) to see how it reports the race. Does it
point to the right lines of code? What other information does it give to you?&lt;/li&gt;
&lt;li&gt;What happens when you remove one of the offending lines of code? Now add a lock around one of the updates to the shared variable, and then around both. What does helgrind report in each of these cases?&lt;/li&gt;
&lt;li&gt;Now let’s look at main-deadlock.c. Examine the code. This code has a problem known as deadlock (which we discuss in much more depth in a forthcoming chapter). Can you see what problem it might have?&lt;/li&gt;
&lt;li&gt;Now run helgrind on this code. What does helgrind report?&lt;/li&gt;
&lt;li&gt;Now run helgrind on main-deadlock-global.c. Examine the code; does it have the same problem that main-deadlock.c has? Should helgrind be reporting the same error? What does this tell you about tools like helgrind?&lt;/li&gt;
&lt;li&gt;Let’s next look at main-signal.c. This code uses a variable (done) to signal that the child is done and that the parent can now continue. Why is this code inefficient? (what does the parent end up spending its time doing, particularly if the child thread takes a long time to complete?)&lt;/li&gt;
&lt;li&gt;Now run helgrind on this program. What does it report? Is the code correct?&lt;/li&gt;
&lt;li&gt;Now look at a slightly modified version of the code, which is found in main-signal-cv.c. This version uses a condition variable to do the signaling (and associated lock). Why is this code preferred to the previous
version? Is it correctness, or performance, or both?&lt;/li&gt;
&lt;li&gt;Once again run helgrind on main-signal-cv. Does it report any errors?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;helgrind 工具可以准确地报告出 &lt;code&gt;main-race.c&lt;/code&gt; 的第 15 行和第 8 行的操作存在 data race。此外，helgrind 可以报告出产生数据竞争的地址 (0 bytes inside data symbol &amp;ldquo;balance&amp;rdquo;)，还可以报告线程创建的过程：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---Thread-Announcement------------------------------------------
Thread #2 was created
   at 0x49A9D42: clone (clone.S:71)
   by 0x4878281: create_thread (createthread.c:103)
   by 0x4879C8B: pthread_create@@GLIBC_2.2.5 (pthread_create.c:821)
   by 0x484D627: ??? (in /usr/libexec/valgrind/vgpreload_helgrind-amd64-linux.so)
   by 0x109209: main (main-race.c:14)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果删除掉任意一个对 balance 的操作，helgrind 就不会报告错误。如果只用锁保护一处操作，helgrind 仍然会报告 data race (且能报告处哪一处操作被锁保护了)。如果用锁将两处操作都保护了，helgrind 不会报告错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;main-deadlock.c&lt;/code&gt; 存在发生死锁的风险：如果 p1 线程获得了锁 m1，然后 p2 线程获得了锁 m2，这时 p1 线程试图获得锁 m2，p2 线程试图获得锁 m1，它们都得不到需要的锁，也不会释放自己已经得到的锁，从而陷入死锁。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;helgrind 会报告形如下面所示的错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Thread #3: lock order &amp;quot;0x10C040 before 0x10C080&amp;quot; violated

Observed (incorrect) order is: acquisition of lock at 0x10C080
 ...
 followed by a later acquisition of lock at 0x10C040
   ...

Required order was established by acquisition of lock at 0x10C040
 ...
 followed by a later acquisition of lock at 0x10C080
  ...

Lock at 0x10C040 was first observed
 ...
 Address 0x10c040 is 0 bytes inside data symbol &amp;quot;m1&amp;quot;

Lock at 0x10C080 was first observed
 ...
 Address 0x10c080 is 0 bytes inside data symbol &amp;quot;m2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于程序中的任意两个锁，所有获取锁的行为都一定要按照相同的顺序，否则就有可能引发死锁。helgrind 根据这个原理检测到了可能存在的死锁，并且给出了两次不同的顺序以及锁的名称。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;为什么我无法使死锁暴露出来？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;两个线程的创建是有时间差的，新创建的线程可以利用这个时间差把两个锁都得到，这样死锁就不会暴露出来。&lt;/p&gt;
&lt;p&gt;为了更好地观测死锁现象，我们可以在 worker() 函数的开头添加一句 &lt;code&gt;usleep(1)&lt;/code&gt;，这会让线程在此处等待至少 1 微秒的时间。考虑到系统活动等因素，&lt;code&gt;usleep(1)&lt;/code&gt; 其实会带来一段时长比较随机的 delay，这让两个线程被拉回“同一起跑线”的几率大大增加。此外，多次重复实验，即可比较容易地观察到死锁现象。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;main-deadlock-global.c&lt;/code&gt; 中由于有一个外层的大锁保护，所以不会发生死锁。但使用 helgrind 检查仍然和会报告有非法的锁获得顺序问题。helgrind 只是记录访问锁的顺序并判断是否出现了环，并不能准确地判断死锁是否可能发生。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;主线程在子线程打印的时候会轮询 done 变量，让 CPU 空转，所以这个方法是不高效的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;helgrind 报告该程序存在数据竞争。两个线程对 done 的读写没有用锁保护起来，存在 race condition。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;main-signal-cv.c&lt;/code&gt; 相较于前者有两个改进：一是两个进程对 done 变量的访问都用锁保护了起来，保证了不会发生 race condition；二是该程序使用了条件变量，如果主线程检查 done 结果为 0，会在条件变量上睡眠，睡眠的线程可以被 CPU 调度出去，从而不占据 CPU 时钟周期。等到子线程打印完了唤醒条件变量，主线程再来检查 done 变量并打印自己的内容。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;helgrind 没有报告任何错误。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
