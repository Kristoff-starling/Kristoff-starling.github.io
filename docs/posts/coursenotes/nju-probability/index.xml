<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NJU-22020170 Probability and Mathematical Statistics | Academic</title>
    <link>/posts/coursenotes/nju-probability/</link>
      <atom:link href="/posts/coursenotes/nju-probability/index.xml" rel="self" type="application/rss+xml" />
    <description>NJU-22020170 Probability and Mathematical Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 13 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>NJU-22020170 Probability and Mathematical Statistics</title>
      <link>/posts/coursenotes/nju-probability/</link>
    </image>
    
    <item>
      <title>Homework 1</title>
      <link>/posts/coursenotes/nju-probability/hw1/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/hw1/</guid>
      <description>&lt;h2 id=&#34;problem-11&#34;&gt;Problem 1.1&lt;/h2&gt;
&lt;p&gt;解：(1) $A_1\cap \overline{A_2}\cap \overline{A_3}$。&lt;/p&gt;
&lt;p&gt;(2) $A_1\cup A_2\cup A_3$。&lt;/p&gt;
&lt;p&gt;(3) $(A_1\cap \overline{A_2}\cap \overline{A_3})\cup (\overline{A_1}\cap A_2\cap \overline{A_3})\cup (\overline{A_1}\cap \overline{A_2}\cap A_3)\cup (\overline{A_1}\cap \overline{A_2}\cap \overline{A_3})$。&lt;/p&gt;
&lt;p&gt;(4) $\overline{A_1\cap A_2\cap A_3}$。&lt;/p&gt;
&lt;p&gt;(5) $(A_1\cap A_2)\cup(A_1\cap A_3)\cup (A_2\cap A_3)$。&lt;/p&gt;
&lt;h2 id=&#34;problem-13&#34;&gt;Problem 1.3&lt;/h2&gt;
&lt;p&gt;解：
$$
P=\frac{\binom{10}{4}\binom{4}{3}\binom{3}{2}}{\binom{17}{9}}=\frac{252}{2431}。
$$&lt;/p&gt;
&lt;h2 id=&#34;problem-17&#34;&gt;Problem 1.7&lt;/h2&gt;
&lt;p&gt;解：
$$
P=\frac{9^n-5^n-8^n+4^n}{9^n}=1-\frac{5^n+ 8^n-4^n}{9^n}
$$&lt;/p&gt;
&lt;h2 id=&#34;problem-110&#34;&gt;Problem 1.10&lt;/h2&gt;
&lt;p&gt;解：记第一天下雨为事件 $A$，第二天下雨为事件 $B$。&lt;/p&gt;
&lt;p&gt;(1) $P(至少有一天下雨)=P(A\cup B)=P(A)+P(B)-P(A\cap B)=0.6+0.3-0.1=0.8$。&lt;/p&gt;
&lt;p&gt;(2) $P(两天都不下雨)=P(\overline{A}\cap \overline{B})=P(\Omega)-P(A\cup B)=1-0.8=0.2$。&lt;/p&gt;
&lt;p&gt;(3) $P(至少有一天不下雨)=P(\overline{A}\cup \overline{B})=P(\Omega)-P(A\cap B)=1-0.1=0.9$。&lt;/p&gt;
&lt;p&gt;(4) $P(第一天下雨且第二天不下雨)=P(A\cap \overline{B})=P(A)-P(AB)=0.6-0.1=0.5$。&lt;/p&gt;
&lt;p&gt;(5) $P(恰好一天下雨)=P(A)+P(B)-2P(AB)=0.6+0.3-0.1-0.1=0.7$。&lt;/p&gt;
&lt;h2 id=&#34;problem-113&#34;&gt;Problem 1.13&lt;/h2&gt;
&lt;p&gt;解：容易发现三条折线能构成三角形 $\Leftrightarrow$ 三条折线的长度均小于 $a$。&lt;/p&gt;
&lt;p&gt;设第一处折点距离线段左端点的距离为 $x$，第二处折点距离线段左端点的距离为 $y$，则上述约束可以被翻译为：
$$
\begin{cases}
x&amp;lt;y\\
x&amp;lt;a\\
y-x&amp;lt;a\\
y&amp;gt;a
\end{cases}
$$
画图：&lt;/p&gt;
&lt;img src=&#34;Homework 01.assets/image-20220312152059033.png&#34; alt=&#34;image-20220312152059033&#34; style=&#34;zoom: 33%;&#34; /&gt;
&lt;p&gt;容易看出，$P(能构成三角形)=0.25$。&lt;/p&gt;
&lt;h2 id=&#34;problem-115&#34;&gt;Problem 1.15&lt;/h2&gt;
&lt;p&gt;解：$P(AB)=P(A)P(B|A)=\frac{1}{4}\cdot \frac{1}{3}=\frac{1}{12}$，所以 $P(B)=\frac{P(AB)}{P(A|B)}=\frac{1}{6}$。&lt;/p&gt;
&lt;p&gt;$P(\bar{A}\bar{B})=P(\Omega-A\cup B)=1-(P(A)+P(B)-P(AB))=1-\frac{1}{4}-\frac{1}{6}+\frac{1}{12}=\frac{2}{3}$。&lt;/p&gt;
&lt;h2 id=&#34;problem-118&#34;&gt;Problem 1.18&lt;/h2&gt;
&lt;p&gt;解：记取了甲车间产品为事件 $A$，乙为事件 $B$，丙为事件 $C$，取到次品为事件 $D$。&lt;/p&gt;
&lt;p&gt;(1) $P(D)=P(AD)+P(BD)+P(CD)=0.25\cdot 0.05+0.35\cdot 0.04+0.4\cdot 0.02=0.0345$。&lt;/p&gt;
&lt;p&gt;(2) $P(A|D)=\frac{P(AD)}{P(D)}=\frac{P(A)P(D|A)}{P(D)}=\frac{0.25\cdot 0.05}{0.0345}=\frac{25}{69}$。&lt;/p&gt;
&lt;h2 id=&#34;problem-121&#34;&gt;Problem 1.21&lt;/h2&gt;
&lt;p&gt;解：一共有 3 个面是红色的，这三个红色的面中只有 1 个面背后是黄色的，所以 $P=\frac{1}{3}$。&lt;/p&gt;
&lt;h2 id=&#34;problem-124&#34;&gt;Problem 1.24&lt;/h2&gt;
&lt;p&gt;解：因为 $A,B$ 相互独立，所以 $P(AB)=P(A)P(B)$。我们列出方程：
$$
\begin{cases}
P(A)-P(A)P(B)&amp;amp;=\frac{5}{9}\\
P(A)+P(B)-P(A)P(B)&amp;amp;=\frac{8}{9}\\
0\leq P(A),P(B)\leq 1
\end{cases}
$$
解得
$$
\begin{cases}
P(A)=\frac{5}{6}\\
P(B)=\frac{1}{3}
\end{cases}
$$
综上，$P(A)=\frac{5}{6}$。&lt;/p&gt;
&lt;h2 id=&#34;problem-129&#34;&gt;Problem 1.29&lt;/h2&gt;
&lt;p&gt;解：$P(A)=\frac{3}{6}=\frac{1}{2},P(B)=\frac{3}{6}=\frac{1}{2},P(C)=\frac{18}{36}=\frac{1}{2}$。&lt;/p&gt;
&lt;p&gt;显然 $P(ABC)=0\neq P(A)P(B)P(C)$。&lt;/p&gt;
&lt;p&gt;但
$$
\begin{align}
P(AB)&amp;amp;=\frac{3\times 3}{6\times 6}=\frac{1}{4}=P(A)P(B)\\
P(AC)&amp;amp;=P(A\cap \overline{B})=\frac{9}{36}=\frac{1}{4}=P(A)P(C)\\
P(BC)&amp;amp;=P(B\cap \overline{A})=\frac{9}{36}=\frac{1}{4}=P(B)P(C)
\end{align}
$$
所以这三个事件两两独立，但不相互独立。&lt;/p&gt;
&lt;h2 id=&#34;problem-132&#34;&gt;Problem 1.32&lt;/h2&gt;
&lt;p&gt;解：记两人命中数一样为事件 $A$，两人都中 $i$ 球为事件 $B_i$，则
$$
\begin{align}
P(A)&amp;amp;=P(A|B_0)+P(A|B_1)+P(A|B_2)\\
&amp;amp;=\left(\frac{1}{3}\right)^2\left(\frac{2}{3}\right)^2+\frac{4}{9}\cdot \frac{4}{9}+\left(\frac{2}{3}\right)^2\left(\frac{1}{3}\right)^2\\
&amp;amp;=\frac{8}{27}
\end{align}
$$&lt;/p&gt;
&lt;h2 id=&#34;problem-134&#34;&gt;Problem 1.34&lt;/h2&gt;
&lt;p&gt;解：记一个人蒙及格为事件 $A$。&lt;/p&gt;
&lt;p&gt;(1)
$$
P(A)=\sum_{k=3}^5\binom{5}{k}\left(\frac{1}{4}\right)^k\left(\frac{3}{4}\right)^{5-k}=\frac{53}{512}
$$
(2)
$$
\begin{align}
P(至少两人蒙及格)&amp;amp;=1-P(至多一人蒙及格)\\
&amp;amp;=1-\sum_{k=0}^1\binom{5}{k}P(A)^k(1-P(A))^{5-k}\\
&amp;amp;=0.0866
\end{align}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Homework 2</title>
      <link>/posts/coursenotes/nju-probability/hw2/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/hw2/</guid>
      <description>&lt;h2 id=&#34;problem-21&#34;&gt;Problem 2.1&lt;/h2&gt;
&lt;p&gt;解：
$$
\begin{align}
P(X=1)&amp;amp;=\frac{\binom{4}{3}\cdot 6}{4^3}=\frac{3}{8}\\
P(X=2)&amp;amp;=\frac{\binom{3}{2}\binom{4}{2}\cdot 2}{4^3}=\frac{9}{16}\\
P(X=3)&amp;amp;=\frac{4}{4^3}=\frac{1}{16}
\end{align}
$$
综上，$X$ 的分布列为&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$X$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$1$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$2$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$3$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$P(X)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{3}{8}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{9}{16}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{16}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;problem-24&#34;&gt;Problem 2.4&lt;/h2&gt;
&lt;p&gt;解：各个常数的计算过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$a=\frac{3}{4}-P(X=-1)=\frac{1}{2}$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$b=1-P(X=-1)-P(X=0)=\frac{1}{4}$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$c=P(X&amp;lt;-1)=0$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$d=P(X&amp;lt;0)=P(X=-1)=\frac{1}{4}$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$e=P(X=-1)+P(X=0)+P(X=1)=1$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;problem-28&#34;&gt;Problem 2.8&lt;/h2&gt;
&lt;p&gt;解：(1)
$$
\begin{align}
P(X\leq 3)&amp;amp;=e^{-\lambda}\sum_{k=0}^3\frac{\lambda^k}{k!}\approx 0.7576\\
P(转港)&amp;amp;=P(X&amp;gt;3)=1-P(X\leq 3)=0.2424
\end{align}
$$
(2) 考察函数 $f(x)=\frac{2.5^x}{x!},x\in \mathbb N$，显然当 $x=2$ 时 $f(x)$ 取得最大值。所以最大可能到达港口的油船数为 2，概率为 $\frac{2.5^2}{2}e^{-2.5}\approx 0.2565$。&lt;/p&gt;
&lt;p&gt;(3) 注意到
$$
\begin{align}
P(X\leq 4)&amp;amp;=e^{-\lambda}\sum_{k=0}^4\frac{\lambda^k}{k!}=0.8912&amp;lt;0.9\\
P(X\leq 5)&amp;amp;=e^{-\lambda}\sum_{k=0}^5\frac{\lambda^k}{k!}=0.9553\geq 0.9
\end{align}
$$
所以服务能力提高到 5 只油船才能使得到达游船以 $90%$ 的概率得到服务。&lt;/p&gt;
&lt;h2 id=&#34;problem-212&#34;&gt;Problem 2.12&lt;/h2&gt;
&lt;p&gt;解：(1)
$$
\int_{-\infty}^{+\infty}p(x)dx=\int_0^1p(x)dx=A\int_0^1x^3dx=\left.\frac{A}{4}x^4\right|_0^1=\frac{A}{4}=1
$$
解得 $A=4$。&lt;/p&gt;
&lt;p&gt;(2)
$$
F(x)=P(X\leq x)=\int_{-\infty}^xp(u)du=
\begin{cases}
0&amp;amp;,x\leq 1\\
x^4&amp;amp;,0&amp;lt;x\leq 1\\
1&amp;amp;,x&amp;gt;1
\end{cases}
$$
(3) 令 $F(x)=0.5$，解得 $x=(\frac{1}{2})^{\frac{1}{4}}$，所以 $B=\left(\frac{1}{2}\right)^{\frac{1}{4}}$。&lt;/p&gt;
&lt;h2 id=&#34;problem-216&#34;&gt;Problem 2.16&lt;/h2&gt;
&lt;p&gt;解：令 $Z=\frac{X-\mu}{4},W=\frac{Y-\mu}{5}$，则 $Z\sim N(0,1),W\sim N(0,1)$。
$$
\begin{align}
p_1&amp;amp;=P(X\leq \mu-4)=P(4Z+\mu\leq \mu-4)=P(Z\leq -1)=P(Z\geq 1)\\
p_2&amp;amp;=P(Y\geq \mu+5)=P(5W+\mu\geq \mu+5)=P(W\geq 1)
\end{align}
$$
又 $Z,W$ 分布函数相同，所以 $p_1=p_2$。&lt;/p&gt;
&lt;h2 id=&#34;problem-219&#34;&gt;Problem 2.19&lt;/h2&gt;
&lt;p&gt;解：(1) $Y=2X$ 的分布律如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$Y$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$-4$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$-1$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$0$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$1$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$8$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$P$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{8}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{4}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{8}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{6}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{3}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(2) $Y=X^2$ 的分布律如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$Y$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$0$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$\frac{1}{4}$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$4$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$16$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$P$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{8}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{5}{12}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{8}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{3}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(3) $Y=\sin \left(\frac{\pi}{2}X\right)$ 的分布律如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$Y$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$-\frac{\sqrt 2}{2}$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$0$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$\frac{\sqrt 2}{2}$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$P$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{4}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{7}{12}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\frac{1}{6}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;problem-226&#34;&gt;Problem 2.26&lt;/h2&gt;
&lt;p&gt;证明：$X$ 服从参数为 2 的指数分布，即
$$
p_X(x)=\begin{cases}
2e^{-2x}&amp;amp;,x\geq 0\\
0&amp;amp;,x&amp;lt;0
\end{cases}
$$
$Y=g(X)=1-e^{-2X}$，$g&amp;rsquo;(x)=-e^{-2x}\cdot (-2)=2e^{-2x}&amp;gt;0$，所以 $g(x)$ 严格单调递增且处处可导。因此
$$
\begin{align}
p_Y(y)&amp;amp;=p_X(g^{-1}(y))\cdot \left|g^{-1}(y)\right|\\
&amp;amp;=\frac{1}{2(1-y)}p_X(-\frac{1}{2}\ln(1-y))\\
&amp;amp;=
\begin{cases}
1&amp;amp;,0\leq x\leq 1\\
0&amp;amp;, otherwise
\end{cases}
\end{align}
$$
可以看到在 $[0,1]$ 中，$p_Y(y)=1=\frac{1}{1-0}$，所以 $Y$ 在 $[0,1]$ 上服从均匀分布。$\blacksquare$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 01: Probability Space</title>
      <link>/posts/coursenotes/nju-probability/lec01/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec01/</guid>
      <description>&lt;p&gt;我们可以通过随机试验来研究随机现象。随机试验应当具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相同条件下可重复；&lt;/li&gt;
&lt;li&gt;试验结果不唯一，试验前未知，但所有可能的结果已知；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;概率空间是一个三元组 $(\Omega, \mathscr F, P)$，其中 $\Omega$ 为样本空间，$\mathscr F$ 为可测事件集，$P$ 为概率测度。&lt;/p&gt;
&lt;h2 id=&#34;sample-space&#34;&gt;Sample Space&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 1.1}$ 随机试验的所有可能结果的集合称为&lt;strong&gt;样本空间 (sample space)&lt;/strong&gt;，记作 $\Omega$。每个随机试验的可能结果称为&lt;strong&gt;样本点/基本事件 (sample point)&lt;/strong&gt;，记作 $e$ 或 $\omega$，$e\in \Omega$。&lt;/p&gt;
&lt;h2 id=&#34;measurable-event-set&#34;&gt;Measurable Event Set&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 1.2.1}$ 称 $\mathscr{F}\subseteq Pot(\Omega)$ 是集合 $\Omega$ 上的一个 &lt;strong&gt;$\sigma$-域 ($\sigma$-field)&lt;/strong&gt;，当且仅当&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega \in \mathscr{F}$；&lt;/li&gt;
&lt;li&gt;若 $A\in \mathscr F$，则 $A^C\in \mathscr F$；&lt;/li&gt;
&lt;li&gt;若 $A_1,\cdots A_n\cdots \in \mathscr F$，则 $\bigcup_{i=1}^\infty A_i\in \mathscr F$ (即可数个集合的并也是 $\mathscr F$ 的元素)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\fbox{Definition 1.2.2}$ &lt;strong&gt;可测事件集&lt;/strong&gt; $\mathscr F$ 是 $\Omega$ 上的一个 $\sigma$-域，若 $A\subseteq \Omega$，$A\in \mathscr F$，则称 $A$ 是一个&lt;strong&gt;事件 (event)&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;probability-metric&#34;&gt;Probability Metric&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 1.3}$ 集合函数 $P:\mathscr F\rightarrow [0,1]$ 是 $(\Omega, \mathscr F)$ 上的一个&lt;strong&gt;概率测度 (probability metric)&lt;/strong&gt;，当且仅当&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(\Omega)=1$；&lt;/li&gt;
&lt;li&gt;对于任意 $A\in \mathscr F$，$P(A)\geq 0$；&lt;/li&gt;
&lt;li&gt;满足可列可加性：对于&lt;u&gt;互不相容&lt;/u&gt;的事件 $A_1,\cdots,A_n\cdots\in \mathscr F$，$P(\bigcup_{n=1}^\infty A_n)=\sum_{n=1}^\infty P(A_n)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是一系列有用的推论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P(\emptyset)=0$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proof: 取 $A_1=\Omega, A_2=A_3=\cdots =\emptyset$，那么
$$
P(\Omega)=P(\bigcup_{n=1}^\infty A_n)=\sum_{n=1}^\infty P(A_n)=P(\Omega)+\sum_{n=2}^\infty P(\emptyset)
$$
所以 $P(\emptyset)=0$。$\blacksquare$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可列可加性可以推出有限可加性，即对于 $n\in \mathbb{N}$ ， $P(\bigcup_{k=1}^nA_k)=\sum_{k=1}^nP(A_k)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proof: 取 $A_{n+1}=A_{n+2}=\cdots =\emptyset$，则有
$$
\begin{align}
P(\bigcup_{k=1}^nA_k)&amp;amp;=P(\bigcup_{k=1}^\infty A_k)=\sum_{k=1}^\infty P(A_k)\\
&amp;amp;=\sum_{k=1}^nP(A_k)+\sum_{k=n+1}^\infty P(\emptyset)=\sum_{k=1}^nP(A_k)
\end{align}
$$
有限可加性得证。$\blacksquare$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A-B)=P(A)-P(A\cap B)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proof: 由 $A=(A-B)\cup (A\cap B)$ 和结论 2 易得。$\blacksquare$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A\cup B)=P(A)+P(B)-P(A\cap B)$。更一般地，
$$
P(\bigcup_{k=1}^nA_k)=\sum_{i=1}^n\left((-1)^{i+1}\sum_{1\leq j_1&amp;lt;j_2&amp;lt;\cdots&amp;lt;j_i\leq n}P(\bigcap_{k=1}^iA_{j_i})\right)
$$
(容斥原理的表达式)。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 02: Classical Probability and Geometric Probability</title>
      <link>/posts/coursenotes/nju-probability/lec02/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec02/</guid>
      <description>&lt;h2 id=&#34;classical-probability&#34;&gt;Classical Probability&lt;/h2&gt;
&lt;p&gt;古典概型的特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega={e_1,e_2,\cdots,e_n},n\in \mathbb{N}$。&lt;/li&gt;
&lt;li&gt;对于任意 $1\leq i\leq n$，$P({e_i})=\frac{1}{n}$ (即每个样本点等可能)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在此基础上，若 $A={e_{i_1},\cdots e_{i_k}}$ ，则 $P(A)=\frac{k}{n}$。&lt;/p&gt;
&lt;h3 id=&#34;permutations-and-combinations&#34;&gt;Permutations and Combinations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$n$ 个可分辨的球选 $r$ 个，可重复选，排列：$n^r$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n$ 个可分辨的球选 $r$ 个，不可重复选，排列：$n^{\underline r}=\frac{n!}{(n-r)!}$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n$ 个可分辨的球选 $r$ 个，不可重复选，组合：$\binom{n}{r}$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n$ 个可分辨的球选 $r$ 个，可重复选，组合：$\binom{n+r-1}{r}$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;考虑一个选择方案 $1\leq x_1\leq x_2\leq \cdots \leq x_r\leq n$，现设计另一个数列 $y$，满足 $y_1=x_1,y_2=x_2+1,y_3=x_3+2,\cdots,y_r=x_r+r-1$。那么 $y$ 严格单调增，且 $1\leq y_i\leq n+r-1$。$x$ 序列和 $y$ 序列一一对应，因此 $x$ 序列个数 = $y$ 序列个数 = $\binom{n+r-1}{r}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【例】求 $(a+b+c)^n$ 合并同类项的展开式中有多少项。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：相当于求形如 $a^{n_1}b^{n_2}c^{n_3},n_1+n_2+n_3=n$ 的个数。可以将其理解为从 $3$ 个物品中选 $n$ 个，可重复选的组合方案数，因此答案为 $\binom{n+2}{n}=\frac{1}{2}(n+1)(n+2)$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;有关组合数的一些性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$(1+x)^n=\sum_{k=0}^n\binom{n}{k}x^k$，令 $x=1$ ，可得 $\sum_{k=0}^n\binom{n}{k}=2^n$。&lt;/li&gt;
&lt;li&gt;$(1+x)^{a+b}=(1+x)^a(1+x)^b$，因此 $\binom{a+b}{n}=\sum_{k=0}^a\binom{a}{k}\binom{b}{n-k}$。若取 $a=b=n$，则可以得到 $\binom{2n}{n}=\sum_{k=0}^n\binom{n}{k}\binom{n}{n-k}=\sum_{k=0}^n\binom{n}{k}^2$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;geometrical-probability&#34;&gt;Geometrical Probability&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 2.1}$ &lt;strong&gt;(几何概型)&lt;/strong&gt; 若 $\Omega$ 中的样本点与一个有界区域 $S$ 中的点一一对应，则事件 $A$ 对应于 $S$ 的一个子集 $D$。若 $A$ 的概率只和 $D$ 的测度有关，而与 $D$ 的形状，位置无关，那么 $P(A)=\frac{D的测度}{S 的测度}$。&lt;/p&gt;
&lt;p&gt;【例】甲乙两人各在 1h 内随机一个时间点到达约会地点，先到的人最多等后到的人 15 分钟，求两人碰面的概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：用数对 $(x,y)$ 表示甲乙两人到达的时间，则两人可以碰面当且即当 $|x-y|\leq 15$，画图：&lt;/p&gt;
&lt;img src=&#34;/img/NJU-22020170-02-01.png&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;因此碰面概率为 $\frac{60^2-45^2}{60^2}=\frac{7}{16}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】 (蒲丰投针) 两平行线间距 $a$，向其投掷长度为 $l$ $(l&amp;lt;a)$ 的针，使针的中点在两平行线之间，求针与两条平行线中任意一条相交的概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解： $l&amp;lt;a$ 保证了针至多只会和一条线相交，根据对称性，我们只考虑针与下面的线相交的情况。&lt;/p&gt;
&lt;p&gt;设针的中点与线的距离为 $x$，针所在的直线与线的夹角为 $\theta$，则针与线相交当且仅当 $x\leq \frac{l}{2}\sin\theta$。我们以 $x$ 和 $\theta$ 为坐标轴画出样本空间和相交事件：
$$
\begin{align}
\Omega &amp;amp;= {(\theta,x)|0&amp;lt;\theta&amp;lt;\pi,0\leq x\leq \frac{a}{2}}\\
A &amp;amp;= {(\theta,x)|0\leq \theta \leq \pi, 0\leq x \leq \frac{l}{2}\sin \theta}
\end{align}
$$
$A$ 的图像是一个正弦函数，要计算面积，只需要计算积分：
$$
P(A)=\frac{1}{\pi\cdot \frac{a}{2}}\int_{0}^\pi\frac{l}{2}\sin \theta d\theta=\left.-\frac{l}{2}\cos\theta\right|_{0}^\pi=\frac{2l}{a\pi}
$$
这个方法可以用于估算 $\pi$ 的值。通过大量重复试验用 $f_N(A)$ 来代替 $P(A)$ 后，$\pi=\frac{2l}{aP(A)}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】 (贝特朗奇论) 在一个半径为 1 的圆中等概率地取一根弦，弦长 $l&amp;gt;\sqrt 3$ 的概率是多少？&lt;/p&gt;
&lt;p&gt;由于这里的“等概率”没有被严格地定义，因此可能有多种对等概率的解读，它们都是对的且会算出不同的结果：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在圆周上固定一个点，然后另一个点在圆周上随机选取：$P(A)=\frac{1}{3}$。&lt;/li&gt;
&lt;li&gt;让一条直线从上往下均匀地扫一遍，发现只有中点距离圆心小于 $\frac{1}{2}$ 时弦长满足要求：$P(A)=\frac{1}{2}$。&lt;/li&gt;
&lt;li&gt;在圆内随机选取弦的中点，发现只有中点位于半径为 $\frac{1}{2}$ 的小圆内是弦长满足要求：$P(A)=\frac{1}{4}$。&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 03: Conditional Probability and Bayes Formula</title>
      <link>/posts/coursenotes/nju-probability/lec03/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec03/</guid>
      <description>&lt;h2 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h2&gt;
&lt;p&gt;“事件 $A$ 和 $B$ 同时发生的概率”与“ $B$ 发生的条件下，$A$ 发生的概率”不一定相等！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“事件 $A$ 和 $B$ 同时发生” 是在 $\Omega$ 中 寻找 $A\cap B$，$P(A\cap B)=\frac{|A\cap B|}{|\Omega|}$。&lt;/li&gt;
&lt;li&gt;“ $B$ 发生的条件下，$A$ 发生”，样本空间退化为 $B$，$P(A|B)=\frac{|A\cap B|}{|B|}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\fbox{Definition 3.1}$ 设事件 $A,B$ 满足 $P(B)&amp;gt;0$，则称 $P(A|B)=\frac{P(AB)}{P(B)}$ 为在事件 $B$ 发生的条件下事件 $A$ 发生的概率。&lt;/p&gt;
&lt;p&gt;注：若将 $B$ 视为一个样本空间 $\Omega_B$，则可定义概率空间 $(\Omega_B,\mathscr{F}_B,P_B)$，其中 $\mathscr{F}_B={A\cap B| A\in \mathscr{F}}$，$P_B(A)=P(A|B)$。此时若直接取 $P$ 为 $\Omega_B$ 的概率测度，即令 $P_B=P$，则 $P_B(B)&amp;lt;1$，违反了概率空间的定义 (出现了某种概率损失)，所以应该对其除掉 $P(B)$ 进行一个&lt;strong&gt;归一化&lt;/strong&gt;：$P_B(A)=\frac{P(AB)}{P(B)}$。&lt;/p&gt;
&lt;p&gt;事实上， $P_B$ 也是 $(\Omega,\mathscr{F})$ 上的一个概率&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_B(A)\geq 0$，$P_B(\Omega)=1$；&lt;/li&gt;
&lt;li&gt;满足可列可加性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，之前推导的关于交、并的概率公式在条件概率上仍然适用。$P_B$ 的特点是将所有发生的事件聚焦在 $B$ 以内，即对于任意 $A\cap B=\emptyset$，$P_B(A)=0$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;乘法公式&lt;/strong&gt;：当 $P(A_{n-1}\cdots A_1)&amp;gt;0$ 时，
$$
P(A_nA_{n-1}\cdots A_1)=P(A_n|A_{n-1}A_{n-2}\cdots A_1)P(A_{n-1}|A_{n-2}\cdots A_1)\cdots P(A_2|A_1)P(A_1)
$$
注：对于任意 $1\leq m\leq n-1$，$\bigcap_{i=1}^{n-1}A_i\subseteq \bigcap_{i=1}^mA_i$，因此 $P(\bigcap_{i=1}^{n-1}A_i)&amp;gt;0$ 的条件已经为之后所有的条件概率做好了假设。&lt;/p&gt;
&lt;p&gt;【例】$n$ 个人抽签，求放回/不放回的情况下，第 $k$ 个人抽中的概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：令 $A_k$ 表示事件“第 $k$ 个人抽中“，则要求 $P(A_k\overline{A_{k-1}}\cdots\overline{A_1})$，根据乘法公式，$P(A_k\overline{A_{k-1}}\cdots\overline{A_1})=P(A_k|\overline{A_{k-1}}\cdots\overline{A_1})P(\overline{A_{k-1}}|\overline{A_{k-2}}\cdots \overline{A_1})\cdots P(\overline{A_2}|\overline{A_1})P(\overline{A_1})$。&lt;/p&gt;
&lt;p&gt;(a) 当 $k=1$ 时，$P(A_k)=\frac{1}{n}$，当 $k\geq 2$ 时，$P(\overline{A_1})=\frac{n-1}{n}$，对于任意 $2\leq m\leq k-1$，$P(\overline{A_m}|\overline{A_{m-1}}\cdots\overline{A_1})=\frac{n-m}{n-m+1}$，$P(A_k|\overline{A_{k-1}}\cdots\overline{A_1})=\frac{1}{n-k+1}$，所以 $P=(\prod_{m=1}^{k-1}\frac{n-m}{n-m+1})\frac{1}{n-k+1}=\frac{1}{n}$&lt;/p&gt;
&lt;p&gt;(b) 当 $k=1$ 时，$P(A_k)=\frac{1}{n}$，当 $k\geq 2$ 时，$P(\overline{A_1})=\frac{n-1}{n}$，对于任意 $2\leq m\leq k-1$，$P(\overline{A_m}|\overline{A_{m-1}}\cdots\overline{A_1})=\frac{n-1}{n}$，$P(A_k|\overline{A_{k-1}}\cdots\overline{A_1})=\frac{1}{n}$，所以 $P=\left(\frac{n-1}{n}\right)^{k-1}\frac{1}{n}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;total-probability-formula&#34;&gt;Total Probability Formula&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 3.2}$ 若事件 $A_1,A_2,\cdots,A_n$ 满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于任意 $1\leq i&amp;lt;j\leq n$，$A_i\cap A_j=\emptyset$；&lt;/li&gt;
&lt;li&gt;$\bigcup_{i=1}^nA_i=\Omega$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则称 $A_1,\cdots,A_n$ 为 $\Omega$ 的一个&lt;strong&gt;划分/完备事件集&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 3.3}$ (全概率公式) 设 $A_1,\cdots,A_n$ 为 $\Omega$ 的一个划分，则
$$
P(B)=\sum_{i=1}^nP(BA_i)=\sum_{i=1}^nP(A_i)P(B|A_i)\upharpoonleft {P(A_i)&amp;gt;0}
$$
其中最后的部分是示性函数 (indicator function)，表示略过 $P(A_i)=0$ 的那些部分的条件概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proof：首先有 $B=B\Omega=B(\bigcup_{i=1}^nA_i)=\bigcup_{i=1}^n(BA_i)$，且 $BA_i$ 彼此互不相容，所以
$$
P(B)=P\left(\bigcup_{i=1}^n(BA_i)\right)\overset{可列可加性}{=}\sum_{i=1}^nP(BA_i) \qquad\qquad\qquad \blacksquare
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：全概率公式对于无限可列的划分仍然成立。&lt;/p&gt;
&lt;p&gt;对于全概率公式的一个感性理解是：&lt;strong&gt;如果要计算事件A的概率，我们可以“分类讨论”A在各种情况下发生的概率，再全部加起来&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对于一个条件概率 $P(B|C)$，我们可以考虑一个划分 $A_1,\cdots,A_n$ 并将其写为 $P(B|C)=\sum_{i=1}^nP(BA_i|C)$。若这仍不好算，仍然可以有以下变形：
$$
\sum_{i=1}^nP(BA_i|C)=\sum_{i=1}^n\frac{P(BA_iC)}{P(C)}=\sum_{i=1}^n\frac{P(A_iC)}{P(C)}P(B|A_iC)\upharpoonleft {P(A_iC)&amp;gt;0}
$$
我们将条件 $C$ 加强到了条件 $A_iC$，从而可能更容易计算。&lt;/p&gt;
&lt;p&gt;【例】有两批灯泡各10支，第一批有一个次品，第二批有两个次品。运输过程中两批各打碎了一个。现从剩余灯泡中抽取一个，抽到次品的概率是多少？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：讨论打碎的两个灯泡的情况：$B_1=(好,好),B_2=(好,次),B_3=(次,好),B_4=(次,次)$。记 $A$ 为抽到次品这个事件
$$
\begin{align}
P(A)&amp;amp;=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+P(B_3)P(A|B_3)+P(B_4)P(A|B_4)\\
&amp;amp;=\frac{9}{10}\cdot \frac{8}{10}\cdot \frac{3}{18}+\frac{9}{10}\cdot \frac{2}{10}\cdot \frac{2}{18}+\frac{1}{10}\cdot \frac{8}{10}\cdot \frac{2}{18}+\frac{1}{10}\cdot \frac{2}{10}\cdot \frac{1}{18}\\
&amp;amp;=\frac{3}{20}
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;bayes-formula&#34;&gt;Bayes Formula&lt;/h2&gt;
&lt;p&gt;在全概率公式 $P(B)=\sum_{i=1}^nP(A_i)P(B|A_i)$ 中，$B$ 可以看作发生的结果，$A_i$ 可以看作 $B$ 发生的可能原因。$P(A_i)$ 被称为先验概率，$P(A_i|B)$ 衡量了 $B$ 已经发生的情况下原因发生的可能性，称为后验概率。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 3.4}$ (贝叶斯公式) 设 $A_1,\cdots,A_n$ 为 $\Omega$ 的一个划分，对于任意 $1\leq k\leq n$，$P(A_k)&amp;gt;0$。则对于 $B\in \mathscr F$，我们有
$$
P(A_k|B)=\frac{P(A_kB)}{P(\Omega B)}=\frac{P(A_k)P(B|A_k)}{\sum_{i=1}^nP(A_i)P(B|A_i)}
$$
【例】有一种罕见病，用某方法来检测时，如果该人患病，那么他被检测出有病的概率是 0.95，如果一个人没有患病，那么他被检测出没病的概率是 0.9，正常人中罕见病发病率为 0.0004。现有一个人检测出有病，他真正有病的概率是多少？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：令 $C={某人患罕见病},A={某人被检测出罕见病}$，则
$$
P(C|A)=\frac{P(C)P(A|C)}{P(C)P(A|C)+P(\overline{C})P(A|\overline{C})}=\frac{0.0004\cdot 0.95}{0.0004\cdot 0.95+0.9996\cdot 0.1}\approx 0.003
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;该概率实际上非常小的原因是：人群中不患病的很多，而不患病情况下的误诊率不够小，这导致有很大概率都是这种情况导致的检测结果异常。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 04: Independence and Bernoulli Experiments</title>
      <link>/posts/coursenotes/nju-probability/lec04/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec04/</guid>
      <description>&lt;h2 id=&#34;independence&#34;&gt;Independence&lt;/h2&gt;
&lt;p&gt;【例】$a$ 个黑球，$b$ 个白球，分别在有放回、无放回的情况下计算：&lt;/p&gt;
&lt;p&gt;(1) 第一次摸到黑球 (A)，第二次摸到黑球的概率 (B)。&lt;/p&gt;
&lt;p&gt;(2) 第二次摸到黑球的概率。&lt;/p&gt;
&lt;p&gt;容易发现，有放回时，$P(B|A)=P(B)$ (放回后，第二次的实验条件与第一次的结果无关)，无放回时 $P(B|A)\neq P(B)$。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 4.1}$ 称事件 $A$ 和 $B$ 互相独立，若 $P(AB)=P(A)P(B)$ (或者说，$P(B)=P(B|A)$)。&lt;/p&gt;
&lt;p&gt;以下是一些推论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\emptyset,\Omega$ 与任意事件独立。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $A,B$ 独立，则 $\overline{A},B$，$A,\overline{B}$ ，$\overline{A},\overline{B}$ 也独立。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proof: 第一条：$P(\overline{A}B)=P(B-BA)=P(B)-P(BA)=P(B)-P(B)P(A)=P(B)(1-P(A))=P(B)P(\overline{A})$&lt;/p&gt;
&lt;p&gt;容易推得第二，第三条也成立。$\blacksquare$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注：事件独立和韦恩图上两个事件没有交集没有任何关系！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 4.2}$ (三个事件的独立性) $A,B,C$ 相互独立，若&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A,B,C$ 两两互相独立。&lt;/li&gt;
&lt;li&gt;$P(ABC)=P(A)P(B)P(C)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：这两条并不能互相推出，以下是例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) 推不出 (2)：一个正四面体，一面红色 (A)，一面绿色 (B)，一面蓝色 (C)，一面三个颜色都有，讨论向下面的颜色：
&lt;ul&gt;
&lt;li&gt;$P(A)=P(B)=P(C)=2/4=1/2$，$P(AB)=1/4=P(A)P(B)$。&lt;/li&gt;
&lt;li&gt;然而 $P(ABC)=1/4\neq P(A)P(B)P(C)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(2) 推不出 (1)：一个正八面体，1,2,3,4面有红色，1,2,3,5面有绿色，1,6,7,8面有蓝色。
&lt;ul&gt;
&lt;li&gt;$P(A)=P(B)=P(C)=4/8=1/2$，$P(ABC)=1/8=P(A)P(B)P(C)$。&lt;/li&gt;
&lt;li&gt;然而 $P(AB)=3/8\neq P(A)P(B)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;类似地可以定义 $n$ 个事件的独立性：对于事件 $A_1,\cdots,A_n$，令 $I$ 为指标集，则它们独立当且仅当对于任意 $S\subseteq I$，有 $P(\bigcap_{\alpha\in S}A_\alpha)=\prod_{\alpha\in S}P(A_\alpha)$。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 4.3}$ 若事件 $A_1,\cdots, A_n$ 互相独立，那么考虑事件集的任意一个划分，每个组里的事件做任意运算的结果与别组的结果也互相独立。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 4.4}$ 若 $A_1,\cdots, A_n$ 相互独立，则 $P(\bigcup_{k=1}^nA_k)=1-\prod_{k=1}^nP(\overline{A_k})$。&lt;/p&gt;
&lt;p&gt;证明：$P(\bigcup_{k=1}^nA_k)=1-P(\overline{\bigcup_{k=1}^nA_k})=1-P(\bigcap_{k=1}^n\overline{A_k})=1-\prod_{k=1}^nP(\overline{A_k})$。$\blacksquare$&lt;/p&gt;
&lt;h2 id=&#34;bernoulli-experiments&#34;&gt;Bernoulli Experiments&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 4.5}$ 若一个独立重复试验满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个事件只有两个结果：$A$ 和 $\overline{A}$，$P(A)=p,P(\overline(A))=1-p$。&lt;/li&gt;
&lt;li&gt;试验可重复，每两次试验之间互相独立。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则 $n$ 次上述实验称为 $n$ 重伯努利试验 (Bernoulli Experiment)。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 4.6}$ 伯努利试验中，记 $P_n(k)$ 为 $A$ 发生 $k$ 次的概率，则 $P_n(k)=\binom{n}{k}p^k(1-p)^{n-k}$。&lt;/p&gt;
&lt;p&gt;【例】 (简单随机游走 simple random walk) 一个粒子从0出发在整数数轴上运动，每次向右移动的概率为 $p$，求跳 $n$ 次后位于 $k$ 的概率。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：以下只讨论 $k\geq 0$ 的情况：
$$
P(n,k)=
\begin{cases}
0&amp;amp;, k&amp;gt;n\\
0&amp;amp;, n与k奇偶性不同\\
\binom{n}{(n+k)/2}p^{(n+k)/2}(1-p)^{(n-k)/2}&amp;amp;，otherwise
\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\fbox{Theorem 4.7}$ (泊松定理, Poisson) 若 $np_n=\lambda$，则对于固定的 $k$，
$$
\lim_{n\rightarrow \infty} \binom{n}{k}p_n^k(1-p_n)^{n-k}=\frac{\lambda^k}{k!}e^{-\lambda}
$$
证明：
$$
\begin{align}
\lim_{n\rightarrow \infty}\binom{n}{k}p_n^k(1-p_n)^{n-k}&amp;amp;=\lim_{n\rightarrow \infty}\frac{n(n-1)\cdots (n-k+1)}{k!}(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k}\\
&amp;amp;=\lim_{n\rightarrow \infty}\frac{\lambda^k}{k!}\cdot 1(1-\frac{1}{n})(1-\frac{2}{n})\cdots (1-\frac{k-1}{n})(1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k}\\
&amp;amp;=\lim_{n\rightarrow \infty}\frac{\lambda^k}{k!}(1-\frac{\lambda}{n})^n\\
&amp;amp;=\frac{\lambda^k}{k!}\left[\lim_{n\rightarrow \infty}(1+\frac{-\lambda}{n})^{\frac{n}{-\lambda}}\right]^{-\lambda}\\
&amp;amp;=\frac{\lambda^k}{k!}e^{-\lambda}
\end{align}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 05: Random Variable and Distribution Function</title>
      <link>/posts/coursenotes/nju-probability/lec05/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec05/</guid>
      <description>&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 5.1}$ 设 $X:\Omega \rightarrow \mathbb{R}$，且对于任意 $\mathbb{R}$ 中的 Borel 集 $B$，$\{e|X(e)\in B\}\in \mathscr{F}$，则称 $X$ 是 $(\Omega, \mathscr F, P)$ 上的随机变量。&lt;/p&gt;
&lt;p&gt;注：1. Borel 集是由所有的 $\{(a,b]|-\infty\leq a,b\leq +\infty\}$ 经过可数次交或并得到的集合。&lt;/p&gt;
&lt;p&gt;​		2. 在大部分场合，只需关注 $X$ 是从 $\Omega$ 到 $\mathbb{R}$ 上的映射即可。&lt;/p&gt;
&lt;p&gt;【例】示性随机变量 (indicator)，对于 $A\in \mathscr{F}$，定义 $X_A(e)=\begin{cases}1,e\in A\\0, e\notin A\end{cases}$。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 5.2}$ 设 $X$ 是随机变量，则称 $F_X(x)\triangleq P(X\leq x)$  为 $X$ 的分布函数。&lt;/p&gt;
&lt;p&gt;分布函数满足以下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;单调不降。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于 $x_1&amp;lt;x_2$，$F(x_2)-F(x_1)=P(x_1&amp;lt;x\leq x_2)\geq 0$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\lim_{x\rightarrow +\infty}F(x)=1,\lim_{x\rightarrow -\infty}F(x)=0$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lemma (单调收敛定理) 当被积函数单调递增时，积分和极限可以换序。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;${X\leq x}\overset{x\rightarrow +\infty}{\longrightarrow}\Omega$，$lim_{x\rightarrow \infty}P(X\leq x)\overset{lemma}{=}P(\lim_{x\rightarrow \infty}{X\leq x})=P(\Omega)=1$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$F$ 右连续且存在左极限 (càdlàg, RCLL)，i.e. $\lim_{x\rightarrow x_0^+}F(x)=F(x_0)$，$F(x_0-0)$ 存在。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$F(x)-F(x_0)=P(X\leq x)-P(X\leq x_0)=P(x_0&amp;lt;X\leq x)\overset{x\rightarrow x_0^+}{\longrightarrow}P(\emptyset)=0$。&lt;/p&gt;
&lt;p&gt;$x&amp;lt;x_0$ 时，$F(x)\leq F(x_0)$ 且 $F(x)$ 单增，所以左极限存在。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：左不一定连续的原因是：$F(x_0)-F(x)=P(x&amp;lt;X\leq x_0)\overset{x\rightarrow x_0^-}{\longrightarrow}P(X=x_0)$ 不一定为 0。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 5.3}$ 如果一个函数 $F$ 满足上述三条性质，那么它必然是某个随机变量 $X$ 的分布函数。&lt;/p&gt;
&lt;h2 id=&#34;discrete-random-variable&#34;&gt;Discrete Random Variable&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 5.4}$ 若随机变量 $X$ 的取值为有限多个或无限可数个，则 $X$ 为离散随机变量。设 $X$ 的取值为 $x_1,x_2,\cdots $，令 $P(x=x_k)=P_k$，称 ${P_k}$ 为 $X$ 的分布列/分布律。&lt;/p&gt;
&lt;p&gt;注：$F(x)=P(X\leq x)=P(\bigcup_{x_k\leq x}P_k)=\sum_{x_k\leq x}P_k$。&lt;/p&gt;
&lt;h2 id=&#34;common-distributions&#34;&gt;Common Distributions&lt;/h2&gt;
&lt;p&gt;【例】 (0-1分布) 设 $A\in \mathscr{F}$，$P(A)=p\in(0,1)$。令 $X_A$ 为 $A$ 的 indicator，则 $P(X_A=1)=P(A)=p,P(X_A=0)=1-p$。&lt;/p&gt;
&lt;p&gt;【例】 (二项分布) 设 $X$ 的分布律为 $p_k=P(X=k)=\binom{n}{k}p^k(1-p)^{n-k},p\in (0,1)$，$p_k$ 即为 $n$ 重 Bernoulli 试验成功 $k$ 次的概率，$X$ 服从二项分布，记为 $X\sim B(n,p)$。&lt;/p&gt;
&lt;p&gt;【例】 (泊松分布) 设 $X$ 的分布律为 $p_k=P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}$，则称 $X$ 服从泊松分布，记为 $X\sim P(\lambda)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\sum_{k=0}^\infty \frac{\lambda^k}{k!}e^{-\lambda}=e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!}\overset{Taylor\space Series}{=}e^{-\lambda}e^\lambda=1
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：在 $\lambda=np_n$ 固定的情况下，当 $n$ 很大时，$p_n$ 则很小。那么在 $k&amp;laquo;n$ 的情况下，二项分布可以近似为泊松分布。&lt;/p&gt;
&lt;p&gt;【例】 (几何分布) 独立重复试验，成功概率为 $p$，记第 $k$ 次试验首次成功的概率为 $p_k=P(X=k)=(1-p)^{k-1}p$，则 $X$ 服从几何分布，记为 $X\sim g(p)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\sum_{k=1}^\infty (1-p)^{k-1}p=p\sum_{k=0}^\infty(1-p)^k=p\cdot\frac{1}{1-(1-p)}=p\cdot \frac{1}{p}=1
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;几何分布没有记忆性。假设前 $t$ 次试验均失败，则再试 $s$ 次成功的概率 $P(X=t+s|X&amp;gt;t)=P(X=s)$。或者：$P(X\geq t+s|x&amp;gt;t)=\sum_{k=s}^\infty P(X=t+k|X&amp;gt;t)=\sum_{k=s}^\infty P(X=k)=P(X\geq s)$。&lt;/p&gt;
&lt;p&gt;【例】 (巴斯卡分布) 独立重复试验，每次成功概率为 $p$，记做完第 $k$ 次试验后恰好取得了 $r$ 次成功 $(k\geq r)$ 的概率为 $p_k$，有
$$
p_k=P(X=k)=\binom{k-1}{r-1}p^r(1-p)^{k-r}
$$
我们称 $X$ 服从巴斯卡分布 ($r=1$ 时的巴斯卡分布就是几何分布)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\sum_{k=r}^\infty p_k=\sum_{k=r}^\infty \binom{k-1}{r-1}p^r(1-p)^{k-r}=p^r\sum_{k=r}^\infty \binom{k-1}{r-1}(1-p)^{k-r}
$$&lt;/p&gt;
&lt;p&gt;令 $q=1-p$，只要证 $\sum_{k=r}^\infty \binom{k-1}{r-1}q^{k-r}=(1-q)^{-r}$。对 $f(x)=(1-x)^{-r}$ 泰勒展开，
$$
\begin{align}
f(x)=(1-x)^{-r}&amp;amp;=\sum_{k=0}^\infty \frac{f^{(k)}(0)}{k!}x^k\\
&amp;amp;=\sum_{k=0}^\infty\frac{1}{k!}(-1)^k(-r)(-r-1)\cdots (-r-k+1)x^k\\
&amp;amp;=\sum_{k=0}^\infty\frac{1}{k!}r(r+1)\cdots (r+k-1)x^k\\
&amp;amp;=\sum_{k=0}^\infty\frac{(r+k-1)!}{k!(r-1)!}x^k\\
&amp;amp;=\sum_{k=0}^\infty\binom{r+k-1}{k}x^k\\
&amp;amp;=\sum_{k=r}^\infty\binom{k-1}{r-1}x^{k-r}
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】 (超几何分布) 对 $N$ 个产品无放回抽样 $n$ 次，其中有 $M$ 个次品，令抽到 $k$ 个次品的概率为 $p_k$，则
$$
p_k=P(X=k)=\frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}
$$
我们称 $X$ 服从超几何分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\sum_{k=0}^\infty p_k=\frac{1}{\binom{N}{n}}\sum_{k=max(0,n-(N-M))}^{min(n,M)} \binom{M}{k}\binom{N-M}{n-k}
$$&lt;/p&gt;
&lt;p&gt;右边的部分用“讲故事法”容易证明等于 $\binom{N}{n}$。如果需要比较数学的方法，可以从 $(1+x)^N$ 和 $(1+x)^M(1+x)^{N-M}$ 两个角度去考察 $x^n$ 前的系数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：当 $N,N-M&amp;raquo;n\geq k$ 时，超几何分布可以近似地当作二项分布计算：
$$
\begin{align}
\frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}&amp;amp;=\frac{n!}{k!(n-k)!}\frac{M(M-1)\cdots (M-k+1)}{N(N-1)\cdots (N-k+1)}\frac{(N-M)\cdots (N-M-(n-k)+1)}{(N-k)\cdots (N-k-(n-k)+1)}\\
&amp;amp;\approx \binom{n}{k}\left(\frac{M}{N}\right)^k\left(\frac{N-M}{N}\right)^{n-k}
\end{align}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 06: Continuous Random Variables</title>
      <link>/posts/coursenotes/nju-probability/lec06/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec06/</guid>
      <description>&lt;p&gt;$\fbox{Definition 6.1}$ 设存在一个非负的可积函数 $p(x)$，若 $X$ 的分布函数 $F(x)=\int_{-\infty}^xp(u)du$，则称 $X$ 为连续型随机变量，$p(x)$ 为密度函数。&lt;/p&gt;
&lt;p&gt;根据定义，我们可以得到
$$
\begin{align}
P(X\leq a)&amp;amp;=F(a)=\int_{-\infty}^ap(x)dx\\
P(a&amp;lt;X\leq b)&amp;amp;=F(b)-F(a)=\int_a^b p(x)dx
\end{align}
$$
一些注意点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$F(x)$ 连续。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;显然 $p(x)$ 有界，设 $p(x)\leq M&amp;lt;+\infty$，对于固定的 $x_0$，
$$
|F(x)-F(x_0)|=\left|\int_{x_0}^xp(u)d(u)\right|\leq \int_{x_0}^x |p(u)|du\leq |x-x_0|M
$$
$\forall \varepsilon&amp;gt;0$，当 $|x-x_0|\leq \frac{\varepsilon}{M}$ 时，$|F(x)-F(x_0)|\leq \varepsilon$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p(x)$ 并不能表示 $P(X=x)$。事实上对于任意 $x_0$，$P(X=x_0)=0$。$p(x)$ 只能理解为 $X$ 在 $x$ 的一个小邻域内的概率与该邻域的长度的近似比，即
$$
P(x&amp;lt;X\leq x+ \Delta x)=\int_{x}^{x+\Delta x}p(u)du\approx p(x)\Delta x
$$
关于 $P(X=x_0)=0$ 的说明：
$$
0\leq P(X=x_0)\leq P(x_0-\Delta x&amp;lt;X\leq x_0)=F(x_0)-F(x_0-\Delta x)\overset{\Delta x\rightarrow 0}{\longrightarrow} 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $p(x)$ 在 $x_0$ 连续，则 $F(x)$ 在 $x_0$ 处可导，且 $F&amp;rsquo;(x_0)=p(x_0)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\begin{align}
F(x)-F(x_0)&amp;amp;=\int_{x_0}^x p(u)du\\
&amp;amp;=\int_{x_0}^x(p(u)-p(x_0))du+\int_{x_0}^xp(x_0)du\\
&amp;amp;=\int_{x_0}^x(p(u)-p(x_0))du+p(x_0)(x-x_0)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;于是
$$
\left|\frac{F(x)-F(x_0)}{x-x_0}-p(x_0)\right|\leq \frac{1}{|x-x_0|}\int_{x_0}^x|p(u)-p(x_0)|du\quad (*)
$$
$\forall \varepsilon&amp;gt; 0,\exists \delta,|x-x_0|&amp;lt;\delta \Rightarrow |p(u)-p(x_0)|&amp;lt;\varepsilon \Rightarrow (*)\leq \frac{1}{|x-x_0|}\cdot \varepsilon |x-x_0|=\varepsilon$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【例】 (均匀分布) $[a,b]$ 的均匀分布 $U[a,b]$ 中，$X$ 的密度函数和分布函数为
$$
\begin{align}
p(x)&amp;amp;=\begin{cases}\frac{1}{b-a}&amp;amp;,a\leq x\leq b\\0&amp;amp;, otherwise\end{cases}\\
F(x)&amp;amp;=\begin{cases}0&amp;amp;,x&amp;lt;a\\\int_a^x\frac{du}{b-a}=\frac{x-a}{b-a}&amp;amp;,a\leq x\leq b\\1&amp;amp;,x&amp;gt;b\end{cases}
\end{align}
$$
【例】 (正态分布) $X$ 服从正态分布，记为 $X\sim N(\mu, \sigma^2)$ ，若
$$
p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}，-\infty&amp;lt;x&amp;lt;\infty
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\begin{align}
\left(\int_{-\infty}^{+\infty}p(x)dx\right)^2&amp;amp;=\left(\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\right)\left(\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-\mu)^2}{2\sigma^2}}dy\right)\\
&amp;amp;\overset{u=\frac{x-\mu}{\sigma},v=\frac{y-\mu}{\sigma}}{=}
\frac{1}{2\pi}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{-\frac{u^2+v^2}{2}}dudv\\
&amp;amp;\overset{u=r\cos \theta,v=r\sin \theta}{=}
\frac{1}{2\pi}\int_0^{2\pi}\int_{0}^{+\infty}e^{-\frac{r^2}{2}}rdrd\theta\\
&amp;amp;=1
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正态分布 $p(x)$ 的图像有以下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x)$ 关于 $x=\mu$ 轴对称。$F(\mu-a)+F(\mu+a)=1$。&lt;/li&gt;
&lt;li&gt;$\sigma^2$ 越大，$p(x)$ 的图像越平。(最大值变小，两头变高，方差 $\uparrow$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 $\mu=0,\sigma=1$ 时，$X$ 称为标准正态分布：
$$
\begin{align}
\varphi(x)&amp;amp;=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\\
\Phi(x)&amp;amp;=\int_{-\infty}^x\varphi(u)du
\end{align}
$$
$\fbox{Theorem 6.2}$ 设 $X\sim N(\mu, \sigma^2)$，则 $Z=\frac{X-\mu}{\sigma}\sim N(0,1)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
\begin{align}
F_Z(x)&amp;amp;=P(\frac{X-\mu}{\sigma}\leq x)=P(X\leq \sigma x+\mu)\\
&amp;amp;=\int_{-\infty}^{\sigma x+\mu}\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(v-\mu)^2}{2\sigma^2}}dv\\
&amp;amp;\overset{\frac{v-\mu}{\sigma}=z}{=}\int_{-\infty}^x\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{z^2}{2}}d(\sigma z+\mu)\\
&amp;amp;=\int_{-\infty}^x\varphi(u)du
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;利用该定理，我们可以将所有的正态分布转换为标准正态分布的计算：
$$
P(X\leq a)=P\left(\frac{X-\mu}{\sigma}\leq \frac{a-\mu}{\sigma}\right)=\Phi\left(\frac{a-\mu}{\sigma}\right)\\
P(a&amp;lt;X\leq b)=\Phi\left(\frac{b-\mu}{\sigma}\right)-\Phi\left(\frac{a-\mu}{\sigma}\right)
$$&lt;/p&gt;
&lt;p&gt;&amp;ldquo;3-$\sigma$&amp;rdquo; 准则：
$$
\begin{align}
P(|X-\mu|\leq \sigma)&amp;amp;=\Phi(1)-\Phi(-1)\approx 0.6826\\
P(|X-\mu|\leq 2\sigma)&amp;amp;\approx 0.9544\\
P(|X-\mu|\leq 3\sigma)&amp;amp;\approx 0.9974
\end{align}
$$&lt;/p&gt;
&lt;p&gt;当某个样本超过均值三个标准差以上时，可以怀疑它存在一些问题。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 6.3}$
$$
(\frac{1}{x}-\frac{1}{x^3})\varphi(x)\leq 1-\Phi(x)\leq \frac{1}{x}\varphi(x)
$$
且当 $x$ 充分大时，$1-\Phi(x)\approx \frac{1}{x}\varphi(x)$。&lt;/p&gt;
&lt;p&gt;【例】 (指数分布) 称 $X$ 服从参数为 $\lambda(\lambda &amp;gt; 0)$ 的指数分布，记为 $X\sim E(\lambda)$，若
$$
\begin{align}
p(x)&amp;amp;=\begin{cases}
\lambda e^{-\lambda x}&amp;amp;,x\geq 0\\
0&amp;amp;, x&amp;lt;0
\end{cases}\\
F(x)&amp;amp;=\begin{cases}
\int_0^x \lambda e^{-\lambda u}du=1-e^{-\lambda x}&amp;amp;, x&amp;gt;0\\
0&amp;amp;, x&amp;lt;0
\end{cases}
\end{align}
$$
注：指数分布具有无记忆性，i.e. $P(X&amp;gt;s+t|x&amp;gt;t)=P(X&amp;gt;s)$。指数分布和几何分布是唯二具有无记忆性的分布。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 07: Distribution of Random Variable Function</title>
      <link>/posts/coursenotes/nju-probability/lec07/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec07/</guid>
      <description>&lt;p&gt;设 $X$ 是一个随机变量，函数 $g(x):\mathbb R\rightarrow \mathbb{R}$。构造随机变量 $Y$，当 $X=x$ 时，$Y=g(x)$，称 $Y$ 是 $X$ 的函数，记为 $Y=g(X)$。在已知 $X$ 的分布时，我们希望求出 $Y$ 的分布。&lt;/p&gt;
&lt;h2 id=&#34;discrete-situation&#34;&gt;Discrete Situation&lt;/h2&gt;
&lt;p&gt;设 $X$ 是一个离散型随机变量，其分布律为&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$x_1$&lt;/th&gt;
&lt;th&gt;$x_2$&lt;/th&gt;
&lt;th&gt;$\cdots$&lt;/th&gt;
&lt;th&gt;$x_k$&lt;/th&gt;
&lt;th&gt;$\cdots$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$p_1$&lt;/td&gt;
&lt;td&gt;$p_2$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;td&gt;$p_k$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;那么 $Y\in {g(x_k)}_{k=1}^n$，去重后可以写成 ${y_1,y_2,\cdots,y_k,\cdots}$，显然 $Y$ 也是离散型随机变量。&lt;/p&gt;
&lt;p&gt;考虑 $P(Y=y)=P(g(X)=y)=P(x\in{x|g(x)=y})$，由可列可加性可知 $P(Y=y)=\sum_{x:g(x)=y}P(X=x)$。&lt;/p&gt;
&lt;p&gt;【例】$X$ 是离散型随机变量，$P(X=0)=0$，对于任意 $k\in \mathbb{N}$，$P(X=k)=P(X=-k)=p^k$。求 $Y=X^2$ 的分布律。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：首先解出 $p$：$\sum_{k=1}^\infty P(X=\pm k)=\sum_{k=1}^\infty 2P(X=k)=2\sum_{k=1}^\infty p^k=1$，解得 $p=\frac{1}{3}$。&lt;/p&gt;
&lt;p&gt;显然 $Y$ 的取值只能是正整数。对于任意 $n\in \mathbb{N}$，有
$$
P(Y=n)=P(X^2=n)=P(X=\pm \sqrt{n})=\begin{cases}2(\frac{1}{3})^{\sqrt{n}}&amp;amp;, \sqrt n\in \mathbb{N}\\0&amp;amp;, otherwise\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：可以看出即使 $P(X=k)\neq P(X=-k)$，只要 $P(X=\pm k)=2p^k$，$Y$ 的分布就是上述结果。也就是说，随机变量的分布和随机变量函数的分布并不是一一映射。&lt;/p&gt;
&lt;h2 id=&#34;continuous-situation&#34;&gt;Continuous Situation&lt;/h2&gt;
&lt;p&gt;设 $X$ 为连续型随机变量，$y=g(x)$ 为连续函数，$Y=g(X)$，一般地，可如下求 $Y$ 的分布函数 $F_Y(y)$ 和密度函数 $P_Y(y)$：
$$
F_Y(y)=P(Y\leq y)=P(g(X)\leq y)=P(x\in {x|g(x)\leq y})=\int_{x:g(x)\leq y} p_X(x)dx
$$
最后的积分式含参数 $y$，结果是关于 $y$ 的表达式。若 $F_Y(y)$ 可导，则 $p_Y(y)=F_Y&amp;rsquo;(y)$。&lt;/p&gt;
&lt;p&gt;【例】$X\sim N(0,1)$，求 $Y=X^2$ 的分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：注意到 $Y\geq 0$，所以对于任意 $y&amp;lt;0$，$F_y(y)=0$。&lt;/p&gt;
&lt;p&gt;对于 $y\geq 0$，有
$$
F_Y(y)=P(Y\leq y)=P(X^2\leq y)=P(-\sqrt{y}\leq X\leq \sqrt{y})=F_X(\sqrt{y})-F_X(-\sqrt{y})
$$
欲求 $p_Y(y)$，我们要对 $F_Y(y)$ 求导，注意使用链式法则：
$$
\begin{align}
P_Y(y)&amp;amp;=F_Y&amp;rsquo;(y)=F_X&amp;rsquo;(\sqrt{y})-F_X&amp;rsquo;(-\sqrt{y})\\
&amp;amp;=p_X(\sqrt{y})\frac{1}{2\sqrt{y}}-p_X(-\sqrt{y})\frac{-1}{2\sqrt{y}}\\
&amp;amp;=\frac{1}{\sqrt{y}}p_X(\sqrt{y})=\frac{1}{\sqrt{2\pi y}}e^{-\frac{y}{2}}
\end{align}
$$
综上，
$$
p_Y(y)=\begin{cases}\frac{1}{\sqrt{2\pi y}}e^{-\frac{y}{2}}&amp;amp;,y\geq 0\\0&amp;amp;,y&amp;lt;0\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：$Y=X^2$ 称为服从一个自由度的 $\chi^2$ 分布。$\chi^2$ 分布是统计学中的一个重要分布。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;上述做法需要先计算 $F_Y(y)$ 再求导计算 $p_Y(y)$。若 $y=g(x)$ 严格单调，则对 $g(x)$ 加以一些可导条件，可以直接计算密度函数 $p_Y(y)$。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 7.1}$ 设 $X$ 为连续型随机变量，密度函数为 $p_X(x)$。设 $y=g(x)$ 严格单调处处可导且恒有 $g&amp;rsquo;(x)&amp;gt;0$ 或 $g&amp;rsquo;(x)&amp;lt;0$，则 $Y=g(X)$ 也为连续型随机变量，且
$$
p_Y(y)=\begin{cases}
p_X(g^{-1}(y))\cdot \left|(g^{-1}(y))&amp;rsquo;\right|&amp;amp;,Y可取到y\\
0&amp;amp;,Y取不到y
\end{cases}
$$
其中 $g^{-1}(y)$ 是 $g(x)$ 的反函数。&lt;/p&gt;
&lt;p&gt;注：当 $g(x)$ 单调且 $g&amp;rsquo;(x_0)$ 存在非零，可以证明 $g^{-1}(y)$ 在 $y_0=g(x_0)$ 处可导。但即使严格单调，$g&amp;rsquo;(x)$ 仍可能在某个 $x_0$ 取到 0 (如 $f(x)=x^3$ 的 $x=0$ 处)，此时 $g^{-1}(y)$ 在 $y_0=g(x_0)$ 处的可导性存在问题，故要求 $g&amp;rsquo;(x)$ 不能取 0。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里仅证单调递增的情况，单调递减大同小异 (最终使结论添上绝对值)：
$$
F_Y(y_0)=P(Y\leq y_0)=P(g(X)\leq y_0)=\int_{-\infty}^{g^{-1}(y_0)}p_X(x)dx
$$
作变量替换 $x=g^{-1}(y)$：
$$
F_Y(y_0)=\int_{-\infty}^{y_0}p_X(g^{-1}(y))dg^{-1}(y)=\int_{-\infty}^{y_0}p_X(g^{-1}(y))(g^{-1}(y))&amp;lsquo;dy
$$
根据微积分基本定理，
$$
p_Y(y_0)=F_Y&amp;rsquo;(y_0)=p_X(g^{-1}(y_0))(g^{-1}(y_0))&amp;rsquo;
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】$X\sim N(\mu, \sigma^2)$，$Z=\frac{X-\mu}{\sigma}$，即 $g(x)=\frac{x-\mu}{\sigma}$，$Z=g(X)$。求 $Z$ 的分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：显然 $g(x)$ 单调递增，且 $g&amp;rsquo;(x)=\frac{1}{\sigma}&amp;gt;0$。$g^{-1}(z)=\sigma z+\mu$。
$$
\begin{align}
p_Z(z)&amp;amp;=p_X(g^{-1}(z))\left|(g^{-1}(z))&amp;rsquo;\right|=p_X(\sigma z+\mu)\cdot \sigma\\
&amp;amp;=\sigma\cdot \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\sigma z+\mu-\mu)^2}{2\sigma^2}}\\
&amp;amp;=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
\end{align}
$$
即 $Z\sim N(0,1)$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】若 $X$ 服从 $\left(-\frac{\pi}{2},\frac{\pi}{2}\right)$ 上的均匀分布，$Y=\tan X$，求 $Y$ 的分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：$y=g(x)=\tan x$，$x=g^{-1}(y)=\arctan y$，$(\arctan y)&amp;rsquo;=\frac{1}{1+y^2}$。于是
$$
\begin{align}
p_Y(y)&amp;amp;=p_X(\arctan y)\cdot \left|\frac{1}{1+y^2}\right|\\
&amp;amp;= \frac{1}{\pi}\cdot \frac{1}{1+y^2}
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：$Y$ 的分布称为柯西分布。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 08: Mult-dimensional Random Variable</title>
      <link>/posts/coursenotes/nju-probability/lec08/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec08/</guid>
      <description>&lt;h2 id=&#34;distribution-function-of-2-dimensional-random-variable&#34;&gt;Distribution Function of 2-Dimensional Random Variable&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 8.1}$ 设 $X,Y$ 是定义在 $(\Omega, \mathscr F, P)$ 上的随机变量，则称 $(X,Y)$ 为 $(\Omega, \mathscr F, P)$ 上的&lt;strong&gt;二维随机变量&lt;/strong&gt;。对于任意 $x,y\in \mathbb R$，称
$$
F(x,y)=P({X\leq x}\cap {Y\leq y})=P(X\leq x,Y\leq y)
$$
为 $(X,Y)$ 的 &lt;strong&gt;(联合) 分布函数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;注：从图像上来看，$F(x_0,y_0)$ 即为落在点 $(x_0,y_0)$ 左下方无穷矩形的概率。&lt;/p&gt;
&lt;p&gt;$F(x,y)$ 的若干性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;固定 $x_0$，$F(x_0,y)$ 单调不减；固定 $y_0$，$F(x,y_0)$ 单调不减。&lt;/p&gt;
&lt;p&gt;(推论：$\forall x_1&amp;gt;x_2,y_1&amp;gt;y_2,F(x_1,y_1)\geq F(x_2,y_2)$。)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固定 $x_0$，$\lim_{y\rightarrow -\infty}F(x_0,y)=F(x_0,-\infty)=0$，类似可得 $F(-\infty,-\infty)=0,F(+\infty,+\infty)=1$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固定 $x_0$，$F(x_0,y)$ 处处有左极限且右连续 (càdlàg)，固定 $y_0$ 时亦然。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\forall x_1&amp;gt;x_2,y_1&amp;gt;y_2,F(x_1,y_1)-F(x_1,y_2)-F(x_2,y_1)+F(x_2,y_2)\geq 0$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：上述性质也是一个函数是某个 $(X,Y)$ 的分布函数的充分条件。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 8.2}$ 给定 $(X,Y)$，$X$ 或 $Y$ 的分布函数称为&lt;strong&gt;边缘概率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 $F(x,y)$ 已知的情况下，边缘概率可以直接求出：
$$
F_X(x)=P(X\leq x)=P(X\leq x,Y\in \mathbb{R})=\lim_{y\rightarrow +\infty}F(x,y)
$$
$\fbox{Definition 8.3}$ 设随机变量 $X,Y$ 满足对于任意 $x,y\in \mathbb{R}$，${X\leq x}$ 与 ${Y\leq y}$ 独立，即 $P(X\leq x,Y\leq y)=P(X\leq x)P(Y\leq y), F(x,y)=F_X(x)F_Y(y)$，则称 $X,Y$ 相互独立。&lt;/p&gt;
&lt;p&gt;注：虽然上述定义只对一部分事件的独立性进行了描述，但更复杂的事件的情形也是可以推出的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$$
\begin{align}
P(x_1&amp;lt;X\leq x_2,y_1&amp;lt;Y\leq y_2)&amp;amp;= F(x_2,y_2)-F(x_2,y_1)-F(x_1,y_2)+F(x_1,y_1)\\
&amp;amp;=F_X(x_2)F_Y(y_2)-F_X(x_2)F_Y(y_1)-F_X(x_1)F_Y(y_2)+F_x(x_1)F_Y(y_1)\\
&amp;amp;=(F_X(x_2)-F_X(x_1))(F_Y(y_2)-F_Y(y_1))\\
&amp;amp;=P(x_1&amp;lt;X\leq x_2)P(y_1&amp;lt;Y\leq Y_2)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设 $I_n,J_m$ 是两列左开右闭区间，且 $\forall n\leq k,I_n\cap I_k=\emptyset,J_n\cap J_k=\emptyset$，那么
$$
\begin{align}
P(X\in \bigcup_{n=1}^\infty I_n,Y\in \bigcup_{m=1}^\infty J_m)&amp;amp;=P(\bigcup_{n=1}^\infty \bigcup_{m=1}^\infty {X\in I_n, Y\in J_m})\\
&amp;amp;\overset{可列可加性}{=}\sum_{n=1}^\infty \sum_{m=1}^\infty P(X\in I_n,Y\in J_m)\\
&amp;amp;\overset{上一条结论}{=}\sum_{n=1}^\infty \sum_{m=1}^\infty P(X\in I_n)P(Y\in J_m)\\
&amp;amp;= \left(\sum_{n=1}^\infty P(X\in I_n)\right)\left(\sum_{m=1}^\infty P(Y\in J_m)\right)\\
&amp;amp;=P(X\in \bigcup_{n=1}^\infty I_n)P(Y\in \bigcup_{m=1}^\infty J_m)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\fbox{Theorem 8.4}$ 若 $X,Y$ 独立，$f(x),g(y)$ 为连续函数或分段连续函数，那么 $f(X)$ 和 $g(Y)$ 也相互独立。&lt;/p&gt;
&lt;h2 id=&#34;n-dimensional-case&#34;&gt;n-Dimensional Case&lt;/h2&gt;
&lt;p&gt;高维情况和二维情况的定义，性质基本相同。求解其 $k$ 维边缘概率时，将剩下的 $n-k$ 维都推到 $+\infty$ 即可。&lt;/p&gt;
&lt;p&gt;$n$ 维随机变量的独立性要求：$\forall (x_1,x_2,\cdots x_n)\in \mathbb{R}^n, P(X_1\leq x_1,\cdots X_n\leq x_n)=\prod_{k=1}^nP(X_k\leq x_k)$。联想 $n$ 个事件独立的定义，该条件似乎更弱：$n$ 个事件的独立性对所有子集都做了约束。但事实上上述条件也对所有子集做了约束：只要将某些维的 $x_i$ 推到 $+\infty$ ，则是一个对子集的约束。&lt;/p&gt;
&lt;h2 id=&#34;2-dimensional-discrete-random-variable&#34;&gt;2-Dimensional Discrete Random Variable&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 8.5}$ 若二维随机变量 $(X,Y)$ 的可能取值是有限多个或可列无限个，则称 $(X,Y)$ 为离散型二维随机变量，设 $(X,Y)$ 所有可能取值为 $(x_i,y_j), \forall i,j=1,2,\cdots$，则称 $P(X=x_i,Y=y_j),\forall i,j=1,2,\cdots$ 为 $(X,Y)$ 的联合分布律，可用表格表示为&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;X/Y&lt;/th&gt;
&lt;th&gt;$y_1$&lt;/th&gt;
&lt;th&gt;$\cdots$&lt;/th&gt;
&lt;th&gt;$y_k$&lt;/th&gt;
&lt;th&gt;$\cdots$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$x_1$&lt;/td&gt;
&lt;td&gt;$p_{11}$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;td&gt;$p_{1k}$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\ddots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$x_k$&lt;/td&gt;
&lt;td&gt;$p_{k1}$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;td&gt;$p_{kk}$&lt;/td&gt;
&lt;td&gt;$\cdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\ddots$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;$(X,Y)$ 联合分布律的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\forall i,j, p_{ij}\geq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sum_{i=1}^\infty \sum_{j=1}^\infty p_{ij}=1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;边缘分布的求法：
$$
\begin{align}
P(X=x_i)&amp;amp;=P\left(X=x_i,Y\in \bigcup_{j=1}^\infty {y_j}\right)=P\left(\bigcup_{j=1}^\infty {X=x_i,Y=y_j}\right)\\
&amp;amp;=\sum_{j=1}^\infty P(X=x_i,Y=y_j)=\sum_{j=1}^\infty p_{i,j}\overset{def}{=} P_{i\cdot} \\
P(Y=y_j)&amp;amp;=\sum_{i=1}^\infty P(X=x_i,Y=y_j)\overset{def}{=}P_{\cdot j}
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再考虑离散型二维随机变量的分布函数及其边缘分布函数：
$$
F(x,y)=\sum_{i:x_i\leq x}\sum_{j:y_j\leq y}p_{i,j}\\
F_X(x)=\sum_{i:x_i\leq x}p_{i\cdot}=\sum_{i:x_i\leq x}\sum_{j=1}^\infty p_{i,j}
$$
一般地，对于区域 $D\subset \mathbb{R}^2$，有
$$
P((x,y)\in D)=\sum_{i,j:(x_i,y_j)\in D}p_{i,j}
$$
对于离散型随机变量的独立性，$X$ 和 $Y$ 独立当且仅当对于任意 $i,j$，$P(X=x_i,Y=y_j)=P(X=x_i)P(Y=y_j)$，或者写成 $p_{i,j}=p_{i\cdot }p_{\cdot j}$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 09: 2-Dimensional Continuous Random Variable</title>
      <link>/posts/coursenotes/nju-probability/lec09/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec09/</guid>
      <description>&lt;h2 id=&#34;examples-of-discrete-2-d-random-variable&#34;&gt;Examples of Discrete 2-D Random Variable&lt;/h2&gt;
&lt;p&gt;【例】 (三项分布) 若二维离散型随机变量 $(X,Y)$ 的分布律为
$$
P(X=i,Y=j)=\frac{n!}{i!j!(n-i-j)!}p_1^ip_2^j(1-p_1-p_2)^{n-i-j}
$$
其中 $i,j=0,1,\cdots,n,i+j\leq n,0\leq p_1,p_2,p_1+p_2\leq 1$，则称 $(X,Y)$ 服从参数 $n,p_1,p_2$ 的三项分布。&lt;/p&gt;
&lt;p&gt;概率背景：在 $n$ 重独立重复试验中，每次试验有三种可能的结果 $A_1,A_2,A_3$，$P(A_1)=p_1,P(A_2)=p_2$。令 $A_1$ 发生次数为 $X$，$A_2$ 发生次数为 $Y$，则 $(X,Y)$ 服从上述三项分布。&lt;/p&gt;
&lt;p&gt;三项分布的边缘分布是二项分布，因为在计算 $P(X=k)$ 时，我们不关心在没有命中 $A_1$ 时命中的是 $A_2$ 还是 $A_3$，相当于只剩下了两种事件。我们也可以从代数上进行验证：
$$
\begin{align}
P(X=k)&amp;amp;=P(X=k,0\leq Y\leq n-k)=\sum_{i=0}^{n-k}P(X=k,Y=i)\\
&amp;amp;=\sum_{i=0}^{n-k}\frac{n!}{k!i!(n-k-i)!}p_1^kp_2^i(1-p_1-p_2)^{n-k-i}\\
&amp;amp;=\frac{n!}{k!(n-k)!}p_1^k\sum_{i=0}^{n-k}\frac{(n-k)!}{i!(n-k-i)!}p_2^i(1-p_1-p_2)^{n-k-i}\\
&amp;amp;=\frac{n!}{k!(n-k)!}p_1^k(p_2+(1-p_1-p_2))^{n-k}\\
&amp;amp;=\binom{n}{k}p_1^k(1-p_1)^{n-k}
\end{align}
$$
一般地，可以定义 $k$ 项分布。记 $P(A_1)=p_1,\cdots, P(A_k)=p_k$，$0\leq p_1,\cdots, p_k,\sum_{i=1}^kp_i\leq 1$，记 $X_j$是 $n$ 次试验中 $A_j$ 发生的次数，则 $(X_1,\cdots, X_k)$ 的分布律为
$$
P(X_1=j_1,\cdots, X_k=j_k)=\frac{n!}{j_1!\cdots j_k!}p_1^{j_1}\cdots p_k^{j_k}
$$
其中 $0\leq j_1,\cdots, j_k\leq n,\sum_{i=1}^kj_i=n$。&lt;/p&gt;
&lt;p&gt;【例】 (二维超几何分布) 若二维离散型随机变量 $(X,Y)$ 的分布律为
$$
P(X=n_1,Y=n_2)=\frac{\binom{N_1}{n_1}\binom{N_2}{n_2}\binom{N_3}{n_3}}{\binom{N}{n}}
$$
其中 $0\leq n_1\leq N_1,0\leq n_2\leq N_2,0\leq n_3\leq N_4,n_1+n_2+n_3=n,N_1+N_2+N_3=N$，则称 $(X,Y)$ 服从二维超几何分布。&lt;/p&gt;
&lt;p&gt;概率背景：设 $N$ 个物品分为三类，各有 $N_1,N_2,N_3$ 个，不放回地挑 $n$ 个，第一类抽到 $X$ 个，第二类抽到 $Y$ 个，则 $(X,Y)$ 服从上述二维超几何分布。&lt;/p&gt;
&lt;p&gt;类似地，二维超几何分布的边缘分布是一维的超几何分布：
$$
\begin{align}
P(X=n_1)&amp;amp;=P(X=n_1,0\leq Y\leq \min{N_2,n-n_1})\\
&amp;amp;=\sum_{k=0}^{\min{N_2,n-n_1}}\frac{\binom{N_1}{n_1}\binom{N_2}{k}\binom{N_3}{n-n_1-k}}{\binom{N}{n}}\\
&amp;amp;=\frac{\binom{N_1}{n_1}}{\binom{N}{n}}\left(\sum_{k=0}^{\min{N_2,n-n_1}}\binom{N_2}{k}\binom{N_3}{n-n_1-k}\right)\\
&amp;amp;=\frac{\binom{N_1}{n_1}}{\binom{N}{n}}\binom{N_2+N_3}{n-n_1}\\
&amp;amp;=\frac{\binom{N_1}{n_1}\binom{N-N_1}{n-n_1}}{\binom{N}{n}}\qquad (一维超几何分布)
\end{align}
$$&lt;/p&gt;
&lt;h2 id=&#34;2-dimensional-continuous-random-variable&#34;&gt;2-Dimensional Continuous Random Variable&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 9.1}$ 对于一个二维随机变量 $(X,Y)$ 的分布函数 $F(x,y)$，若存在非负可积函数 $p(x,y)$，使得对于任意 $(x,y)\in \mathbb{R}^2$，有
$$
F(x,y)=\int_{-\infty}^x\int_{-\infty}^yp(u,v)dudv
$$
则称 $(X,Y)$ 是&lt;strong&gt;二维连续型随机变量&lt;/strong&gt;，称 $p(x,y)$ 是 $(X,Y)$ 的 (联合) 概率密度函数。&lt;/p&gt;
&lt;p&gt;$p(x,y)$ 的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\forall (x,y)\in \mathbb{R}^2,p(x,y)\geq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}p(x,y)dxdy=F(+\infty,+\infty)=1
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设 $D\subseteq \mathbb{R}^2$，则 $(X,Y)$ 落入 $D$ 中的概率为
$$
P((X,Y)\in D)=\iint_D p(x,y)dxdy
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $p(x,y)$ 在 $(x_0,y_0)$ 附近连续，则有
$$
\left.\frac{\partial^2 F(x,y)}{\partial x\partial y}\right|_{(x,y)=(x_0,y_0)}=p(x_0,y_0)
$$
注：和一维的情形类似，在二维连续型随机变量中，$p(x_0,y_0)$ 不能理解为 $x=x_0,y=y_0$ 的概率。事实上，$\forall x_0,y_0\in \mathbb{R},P(X=x_0,Y=y_0)=0$。$p(x_0,y_0)$ 只能理解为 $(X,Y)$ 落入 $(x_0,y_0)$ 附近一小块面积的概率的近似值，即
$$
\int_{x_0}^{x_0+\Delta x}\int_{y_0}^{y_0+\Delta y}p(x,y)dxdy\approx p(x_0,y_0)\Delta x\Delta y
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;边缘分布和密度函数的求法：&lt;/p&gt;
&lt;p&gt;$X$ 的边缘分布函数为
$$
F_X(x)=F(X,+\infty)=\int_{-\infty}^x\left[\int_{-\infty}^{+\infty}p(u,y)dy\right]du
$$
$X$ 的边缘密度函数为
$$
p_X(x)=\frac{d}{dx}F_X(x)=\int_{-\infty}^{\infty}p(x,y)dy
$$
(直观地想，离散时边缘密度的求法是固定 $x$，对所有可能的 $y$ 求和，那么在连续型中将求和换作积分即可。)&lt;/p&gt;
&lt;p&gt;$Y$ 的边缘分布和密度函数求法类似。&lt;/p&gt;
&lt;p&gt;关于独立性：对于一般的二维随机变量，$X,Y$ 的独立性定义为
$$
\forall (x,y)\in \mathbb{R}^2,F(x,y)=F_X(x)F_Y(y)
$$
在连续的情形中，即
$$
\int_{-\infty}^x\int_{-\infty}^yp(u,v)dudv=\int_{-\infty}^xp_X(u)du\int_{-\infty}^yp_Y(v)dv=\int_{-\infty}^x\int_{-\infty}^yp_X(u)p_Y(v)dudv
$$
因为上式对于任意 $x,y$ 均成立，所以独立性条件可以用密度函数直接表示为
$$
\forall (x,y)\in \mathbb{R}^2,p(x,y)=p_X(x)p_Y(y)
$$&lt;/p&gt;
&lt;h3 id=&#34;n-dimensional-continuous-random-variable&#34;&gt;n-Dimensional Continuous Random Variable&lt;/h3&gt;
&lt;p&gt;$\fbox{Definition 9.2}$ 设 $n$ 维随机变量 $(X_1,\cdots, X_n)$ 的分布函数为 $F(x_1,\cdots, x_n)$，若存在非负可积函数 $p(x_1,\cdots, x_n)$ 使得
$$
\forall (x_1,\cdots,x_n)\in \mathbb{R}^n,F(x_1,\cdots, x_n)=\int_{-\infty}^{x_1}\cdots \int_{-\infty}^{x_n} p(u_1,\cdots, u_n)du_1\cdots du_n
$$
则称 $(X_1,\cdots, X_n)$ 为 $n$ 维连续型随机变量，$p(x_1,\cdots, x_n)$ 为 (联合) 概率密度函数。&lt;/p&gt;
&lt;h2 id=&#34;examples-of-2-d-continuous-random-variable&#34;&gt;Examples of 2-D Continuous Random Variable&lt;/h2&gt;
&lt;p&gt;【例】 (二维均匀分布) 设 $D\subset \mathbb{R}^2$ 为有界区域，面积为 $S_D$。若 $(X,Y)$ 的联合密度函数为
$$
p(x,y)=\begin{cases}
\frac{1}{S_D}&amp;amp;, (X,Y)\in D\\
0&amp;amp;, (X,Y)\in D
\end{cases}
$$
则称 $(X,Y)$ 服从区域 $D$ 上的二维均匀分布。&lt;/p&gt;
&lt;p&gt;(注：事实上该定义和一维情况相同，一维情况的测度是长度，二维情况的测度是面积。)&lt;/p&gt;
&lt;p&gt;该分布的均匀性体现在：对于任意 $A\subseteq D$，若 $A$ 的面积为 $S_A$，则 $P((X,Y)\in A)=\frac{S_A}{S_D}$，与 $A$ 的形状、位置无关，只与 $A$ 的面积有关 (类比二维几何概型)。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 10: 2-Dimensional Normal Distribution and Random Variable Function</title>
      <link>/posts/coursenotes/nju-probability/lec10/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec10/</guid>
      <description>&lt;p&gt;$\fbox{Definition 10.1}$ 若二维随机变量 $(X,Y)$ 的联合密度为
$$
p(x,y)=\frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}[(\frac{x-\mu_1}{\sigma_1})^2-2\rho(\frac{x-\mu_1}{\sigma_1})(\frac{y-\mu_2}{\sigma_2})+(\frac{y-\mu_2}{\sigma_2})^2]}
$$
其中 $\mu_1,\mu_2\in \mathbb R$，$\sigma_1,\sigma_2&amp;gt;0$，$|\rho|&amp;lt;1$，则称 $(X,Y)$ 服从二维正态分布。记为 $(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ (这里的每个参数都有具体含义)。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 10.2}$ 若 $(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，则其边缘分布为 $X\sim N(\mu_1,\sigma_1^2)$，$Y\sim N(\mu_2,\sigma_2^2)$。&lt;/p&gt;
&lt;p&gt;证明：根据对称性，我们仅需证明 $X$ 的边缘分布为正态分布。&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
p_X(x)&amp;amp;=\int_{-\infty}^{+\infty}p(x,y)dy\\
&amp;amp;=\frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left[\left(\frac{y-\mu_2}{\sigma_2\sqrt {1-\rho^2}}\right)^2-\frac{2\rho}{1-\rho^2}\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)\right]}dy\\
&amp;amp;\overset{配方}{=} \frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left[\left(\frac{y-\mu_2}{\sigma_2\sqrt {1-\rho^2}}\right)^2-\frac{2\rho}{1-\rho^2}\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)+\left(\frac{x-\mu_1}{\sigma_1\sqrt{1-\rho^2}}\right)^2\right]+\frac{1}{2}\frac{\rho^2(x-\mu_1)^2}{\sigma_1^2(1-\rho^2)}}dy\\
&amp;amp;=\frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2}(\frac{x-\mu_1}{\sigma_1})^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left(\frac{y-\mu_2}{\sigma_2 \sqrt{1-\rho^2}}-\frac{\rho (x-\mu_1)}{\sigma_1 \sqrt{1-\rho^2}}\right)^2}dy\\
&amp;amp;\overset{u=\frac{y-\mu_2}{\sigma_1\sqrt{1-\rho^2}}-\frac{\rho(x-\mu_1)}{\sigma_1\sqrt{1-\rho^2}}}{=}\frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}}e^{\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}u^2}d(\sigma_2\sqrt{1-\rho^2}u)\\
&amp;amp;=\frac{1}{2\pi \sigma_1}e^{-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}\int_{-\infty}^{+\infty}e^{-\frac{u^2}{2}}du\\
&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma_1}e^{-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;因此 $X\sim N(\mu_1,\sigma_1^2)$。$\blacksquare$&lt;/p&gt;
&lt;p&gt;注：二维正态分布可以用矩阵描述成如下更简洁的形式：记 $\Sigma=\begin{bmatrix}\sigma_1^2 &amp;amp; \rho\sigma_1\sigma_2\\\rho \sigma_1\sigma_2 &amp;amp; \sigma_2^2 \end{bmatrix}$，则
$$
p(x,y)=\frac{1}{2\pi\sqrt{|\Sigma|}}e^{-\frac{1}{2}(x-\mu_1,y-\mu_2)\Sigma^{-1}\begin{pmatrix}x-\mu_1\\y-\mu_2\end{pmatrix}}
$$
二维正态分布的边缘分布都是正态分布，那么两个边缘分布都是正态分布的二维分布是否一定是二维正态分布呢？答案是否定的。考虑如下分布：令 $\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$，$g(x)=\begin{cases}\cos x&amp;amp;, |x|\leq \pi\\ 0&amp;amp;,|x| &amp;gt; \pi\end{cases}$，可以看到 $\varphi$ 就是一维标准正态分布的密度函数。令
$$
p(x,y)=\varphi(x)\varphi(y)+\frac{1}{2\pi} e^{-\pi^2}g(x)g(y)
$$
这显然不是联合正态密度函数，但我们逐一验证二维分布的条件和边缘分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$p(x,y)\geq 0$：我们只需考虑 $|x|\leq \pi, |y|\leq \pi$ 的方形区域：
$$
p(x,y)=\frac{1}{2\pi}\left(e^{-\frac{x^2+y^2}{2}}+e^{-\pi^2}\cos x\cos y\right)\geq \frac{1}{2\pi}\left(e^{-\frac{x^2+y^2}{2}}-e^{-\pi^2}\right)\geq 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$
\begin{align}
p_X(x)&amp;amp;=\int_{-\infty}^{+\infty}\varphi(x)\varphi(y)dy+\frac{1}{2\pi}e^{-\pi^2}\int_{-\infty}^{+\infty}g(x)g(y)dy\\
&amp;amp;=\varphi(x)+\frac{1}{2\pi}e^{-\pi^2}g(x)\int_{-\pi}^{\pi}\cos y dy\\
&amp;amp;=\varphi(x)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$
\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}p(x,y)dy\right)dx=\int_{-\infty}^{+\infty}\varphi(x)dx=1
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\fbox{Theorem 10.3}$ 若 $(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ ，则 $X,Y$ 独立 $\Leftrightarrow$ $\rho =0$。&lt;/p&gt;
&lt;p&gt;证明：$\Leftarrow$：当 $\rho=0$ 时，
$$
p(x,y)=\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{1}{2}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2+\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right]}=\frac{1}{\sqrt{2\pi}\sigma_1}e^{-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2}\cdot \frac{1}{\sqrt{2\pi}\sigma_2}e^{-\frac{1}{2}\left(\frac{y-\mu_2}{\sigma_2}\right)^2}=p_X(x)p_Y(y)
$$
$\Rightarrow$：当 $X,Y$ 独立时，取 $x=\mu_1,y=\mu_2$，则
$$
p(\mu_1,\mu_2)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}=p_X(x)p_Y(y)=\frac{1}{2\pi\sigma_1\sigma_2}
$$
所以 $\sqrt{1-\rho^2}=1$，$\rho = 0$。$\blacksquare$&lt;/p&gt;
&lt;p&gt;【习题3-5】二维随机变量 $(X,Y)$ 的联合密度函数为
$$
p(x,y)=\begin{cases}xe^{-y}&amp;amp;,0&amp;lt;x&amp;lt;y\\0&amp;amp;,otherwise\end{cases}
$$
求 $(X,Y)$ 的联合分布函数。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;(ATTENTION：$F(x,y)$ 的含义是落在 $(x,y)$ 左下方矩形的概率，切勿在 $p(x,y)$ 为零时直接推测 $F(x,y)$ 也为零！)&lt;/u&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：当 $x&amp;lt;0$ 或 $y&amp;lt;0$ 时，$F(x,y)=0$。&lt;/p&gt;
&lt;p&gt;当 $0&amp;lt;x\leq y$ 时，我们要积的是一个梯形区域，
$$
\begin{align}
F(x,y)&amp;amp;=\int_0^x\left(\int_{u}^{y}ue^{-v}dv\right)du\\
&amp;amp;=\int_{0}^xu(e^{-u}-e^{-y})du\\
&amp;amp;=\left(\int_0^xue^{-u}du\right)-\frac{1}{2}x^2e^{-y}\\
&amp;amp;=1-e^{-x}-xe^{-x}-\frac{1}{2}x^2e^{-y}
\end{align}
$$
考虑到 $F$ 的连续性，当 $0&amp;lt;y&amp;lt;x$ 时，$F(x,y)=F(y,y)=1-e^{-y}-ye^{-y}-\frac{1}{2}y^2e^{-y}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;2-dimensional-random-variable-function&#34;&gt;2-Dimensional Random Variable Function&lt;/h2&gt;
&lt;p&gt;设 $(X,Y)$ 为二维随机变量，$z=g(x,y)$ 为二元实函数，定义 $(X,Y)$ 的函数 $Z=g(X,Y)$，即 $Z$ 在 $(X,Y)=(x,y)$ 时取值 $g(x,y)$，则 $Z$ 是一个随机变量。下面给出求 $Z$ 的分布的方法。
$$
F_Z(z)=P(Z\leq z)=P(g(x,y)\leq z)
$$
对于离散情形，记 $(X,Y)$ 的分布律为 $P(X=x_i,Y=y_j)=p_{ij},i,j=1,2,\cdots$，则 $Z$ 也为离散型随机变量。记 $Z$ 的可能取值为 $z_k,k=1,2,\cdots$，则
$$
\begin{align}
P(Z=z_k)&amp;amp;=P(g(X,Y)=z_k)=P((X,Y)\in {(x,y)|g(x,y)=z_k})\\
&amp;amp;= \sum_{i,j:g(x_i,y_j)=z_k}P(X=x_i,Y=y_j)=\sum_{i,j:g(x_i,y_j)=z_k}p_{ij}
\end{align}
$$
对于连续情形，记 $(X,Y)$ 的密度函数为 $p(x,y)$，则
$$
\begin{align}
F_Z(z)&amp;amp;=P(g(X,Y)\leq z)=P((X,Y)\in {(x,y)|g(x,y)\leq z})\\
&amp;amp;=\iint_{{(x,y):g(x,y)\leq z}}p(x,y)dxdy
\end{align}
$$
若 $F_Z(z)$ 可导，则 $p_Z(z)=F_Z&amp;rsquo;(z)$。&lt;/p&gt;
&lt;p&gt;对于混合情形，若 $X$ 为连续型随机变量，密度函数为 $p(x)$，$Y$ 为离散型随机变量，分布为 $P(Y=y_j)=q_j,j=1,2,\cdots$。那么
$$
\begin{align}
F_Z(z)&amp;amp;=P(g(X,Y)\leq z)=\sum_{j=1}^\infty P(g(X,Y)\leq z,Y=y_j)\\
&amp;amp;=\sum_{j=1}^\infty P(g(X,y_j)\leq z,Y=y_j)\\
&amp;amp;=\sum_{j=1}^\infty q_jP(g(X,y_j)\leq z|Y=y_j)\quad (条件概率)
\end{align}
$$
若 $X,Y$ 独立，则上式可化为
$$
\sum_{j=1}^\infty P(g(X,y_j)\leq z)=\sum_{j=1}^\infty q_j\int_{x:g(x,y_j)\leq z}p(x)dx
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 11: Examples of 2-Dimensional Random Variable Function</title>
      <link>/posts/coursenotes/nju-probability/lec11/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec11/</guid>
      <description>&lt;p&gt;【例】 (顺序统计量, order statistics) 设 $X,Y$ 相互独立，分布函数分别为 $F_X(x)$ 和 $F_Y(y)$。令 $M=\max \{X,Y\}$，$N=\min \{X,Y\}$，现考虑 $M,N$ 的分布。
$$
\begin{align}
F_M(z)&amp;amp;=P(M\leq z)=P(\max {X,Y}\leq z)=P(X\leq z,Y\leq z)\overset{独立性}{=}P(X\leq z)P(Y\leq z)=F_X(z)F_Y(z)\\
F_N(z)&amp;amp;=P(N\leq z)=P(\min {X,Y}\leq z)=1-P(X&amp;gt;z,Y&amp;gt;z)=1-(1-F_X(z))(1-F_Y(z))
\end{align}
$$
注：(1) 设 $X,Y$ 只取整数，则 $P(M=n)=P(M\leq n)-P(M\leq n-1)=F_M(n)-F_M(n-1)$。&lt;/p&gt;
&lt;p&gt;(2) 上述结果可以推广到 $n$ 个独立随机变量 $X_1,\cdots, X_n$ 的情形，此时有
$$
\begin{align}
F_M(z)&amp;amp;=\prod_{k=1}^nF_{X_i}(z)\\
F_N(z)&amp;amp;=1-\prod_{k=1}^n(1-F_{X_i}(z))
\end{align}
$$
【例】 (和的分布) 令 $Z=X+Y$，考虑下列情形中 $Z$ 的分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X,Y$ 相互独立，且取值均为非负整数。此时 $X,Y$ 显然均为离散型随机变量，记 $P(X=k)=p_k$，$P(Y-k)=q_k$，$k=0,1,2,\cdots$。&lt;/li&gt;
&lt;li&gt;$(X,Y)$ 的密度函数为 $p(x,y)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于离散情形：
$$
P(Z=n)=P(X+Y=n)=\sum_{k=0}^nP(X=k,Y=n-k)=\sum_{k=0}^nP(X=k)P(Y=n-k)=\sum_{k=0}^np_kq_{n-k}
$$
上式称为 (离散) 卷积 (convolution) 公式。&lt;/p&gt;
&lt;p&gt;对于连续情形：
$$
\begin{align}
F_Z(z)&amp;amp;=P(X+Y\leq z)=\iint_{{(x,y):x+y\leq z}}p(x,y)dxdy\\
&amp;amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{z-x}p(x,y)dydx\\
&amp;amp;\overset{y=v-x}{=}\int_{-\infty}^{+\infty}\int_{-\infty}^zp(x,v-x)dvdx\\
&amp;amp;\overset{换序}{=}\int_{-\infty}^z\left(\int_{-\infty}^{+\infty}p(x,v-x)dx\right)dv
\end{align}
$$
因为 $\displaystyle{F_Z(z)=\int_{-\infty}^z p_Z(u)du}$，和上式对比，我们发现括号内的部分正好是 $Z$ 的密度函数。
$$
p_Z(v)=\int_{-\infty}^{+\infty}p(x,v-x)dx
$$
上式称为 (连续) 卷积公式。&lt;/p&gt;
&lt;p&gt;进一步地，若 $X,Y$ 独立，则 $p(x,y)=p_X(x)p_Y(y)$，因此 $Z$ 的密度为
$$
p_Z(v)=\int_{-\infty}^{+\infty}p_X(x)p_Y(v-x)dx
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;【例题】 设 $X\sim B(n_1,p),Y\sim B(n_2,p)$，且 $X,Y$ 独立，求 $X+Y$ 的分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：$P(X=k)=\binom{n_1}{k}p^k(1-p)^{n_1-k},k=0,\cdots,n_1$，$P(Y=k)=\binom{n_2}{k}p^k(1-p)^{n_2-k},k=0,\cdots n_2$。&lt;/p&gt;
&lt;p&gt;令 $Z=X+Y$，则
$$
\begin{align}
P(Z=n)&amp;amp;=\sum_{k=0}^nP(X=k,Y=n-k)=\sum_{k=0}^nP(X=k)P(Y=n-k)\\
&amp;amp;=\sum_{k=\max \{0,n-n_2\}}^{\min\{n,n_1\}}\binom{n_1}{k}p^k(1-p)^{n_1-k}\binom{n_2}{n-k}p^{n-k}(1-p)^{n_2-n+k}\\
&amp;amp;=p^n(1-p)^{n_1+n_2-n}\sum_{k=\max \{0,n-n_2\}}^{\min \{n,n_1\}}\binom{n_1}{k}\binom{n_2}{n-k}\\
&amp;amp;=p^n(1-p)^{n_1+n_2-n}\binom{n_1+n_2}{n}
\end{align}
$$
因此 $Z\sim B(n_1+n_2,p)$。&lt;/p&gt;
&lt;p&gt;在多次实验 $p$ 相同的情况下，这个结论是容易理解的：先做 $n_1$ 次再做 $n_2$ 次和一共做 $n_1+n_2$ 次是一样的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：上述结论可以推广到 $n$ 个变量的情形。且类似可证对于独立的泊松分布 $X_k\sim P(\lambda_k),k=1,\cdots n$，有 $X=\sum_{k=1}^nX_k\sim P(\sum_{k=1}^n\lambda_k)$。&lt;/p&gt;
&lt;p&gt;【例题】设 $X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)$ 且 $X,Y$ 相互独立，求 $Z=X+Y$ 的分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：$p_X(x)=\frac{1}{\sqrt {2\pi}\sigma_1}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}},p_Y(y)=\frac{1}{\sqrt {2\pi}\sigma_2}e^{-\frac{(y-\mu_2)^2}{2\sigma_2^2}}$。&lt;/p&gt;
&lt;p&gt;正态分布变量的取值均为 $\mathbb R$ ，因此我们直接使用卷积公式：
$$
\begin{align}
p_Z(z)=\int_{-\infty}^{+\infty}p_X(x)p_Y(z-x)dx=\frac{1}{2\pi \sigma_1 \sigma_2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2+\left(\frac{z-x-\mu_2}{\sigma_2}\right)^2\right]}dx
\end{align}
$$
遇到这种 e 上带指数，积分区域是 $\mathbb R$ 的积分，常见的处理手法是将其凑成平方的形式，然后套用高斯积分。因此我们现在希望中括号中两个平方式的交叉项消掉。这里给出一个处理技巧：令 $x=y+a$，$a$ 为待定的系数，则
$$
\begin{align}
p_Z(z)=\frac{1}{2\pi \sigma_1 \sigma_2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left[\left(\frac{y+a-\mu_1}{\sigma_1}\right)^2+\left(\frac{y+a+\mu_2-z}{\sigma_2}\right)^2\right]}dy
\end{align}
$$
令交叉项为零，即
$$
\frac{y}{\sigma_1}\cdot \frac{a-\mu_1}{\sigma_1}+\frac{y}{\sigma_2}\cdot \frac{a+\mu_2-z}{\sigma_2}=0
$$
解得
$$
a=\frac{\mu_1\sigma_2^2-\mu_2\sigma_1^2+z\sigma_1^2}{\sigma_1^2+\sigma_2^2}
$$
带回原式，有
$$
\begin{align}
p_Z(z)&amp;amp;=\frac{1}{2\pi \sigma_1 \sigma_2}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\left[\left(\frac{y}{\sigma_1}\right)^2+\left(\frac{y}{\sigma_2}\right)^2+\frac{(z-\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}\right]}dy\\
&amp;amp;=\frac{1}{2\pi \sigma_1 \sigma_2}e^{-\frac{(z-\mu_1-\mu_2)^2}{2\left(\sigma_1^2+\sigma_2^2\right)}}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}\cdot \frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}y^2}dy\\
&amp;amp;\overset{v=\sqrt{\frac{\sigma_1^2+\sigma_2^2}{\sigma_1^2\sigma_2^2}}y}{=}\frac{1}{2\pi \sigma_1 \sigma_2}e^{-\frac{(z-\mu_1-\mu_2)^2}{2\left(\sigma_1^2+\sigma_2^2\right)}}\int_{-\infty}^{+\infty}e^{-\frac{v^2}{2}}d\left(\sqrt{\frac{\sigma_1\sigma_2}{\sigma_1^2+\sigma_2^2}}y\right)\\
&amp;amp;=\frac{1}{\sqrt{2\pi}\cdot \sqrt{\sigma_1^2+\sigma_2^2}}e^{-\frac{(z-\mu_1-\mu_2)^2}{2\left(\sigma_1^2+\sigma_2^2\right)}}
\end{align}
$$
因此 $Z\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例题】 (习题 3.25) 设 $X\sim U[0,2],Y\sim U[0,1]$ 且 $X,Y$ 独立，求 $Z=X+Y$ 的密度函数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：该题随机变量取值不是 $\mathbb R$，因此不能套用卷积公式，要从定义出法求解。&lt;/p&gt;
&lt;p&gt;$p_X(x)=\begin{cases}\frac{1}{2}&amp;amp;,0\leq x\leq 2\\0&amp;amp;,otherwise\end{cases}$，$p_Y(y)=\begin{cases}1&amp;amp;,0\leq y\leq 1\\0&amp;amp;,otherwise\end{cases}$。$(X,Y)$ 的有效区域是一个长方形。
$$
F_Z(z)=P(X+Y\leq z)=\iint_{{(x,y):x+y\leq z}}p(x,y)dxdy
$$
在有效区域内，有 $p(x,y)=p_X(x)p_Y(y)=\frac{1}{2}$。考虑拿直线 $x+y\leq z$ 滑过平面，看直线左侧。&lt;/p&gt;
&lt;p&gt;容易看出 $z\leq 0$ 时 $F_Z(z)=0$；$z\geq 3$ 时 $F_Z(z)=1$。剩下的几种情形需要仔细考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$0&amp;lt;z&amp;lt;1$，此时获得的是一个三角形，$F_Z(z)=\frac{1}{2}\cdot \frac{z^2}{2}=\frac{z^2}{4},p_Z(z)=F&amp;rsquo;_Z(z)=\frac{z}{2}$。&lt;/li&gt;
&lt;li&gt;$1\leq z&amp;lt;2$，此时获得的是一个梯形，$F_Z(z)=\frac{1}{2}\cdot \frac{z-1+z}{2}=\frac{z}{2}-\frac{1}{4}，p_Z(z)=\frac{1}{2}$。&lt;/li&gt;
&lt;li&gt;$2\leq z&amp;lt;3$，此时获得的是矩形减去一个三角形，$F_Z(z)=\frac{1}{2}\cdot (2-\frac{1}{2}(3-z)^2)=1-\frac{1}{4}(3-z)^2$，$p_Z(z)=\frac{1}{2}(3-z)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title> Lecture 12: More Examples of 2-Dimensional Random Variable Function</title>
      <link>/posts/coursenotes/nju-probability/lec12/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec12/</guid>
      <description>&lt;p&gt;【例】 (差的分布) 设 $(X,Y)$ 的密度为 $p(x,y)$，考虑 $Z=X-Y$ 的分布：
$$
\begin{align}
F_Z(z)&amp;amp;=P(X-Y\leq z)=\iint_{{(x,y):x-y\leq z}}p(x,y)dxdy\\
&amp;amp;=\int_{-\infty}^{+\infty}\left(\int_{x-z}^{+\infty}p(x,y)dy\right)dx\\
&amp;amp;\overset{y=x-v}{=}\int_{-\infty}^{+\infty}\int_z^{-\infty}p(x,x-v)d(-v)dx\\
&amp;amp;=\int_{-\infty}^z\left(\int_{-\infty}^{+\infty}p(x,x-v)dx\right)dv
\end{align}
$$
因此 $Z$ 的密度函数为
$$
p_Z(v)=\int_{-\infty}^{+\infty}p(x,x-v)dx
$$
如果先积 x，可以类似地得到另一种表达式：$p_Z(u)=\int_{-\infty}^{+\infty}p(y+u,y)dy$。&lt;/p&gt;
&lt;p&gt;【例题】 (习题 3.27) 设 $(X,Y)$ 的密度为 $p(x,y)=\begin{cases}3x&amp;amp;,0&amp;lt;y&amp;lt;x&amp;lt;1\\0&amp;amp;,otherwise\end{cases}$，求 $Z=X-Y$ 的密度。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：画图容易得出 $p(x,y)$ 的有效区域是一个三角形。
$$
F_Z(z)=P(X-Y\leq z)=\iint_{{(x,y):x-y\leq z}}p(x,y)dxdy
$$
用直线 $x=y+z$ 滑过平面，关注直线的左侧。容易看出 $z&amp;lt;0$ 时 $F_Z(z)=0$，$z\geq 1$ 时 $F_Z(z)=1$。这两中情况下 $p_Z(z)=0$。$0\leq z&amp;lt;1$ 时，区域不太规则，要分成两个部分：
$$
\begin{align}
F_Z(z)&amp;amp;=\int_0^z\int_0^x3xdydx+\int_z^1\int_{x-z}^x3xdydx\\
&amp;amp;=\int_0^z3x^2dx+\int_z^13zxdx\\
&amp;amp;=-\frac{1}{2}z^3+\frac{3}{2}z
\end{align}
$$
从而 $p_Z(z)=F_Z&amp;rsquo;(z)=-\frac{3}{2}z^2+\frac{3}{2}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【例】 (积和商的分布) 设 $(X,Y)$ 的密度为 $p(x,y)$，考虑两者积和商的分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Z=XY$&lt;/li&gt;
&lt;li&gt;$Z=\frac{X}{Y}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于积的分布，
$$
F_Z(z)=P(XY\leq z)=\iint_{{(x,y):xy\leq z}}p(x,y)dxdy
$$
我们要小心 $x$ 的符号对积分上下限的影响，因此要分类讨论 $x&amp;lt;0$ 和 $x&amp;gt;0$ 的情况：
$$
\begin{align}
F_Z(z)&amp;amp;=\int_{-\infty}^0\left(\int_{z/x}^{+\infty} p(x,y)dy\right)dx+\int_0^{+\infty}\left(\int_{-\infty}^{z/x}p(x,y)dy\right)dx\\
&amp;amp;\overset{y=\frac{v}{x}}{=}\int_{-\infty}^0\left(\int_z^{-\infty}p(x,\frac{v}{x})d\frac{v}{x}\right)
dx+\int_0^{+\infty}\left(\int_{-\infty}^zp(x,\frac{v}{x})d\frac{v}{x}\right)dx\\
&amp;amp;=\int_{-\infty}^z\int_{-\infty}^0-p(x,\frac{v}{x})\frac{1}{x}dxdv+\int_{-\infty}^z\int_0^{+\infty}p(x,\frac{v}{x})\frac{1}{x}dxdv\\
&amp;amp;=\int_{-\infty}^z\left(\int_{-\infty}^{+\infty}p(x,\frac{v}{x})\frac{1}{|x|}dx\right)dv
\end{align}
$$
因此密度函数
$$
p_Z(z)=\int_{-\infty}^{+\infty}p(x,\frac{v}{x})\frac{1}{|x|}dx
$$
当然，如果先积 $y$ 再积 $x$，我们可以得到对称的形式：$p_Z(u)=\int_{-\infty}^{+\infty}p(\frac{u}{y},y)\frac{1}{|y|}dy$。&lt;/p&gt;
&lt;p&gt;对于商的分布，
$$
F_Z(z)=P(X/Y\leq z)=\iint_{{(x,y):\frac{x}{y}\leq z}}p(x,y)dxdy
$$
类似地，考虑 $y&amp;lt;0$ 和 $y&amp;gt;0$，
$$
\begin{align}
F_Z(z)&amp;amp;=\int_{-\infty}^0\left(\int_{yz}^{+\infty}p(x,y)dx\right)dy+\int_0^{+\infty}\left(\int_{-\infty}^{yz}p(x,y)dx\right)dy\\
&amp;amp;\overset{x=yu}{=}\int_{-\infty}^0\left(\int_z^{-\infty}p(yu,y)d(yu)\right)dy+\int_0^{+\infty}\left(\int_{-\infty}^zp(yu,y)d(yu)\right)dy\\
&amp;amp;=\int_{-\infty}^z\left(\int_{-\infty}^0p(yu,y)(-y)dy\right)du+\int_{-\infty}^z\left(\int_0^{+\infty}p(yu,y)ydy\right)du\\
&amp;amp;=\int_{-\infty}^z\left(\int_{-\infty}^{+\infty}p(yu,y)|y|dy\right)du
\end{align}
$$
因此密度函数
$$
p_Z(u)=\int_{-\infty}^{+\infty}p(yu,y)|y|dy
$$
【例题】 (习题 3.28) $(X,Y)$ 的密度为 $p(x,y)=\begin{cases}\frac{1}{2}&amp;amp;,0\leq x\leq 2,0\leq y\leq 1\\0&amp;amp;,otherwise\end{cases}$，求 $Z=XY$ 的密度。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：用反比例函数曲线 $z=xy$ 滑过平面，考虑曲线下方的部分。显然当 $z&amp;lt;0$ 时 $F_Z(z)=0$，$z\geq 2$ 时 $F_Z(z)=1$。下面考虑 $0\leq z&amp;lt;2$ 的部分：
$$
F_Z(z)=\int_0^z\int_0^1\frac{1}{2}dydx+\int_z^2\int_0^{\frac{z}{x}}\frac{1}{2}dydx=\frac{z}{2}(\ln 2+1-\ln z)
$$
从而 $p_Z(z)=F_Z&amp;rsquo;(z)=\frac{1}{2}(\ln 2-\ln z)$。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 13: Mathematical Expectation</title>
      <link>/posts/coursenotes/nju-probability/lec13/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec13/</guid>
      <description>&lt;h2 id=&#34;expectation-of-discrete-random-variables&#34;&gt;Expectation of Discrete Random Variables&lt;/h2&gt;
&lt;p&gt;离散型随机变量的数学期望类似于“加权平均数”，但我们要将频率换成严格的概率。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 13.1}$ 设离散型随机变量 $X$ 的分布律为 $P(X=x_i)=p_i,i=1,2,\cdots$。若级数 $\sum_{i=1}^\infty |x_i|p_i$ 收敛，则称
$$
E[x]\triangleq \sum_{i=1}^\infty x_ip_i
$$
为 $X$ 的数学期望，简称均值或期望。&lt;/p&gt;
&lt;p&gt;注：(1) 分布唯一决定了期望。&lt;/p&gt;
&lt;p&gt;(2) 在定义中，我们要求级数 $\sum_{i=1}^\infty x_ip_i$ 绝对收敛，这是为了保证重排 $x_ip_i$ 的顺序不影响期望的值 (采样顺序不应影响均值)。&lt;/p&gt;
&lt;p&gt;(3) 当 $\sum_{i=1}^\infty |x_i|p_i$ 不收敛时，称 $X$ 的期望不存在。&lt;/p&gt;
&lt;p&gt;【例】考虑泊松分布 $P(\lambda)$ 的期望。泊松分布的分布律为
$$
P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}\qquad k=0,1,\cdots
$$
因此
$$
\begin{align}
E[x]&amp;amp;=\sum_{k=1}^\infty kP(X=k)=\sum_{k=1}^\infty k\frac{\lambda^k}{k!}e^{-\lambda}=e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^k}{(k-1)!}\\
&amp;amp;=e^{-\lambda}\lambda\cdot \sum_{n=0}^{\infty} \frac{\lambda^n}{n!}=e^{-\lambda}\cdot \lambda\cdot e^{\lambda}\\
&amp;amp;=\lambda
\end{align}
$$
【例】考虑二项分布 $X\sim B(n,p)$ 的期望。二项分布的分布律为
$$
P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}\qquad k=0,1,\cdots, n
$$
因此
$$
\begin{align}
E[x]&amp;amp;=\sum_{k=1}^\infty kP(X=k)=\sum_{k=1}^n k\binom{n}{k}p^k(1-p)^{n-k}\\
&amp;amp;=\sum_{k=1}^n \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k}\\
&amp;amp;=np\sum_{k=1}^n\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{n-k}\\
&amp;amp;=np\sum_{k&amp;rsquo;=0}^{n-1}\binom{n-1}{k&amp;rsquo;}p^{k&amp;rsquo;}(1-p)^{n-1-k&amp;rsquo;}\\
&amp;amp;=np\cdot [p+(1-p)]^{n-1}\\
&amp;amp;=np
\end{align}
$$
【例】考虑几何分布 $X\sim g(p)$ 的期望。几何分布的分布律为
$$
P(X=k)=(1-p)^{k-1}p\qquad k=1,2,\cdots
$$
因此
$$
\begin{align}
E[x]&amp;amp;=\sum_{k=1}^\infty kP(X=k)=\sum_{k=1}^\infty k(1-p)^{k-1}p=p\sum_{k=1}^\infty k(1-p)^{k-1}
\end{align}
$$
令
$$
F(p)=\sum_{k=1}^\infty (1-p)^k=\frac{1-p}{p}
$$
(注：$1-p\in [0,1]$ 在收敛半径内，因此等式成立，后续的求导和求和可以交换。)&lt;/p&gt;
&lt;p&gt;则
$$
\frac{d}{dp}F(p)=-\sum_{k=1}^\infty k(1-p)^{k-1}
$$
所以
$$
E[x]=-pF&amp;rsquo;(p)=-p\cdot \left(\frac{1-p}{p}\right)&amp;rsquo;=-p\cdot -\frac{1}{p^2}=\frac{1}{p}
$$
【例】 (圣彼得堡悖论) 连续扔一枚均匀硬币，当出现反面时停止。若此前出现的正面次数为 $k$ 次，则收益为 $2^k$，求该游戏收益的期望。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：设收益为 $X$，则 $P(X=2^k)=\frac{1}{2^{k+1}},k=0,1,\cdots$
$$
E[x]=\sum_{k=0}^\infty x_kP(X=x_k)=\sum_{k=0}^\infty 2^k\frac{1}{2^{k+1}}=\sum_{k=0}^\infty \frac{1}{2}=+\infty
$$
因此期望不存在。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是一个期望不存在的例子。有其他角度可以研究如何定价可以使该游戏公平。一个合理的定价是支付 $n\log_2 n$ 玩 $n$ 次。&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-continuous-random-variables&#34;&gt;Expectation of Continuous Random Variables&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 13.2}$ 设连续型随机变量 $X$ 的密度为 $p(x)$，若积分 $\int_{-\infty}^{+\infty}|x|p(x)dx&amp;lt;+\infty$，则称
$$
E[x]\triangleq \int_{-\infty}^{+\infty}xp(x)dx
$$
为 $X$ 的期望。&lt;/p&gt;
&lt;p&gt;注：若对 $\mathbb R$ 取很密的划分
$$
\begin{align}
&amp;amp;0=x_0&amp;lt;x_1&amp;lt;\cdots &amp;lt;x_n&amp;lt;\cdots&amp;lt;+\infty\\
&amp;amp;0=y_0&amp;gt;y_1&amp;gt;\cdots &amp;gt;y_n&amp;gt;\cdots&amp;gt;-\infty
\end{align}
$$
把 $X$ 近似看成”随机变量“ $\tilde{X}$，满足
$$
\begin{align}
\tilde P(\tilde X=x_k)&amp;amp;=p(x_k)(x_{k+1}-x_k),k\geq 0\\
\tilde P(\tilde X=y_k)&amp;amp;=p(y_k)(y_{k-1}-y_k),k\geq 1
\end{align}
$$
则
$$
\tilde E[\tilde X]=\sum_{k=0}^\infty x_kp(x_k)(x_{k+1}-x_k)+\sum_{k=1}^\infty y_kp(y_k)(y_{k-1}-y_k)
$$
为 $E[x]$ 的渐进和式。&lt;/p&gt;
&lt;p&gt;【例】考虑指数分布 $X\sim E(\lambda)$ 的期望。指数分布的密度为
$$
p(x)=\begin{cases}\lambda e^{-\lambda x}&amp;amp;,x\geq 0\\0&amp;amp;,x&amp;lt;0\end{cases}
$$
因此&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
E[x]&amp;amp;=\int_{-\infty}^{+\infty}xp(x)dx=\int_0^{+\infty}x\cdot \lambda e^{-\lambda x}dx=-\int_0^{+\infty}xe^{-\lambda x}d(-\lambda x)=-\int_{0}^{+\infty}xd(e^{-\lambda x})\\
&amp;amp;=\left .-xe^{-\lambda x}\right|_{0}^{\infty}+\int_0^{+\infty}e^{-\lambda x}dx=-\frac{1}{\lambda}\int_0^{+\infty}e^{-\lambda x}d(-\lambda x)=\left.-\frac{e^{-\lambda x}}{\lambda}\right|_0^{+\infty}\\
&amp;amp;=\frac{1}{\lambda}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;【例】考虑正态分布 $X\sim N(\mu,\sigma^2)$ 的期望。正态分布的密度为
$$
p(x)-\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
因此
$$
\begin{align}
E[x]&amp;amp;=\int_{-\infty}^{+\infty}xp(x)dx=\int_{-\infty}^{+\infty}x\cdot \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
&amp;amp;\overset{y=x-\mu}{=}\int_{-\infty}^{+\infty}(y+\mu)\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{y^2}{2\sigma^2}}dy\\
&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}ye^{-\frac{y^2}{2\sigma^2}}dy+\mu\cdot \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2\sigma^2}}dy
\end{align}
$$
第一项是奇函数在对称区域上的积分，为 0。第二项除了 $\mu$ 后面的部分正好是正态分布全区域上的 $p(x$) 的积分，为 1，因此 $E[x]=\mu$。&lt;/p&gt;
&lt;p&gt;【例】 连续型随机变量的期望也可能不存在。考虑柯西分布 $X$，$p(x)=\frac{1}{\pi(1+x^2)}$，求其期望。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：
$$
\begin{align}
\int_{-\infty}^{+\infty}|x|p(x)dx&amp;amp;=\int_{-\infty}^{+\infty}\frac{|x|dx}{\pi(1+x^2)}=2\int_0^{+\infty}\frac{xdx}{\pi(1+x^2)}\\
&amp;amp;=\frac{1}{\pi}\int_0^{+\infty}\frac{d(1+x^2)}{1+x^2}=\left.\frac{1}{\pi}\ln (1+x^2)\right|_{0}^{+\infty}=+\infty
\end{align}
$$
因此期望不存在。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：微妙的一点是：柯西分布的密度函数是关于 $y$ 轴对称的，但这并不能说明期望存在且等于 0。&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-random-variable-functions&#34;&gt;Expectation of Random Variable Functions&lt;/h2&gt;
&lt;p&gt;设 $X$ 的分布已知，$Y=g(X)$，可通过下面的定理求 $E[Y]$ 而不必先求出 $Y$ 的分布再求期望：&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 13.3}$ 若 $X$ 为离散随机变量，分布律为 $P(X=x_k)=p_k,k=1,2,\cdots$，若 $\sum_{k=1}^\infty |g(x_k)|p_k&amp;lt;+\infty$，则
$$
E[Y]=E[g(x)]=\sum_{k=1}^{\infty} g(x_k)p_k
$$
若 $X$ 为连续型随机变量，密度为 $p(x)$，且期望存在，则
$$
E[Y]=E[g(X)]=\int_{-\infty}^{+\infty}g(x)p(x)dx
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里仅讨论离散情形的证明。设 $Y=g(X)$ 的取值为 $y_1,y_2,\cdots$，$Y$ 显然也是离散型随机变量。记 $A_n={X的取值}\cap {x|g(x)=y_n}={x_n^1,x_n^2,\cdots}$，则 $A$ 构成了全空间 $\Omega$ 的一个划分。
$$
\begin{align}
E[Y]&amp;amp;=\sum_{n=1}^\infty y_nP(X\in A_n)=\sum_{n=1}^\infty y_n\sum_{m=1}^\infty P(X=x_n^m)\\
&amp;amp;=\sum_{n=1}^\infty \sum_{m=1}^\infty y_nP(X=x_n^m)=\sum_{n=1}^\infty\sum_{m=1}^\infty g(x_n^m)P(X=x_n^m)\\
&amp;amp;=\sum_{n=1}^\infty \sum_{x\in A_n}g(x)P(X=x)\\
&amp;amp;=\sum_{k=1}^\infty g(x_k)P(X=x_k)\qquad \blacksquare
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于二维随机变量函数我们也有类似的结论：&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 13.4}$ 设 $(X,Y)$ 为二维随机变量，$Z=g(X,Y)$。&lt;/p&gt;
&lt;p&gt;(1) 离散情形：$(X,Y)$ 有分布律 $P(X=x_i,Y=y_j)=p_{i,j}$。若期望存在，那么
$$
E[Z]=E[g(X,Y)]=\sum_{i=1}^\infty\sum_{j=1}^\infty g(x_i,y_j)p_{i,j}
$$
(2) 连续情形：$(X,Y)$ 的密度函数为 $p(x,y)$，若期望存在，则
$$
E[Z]=E[g(X,Y)]=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x,y)p(x,y)dxdy
$$
【例题】 (习题 4.5) 商品出口需求 $X\sim U[2000,4000]$，售出 1 单位商品可以获得 3 万元收益，不能售出要倒贴  1 万元。问应出口多少吨才能得到最大收益。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：设出口量为 $y$，显然有 $2000\leq y\leq 4000$。令收益为 $Z$：
$$
Z=g(X)=\begin{cases}3y&amp;amp;,X&amp;gt;y\\3X-(y-X)&amp;amp;,X\leq y\end{cases}
$$
考虑 $Z$ 的期望：
$$
\begin{align}
E[Z]&amp;amp;=E[g(X)]=\int_{-\infty}^{+\infty}g(x)p(x)dx=\int_{2000}^{4000}g(x)\frac{1}{2000}dx\\
&amp;amp;=\frac{1}{2000}\left(\int_{2000}^y(4x-y)dx+\int_y^{4000}3ydx\right)\\
&amp;amp;=\frac{1}{1000}(-y^2+7000y-4\cdot 10^6)
\end{align}
$$
当 $y=3500$ 时期望收益最大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;properties-of-expectation&#34;&gt;Properties of Expectation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若随机变量 $X\equiv a$，则 $E[x]=a$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于任意常数 $a,b$，有 $E[aX+bY]=aE[X]+bE[Y]$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里的证明考虑 $X,Y$ 是离散型随机变量的情况：&lt;/p&gt;
&lt;p&gt;设 $X,Y$ 的分布律为 $P(X=x_i,Y=y_j)=p_{ij}$，那么
$$
\begin{align}
E[aX+bY]&amp;amp;=\sum_{i=1}^\infty\sum_{j=1}^\infty(ax_i+by_j)p_{ij}\\
&amp;amp;=a\sum_{i=1}^\infty x_i\sum_{j=1}^\infty p_{ij}+b\sum_{j=1}^\infty y_j\sum_{i=1}^\infty p_{ij}\\
&amp;amp;=a\sum_{i=1}^\infty x_ip_{i\cdot}+b\sum_{j=1}^\infty y_jp_{\cdot j}\\
&amp;amp;=aE[X]+bE[Y]
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $X,Y$ 独立，则 $E[XY]=E[X]E[Y]$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里的证明考虑 $X,Y$ 是离散型随机变量的情况：&lt;/p&gt;
&lt;p&gt;设 $X,Y$ 的分布律为 $P(X=x_i,Y=y_j)=p_{ij}$，因为 $X,Y$ 相互独立，所以 $p_{ij}=p_{i\cdot}\cdot p_{\cdot j}$。那么
$$
\begin{align}
E(XY)&amp;amp;=\sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp_{ij}\\
&amp;amp;=\sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp_{i\cdot }p_{\cdot j}\\
&amp;amp;=\left(\sum_{i=1}^\infty x_ip_{i\cdot }\right)\left(\sum_{j=1}^\infty y_jp_{\cdot j}\right)\\
&amp;amp;=E[X]E[Y]
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用期望的线性性，我们可以很方便地解决一些问题，比如考虑 $X\sim B(n,p)$ 的期望，令 $x_i$ 表示第 $i$ 次实验是否成功的示性随机变量 i.e. $x_i=\begin{cases}1&amp;amp;,第i次实验成功\\0&amp;amp;,otherwise\end{cases}$，容易发现 $E(x_i)=P(x_i=1)=p$。那么
$$
E[X]=E\left[\sum_{i=1}^nx_i\right]=\sum_{i=1}^nE[x_i]=\sum_{i=1}^nP(x_i=1)=np.
$$
再举一例，设 $X$ 服从超几何分布，考虑 $X$ 的期望。类似地，我们令示性随机变量 $x_i$ 刻画第 $i$ 个次品被抽中的情况，显然有 $P(x_i=1)=\frac{\binom{N-1}{n-1}}{\binom{N}{n}}=\frac{n}{N}$ (分子的意义是：保证该次品被抽中，剩下的 $n-1$ 个物品随便抽)，那么
$$
E[x]=E\left[\sum_{i=1}^Mx_i\right]=\sum_{i=1}^ME[x_i]=\sum_{i=1}^M\frac{n}{N}=\frac{nM}{N}.
$$
【例题】 (习题 4.8) $n$ 个人无放回拿 $n$ 个帽子，求拿到自己的帽子的人数的期望。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：令 $x_i$ 是刻画第 $i$ 个人是否拿到帽子的示性随机变量，那么
$$
E[X]=E\left[\sum_{i=1}^nx_i\right]=\sum_{i=1}^nE[x_i]=\sum_{i=1}^n\frac{(n-1)!}{n!}=1.
$$&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 14: Variance</title>
      <link>/posts/coursenotes/nju-probability/lec14/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec14/</guid>
      <description>&lt;p&gt;我们希望衡量一个随机变量每个取值和平均值的差异，并且希望这个值是正数。因此我们引入方差的概念。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 14.1}$ 设 $X$ 是一个随机变量。若 $E[|X|^2]$ 存在，则称
$$
D(X)\triangleq E[(X-E[x])^2]
$$
为 $X$ 的方差，同时称 $\sigma(X)\triangleq \sqrt{D(X)}$ 为 $X$ 的均方差/标准差。&lt;/p&gt;
&lt;p&gt;注：(1) $(X-E[X])^2$ 的数学性质好于 $|X-E[X]|$，比如可导性，以及代数运算时的方便性。&lt;/p&gt;
&lt;p&gt;(2) $E[X^2]&amp;lt;+\infty$ 可以推出 $E[X]&amp;lt;+\infty$ (反过来不成立)。&lt;/p&gt;
&lt;p&gt;(3) 对于离散型随机变量，设 $P(X=x_i)=p_i$，则
$$
D(X)=\sum_{k=1}^\infty(x_k-E[X])^2p_k
$$
对于连续型随机变量，设密度函数为 $p(x)$，则
$$
D(X)=\int_{-\infty}^{+\infty}(x-E[X])^2p(x)dx
$$
(4) 方差公式还有另外一种形式：
$$
\begin{align}
D(X)&amp;amp;=E[(X-E[X])^2]\\
&amp;amp;=E[X^2-2E[X]X+E[X]^2]\\
&amp;amp;=E[X^2]-E[2E[X]X]+E[E[X]^2]\\
&amp;amp;=E[X^2]-2E[X]^2+E[X]^2\\
&amp;amp;=E[X^2]-E[X]^2
\end{align}
$$
从这个公式中我们可以看出：$D(X)\leq E[X^2]$。&lt;/p&gt;
&lt;p&gt;【例】考虑 $X\sim B(n,p)$ 的方差。$X$ 的分布律为 $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$。我们已知 $E[X]=np$，因此只需要求 $E[X^2]$。
$$
\begin{align}
E[X^2]&amp;amp;=\sum_{k=1}^nk^2\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\
&amp;amp;=\sum_{k=1}^nk\frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k}\\
&amp;amp;=\sum_{k=1}^n(k-1)\frac{n!}{(k-1)!(n-k!)}p^k(1-p)^{n-k}+\sum_{k=1}^n\frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k}\\
&amp;amp;=n(n-1)p^2\sum_{k=2}^n\frac{(n-2)!}{(k-2)!(n-k)!}p^{k-2}(1-p)^{n-k}+np\sum_{k=1}^n\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{n-k}\\
&amp;amp;=n(n-1)p^2[1+(1-p)]^{n-2}+np[1+(1-p)]^{n-1}\\
&amp;amp;=n(n-1)p^2+np
\end{align}
$$
因此 $D(X)=E[X^2]-E[X]^2=np-np^2=np(1-p)$。&lt;/p&gt;
&lt;p&gt;【例】考虑 $X\sim p(\lambda)$ 的泊松分布的方差。$X$ 的分布律为 $P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}$，我们已知 $E[X]=\lambda$，因此只需要求 $E[X^2]$。
$$
\begin{align}
E[X^2]&amp;amp;=\sum_{k=1}^\infty k^2\frac{\lambda^k}{k!}e^{-\lambda}=\sum_{k=1}^\infty k\frac{\lambda^k}{(k-1)!}e^{-\lambda}\\
&amp;amp;=\sum_{k=1}^\infty(k-1)\frac{\lambda^k}{(k-1)!}e^{-\lambda}+\sum_{k=1}^\infty\frac{\lambda^k}{(k-1)!}e^{-\lambda}\\
&amp;amp;=\lambda^2\sum_{k=2}^\infty\frac{\lambda^{k-2}}{(k-2)!}e^{-\lambda}+\lambda\sum_{k=1}^\infty\frac{\lambda^{k-1}}{(k-1)!}e^{-\lambda}\\
&amp;amp;=\lambda^2+\lambda
\end{align}
$$
因此 $D(X)=E[X^2]-E[X]^2=\lambda$。&lt;/p&gt;
&lt;p&gt;【例】 考虑指数分布 $X\sim E(\lambda)$ 的方差。$X$ 的密度函数为 $p(x)=\begin{cases}\lambda e^{-\lambda x}&amp;amp;,x&amp;gt;0\\0&amp;amp;,x\leq 0\end{cases}$。我们已知 $E[X]=\frac{1}{\lambda}$，因此只需要求 $E[X^2]$。
$$
\begin{align}
E[X^2]&amp;amp;=\int_{-\infty}^{+\infty}x^2p(x)dx=\int_0^{+\infty}x^2\lambda e^{-\lambda x}dx=-\int_0^{+\infty}x^2e^{-\lambda x}d(-\lambda x)\\
&amp;amp;=-\int_0^{+\infty}x^2d(e^{-\lambda x})=\left.x^2e^{-\lambda x}\right|_0^{+\infty}+2\int_0^{+\infty}xe^{-\lambda x}dx\\
&amp;amp;=\frac{2}{\lambda}\int_0^{+\infty}x\lambda e^{-\lambda x}dx\\
&amp;amp;=\frac{2}{\lambda}E[X]=\frac{2}{\lambda^2}
\end{align}
$$
因此 $D(X)=E[X^2]-E[X]^2=\frac{1}{\lambda^2}$。&lt;/p&gt;
&lt;p&gt;【例】考虑正态分布 $X\sim N(\mu, \sigma^2)$ 的方差。$X$ 的密度函数为 $p(x)=\frac{1}{\sqrt {2\pi }\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$。我们容易发现这里直接方差的原始定义式计算会更加方便：
$$
\begin{align}
D(X)&amp;amp;=E[(X-E[X])^2]=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}(x-\mu)^2e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
&amp;amp;\overset{y=\frac{x-\mu}{\sigma}}{=}\frac{\sigma^2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}y^2e^{-\frac{y^2}{2}}dy\\
&amp;amp;=-\frac{\sigma^2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}yd\left(e^{-\frac{y^2}{2}}\right)\\
&amp;amp;=\left.-\frac{\sigma^2}{\sqrt{2\pi}}ye^{-\frac{y^2}{2}}\right|_{-\infty}^{+\infty}+\sigma^2\cdot \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}dy\\
&amp;amp;=0+\sigma^2\cdot (标准正态分布在 \mathbb R 上的密度函数积分)\\
&amp;amp;=\sigma^2
\end{align}
$$
方差的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若 $X\equiv a$，则 $D(X)=E[(X-a)^2]=0$ 。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $D(X)$ 存在，对于任意 $a,b\in \mathbb R$，$D(aX+b)=a^2D(X)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
D(aX+b)&amp;amp;=E[(aX+b-E[aX+b])^2]\\
&amp;amp;=E[(aX+b-aE[X]-b)^2]\\
&amp;amp;=E[(a(X-E[X]))^2]\\
&amp;amp;=a^2D(X)
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;直观来看这个公式也容易理解：全局加上 $b$ 不影响对期望的偏离；全局乘 $a$ 在 L2 偏差下最终反映为 $a^2$。&lt;/p&gt;
&lt;p&gt;在该性质的支撑下，对于随机变量 $X$，若 $D(X)$ 存在，考虑 $Z=\frac{X-E[X]}{\sqrt{D(X)}}$，则 $E[Z]=0$，$D(Z)=\left(\frac{1}{\sqrt{D(X)}}\right)^2D(X)=1$，因此称 $Z$ 为 $X$ 的标准化。(标准正态分布和正态分布之间的转化是该公式的一个特例)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设 $X,Y$ 为随机变量，则 $D(X\pm Y)=D(X)+D(Y)\pm 2E[(X-E&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; )(Y-E(Y))]$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里只证 + 的情况，- 的情况同理。
$$
\begin{align}
D(X+Y)&amp;amp;=E[(X+Y-E[X+Y])^2]=E[((X-E[X])+(Y-E[Y]))^2]\\
&amp;amp;=E[(X-E[X])^2]+E[(Y-E[Y])^2]+2E[(X-E[X])(Y-E[Y])]\\
&amp;amp;=D(X)+D(Y)+2E[(X-E[X])(Y-E[Y])]
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;特别地，若 $X,Y$ 独立，则 $X-E[X], Y-E[Y]$ 也独立，则
$$
E[(X-E[X])(Y-E[Y])]=E[X-E[X]]E[Y-E[Y]]=0
$$
从而 $D(X\pm Y)=D(X)+D(Y)$ (注意符号！)。&lt;/p&gt;
&lt;p&gt;上述结论可以推广到 $n$ 个随机变量的情形：
$$
D\left(\sum_{i=1}^nx_i\right)=\sum_{i=1}^nD(x_i)-\sum_{1\leq i&amp;lt;j\leq n}E[(x_i-E[x_i])(x_j-E[x_j])]
$$
若这些变量两两独立 (注意，该条件比 $n$ 个变量的独立性弱！)，则
$$
D\left(\sum_{i=1}^nx_i\right)=\sum_{i=1}^nD(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于任意 $c\in \mathbb R$，$D(X)\leq E[(X-c)^2]$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
D(X)&amp;amp;=E[(X-E[X])^2]=E[(X-c+c-E[X])^2]\\
&amp;amp;=E[(X-c)^2]+2E[(X-c)(c-E[X])]+E[(c-E[X])^2]\\
&amp;amp;=E[(X-c)^2]+2(c-E[X])(E[X]-c)+(c-E[X])^2\\
&amp;amp;=E[(X-c)^2]-(c-E[X])^2\\
&amp;amp;\leq E[(X-c)^2]
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用期望的可拆分性 (性质 3) 有时能大幅简化计算过程，这里以二项分布 $X\sim B(n,p)$ 为例。令 $x_i$ 为描述第 $i$ 次实验是否成功的示性随机变量，显然 $x_1,\cdots, x_n$ 两两独立。那么
$$
\begin{align}
D(X)&amp;amp;=D\left(\sum_{i=1}^nx_i\right)=\sum_{i=1}^nD(x_i)=nD(x_1)\\
&amp;amp;=nE[(x_1-E[x_1])^2]=n((1-p)^2p+(0-p)^2(1-p))\\
&amp;amp;=np(1-p)
\end{align}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 15: Covariance and Correlation Coefficient</title>
      <link>/posts/coursenotes/nju-probability/lec15/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec15/</guid>
      <description>&lt;p&gt;$\fbox{Theorem 15.1}$ (Chebyshev) 设随机变量 $X$ 的方差存在，则有
$$
\forall \varepsilon&amp;gt;0,P(|X-E[X]|&amp;gt;\varepsilon)\leq \frac{D(X)}{\varepsilon^2}
$$
证明：设 $X$ 存在密度函数 $p(x)$，则
$$
\begin{align}
P(|X-E[X]|&amp;gt;\varepsilon)&amp;amp;=\int_{{x:|X-E[X]|&amp;gt;\varepsilon}}p(x)dx\\
&amp;amp;\leq \int_{{x:|X-E[X]|&amp;gt;\varepsilon}}\frac{|X-E[x]|^2}{\varepsilon^2}p(x)dx\\
&amp;amp;\leq \frac{1}{\varepsilon^2}\int_{-\infty}^{+\infty}|X-E[X]|^2p(x)dx\\
&amp;amp;=\frac{D(X)}{\varepsilon^2}\qquad \blacksquare
\end{align}
$$
我们的证明过程中使用了两次看上去很“松”的放缩，但下面的例子可以说明，在对 $X$ 没有任何额外限制的情况下，Chebyshev 的结果已经是最紧的了：&lt;/p&gt;
&lt;p&gt;例：设 $X$ 的分布律为 $P(X=1)=P(X=-1)=\frac{1}{2k^2}$，$P(X=0)=1-\frac{1}{k^2}$，显然有 $E[X]=0$。那么
$$
P(|X-E[x]|\geq 1)=P(X=\pm 1)=\frac{1}{k^2}\leq \frac{D(X)}{\varepsilon^2}=\frac{1/k^2}{1^2}=\frac{1}{k^2}
$$
【例题】用 Chebyshev 不等式证明：如果随机变量 $X$ 满足 $D(X)=0$，则 $P(X=E[X])=1$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：令 $A={w:|X(w)-E[X]|&amp;gt;0}$，我们要证明的目标是 $P(A)=0$。&lt;/p&gt;
&lt;p&gt;令 $A_n={w:|X(w)-E[X]|&amp;gt;\frac{1}{n}}$，那么显然有 $A=\bigcup_{n=1}^\infty A_n$，从而
$$
P(A)=P(\bigcup_{n=1}^\infty A_n)\leq \sum_{n=1}^\infty P(A_n)=\sum_{n=1}^\infty P(X-E[X]&amp;gt;\frac{1}{n})\leq \sum_{n=1}^\infty \frac{1}{n^2}D(X)=0
$$
又 $P(A)\geq 0$，所以 $P(A)=0$。&lt;/p&gt;
&lt;p&gt;（注：第一个不等号是 union bound。注意 $A_n$ 之间不是互不相交的，所以不能用“可列可加性”。）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;covariance&#34;&gt;Covariance&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 15.2}$ $X,Y$ 为随机变量，$E[X],E[Y],E[XY]$ 均存在，则
$$
cov(X,Y)\triangleq E[(X-E[X])(Y-E[Y])]
$$
为 $X$ 和 $Y$ 的&lt;strong&gt;协方差&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;注：(1) $cov(X,X)=D(X)$。&lt;/p&gt;
&lt;p&gt;(2) $D(X\pm Y)=D(X)+D(Y)\pm 2cov(X,Y)$。&lt;/p&gt;
&lt;p&gt;一般地，
$$
D(\sum_{k=1}^nX_k)=\sum_{k=1}^nD(X_k)+2\sum_{1\leq i&amp;lt;j\leq n}cov(X_i,X_j).
$$
(3) $cov(X,Y)=E[XY-XE[Y]-YE[X]+E[X]E[Y]]=E[XY]-E[X]E[Y]$。&lt;/p&gt;
&lt;p&gt;(4) $cov(X,Y)&amp;gt;0$ 说明 $X$ 和 $Y$ 有很大倾向同时大于或小于各自的期望；$cov(X,Y)&amp;lt;0$ 说明 $X$ 和 $Y$ 有很大倾向一个大于自身期望，一个小于自身期望。&lt;/p&gt;
&lt;p&gt;(5) $cov(X,Y)$ 依赖于单位。比如如果将两者的单位本来是米，将它们改成毫米，协方差的数值将会变大很多。因此协方差不是一个很好的衡量相关性的客观数值。&lt;/p&gt;
&lt;p&gt;协方差的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$cov(X,Y)=cov(Y,X)$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于任意 $a,b,c,d\in \mathbb R$，$cov(aX+c,bY+d)=abcov(X,Y)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
cov(aX+c,bY+d)&amp;amp;=E[(aX+c)(bY+d)]-E[aX+c]E[bY+d]\\
&amp;amp;=E[abXY+adX+bcY+cd]-(aE[X]+c)(bE[Y]+d)\\
&amp;amp;=abE[XY]-abE[X]E[Y]\\
&amp;amp;=abcov(X,Y)
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$cov(X_1+X_2,Y)=cov(X_1,Y)+cov(X_2,Y)$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
cov(X_1+X_2,Y)&amp;amp;=E[(X_1+X_2)Y]-E[X_1+X_2]E[Y]\\
&amp;amp;=E[X_1Y]+E[X_2Y]-E[X_1]E[Y]-E[X_2]E[Y]\\
&amp;amp;=cov(X_1,Y)+cov(X_2,Y)
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $X,Y$ 独立，则 $cov(X,Y)=0$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：$cov(X,Y)=E[XY]-E[X]E[Y]=E[X]E[Y]-E[X]E[Y]=0$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面展示一个比较精巧的计算协方差的例子：&lt;/p&gt;
&lt;p&gt;【例】设 $(X,Y)$ 服从参数为 $n,p_1,p_2$ 的三项分布，即对于任意 $i,j$，$i+j\leq n$，
$$
P(X=i,Y=j)=\frac{n!}{i!j!(n-i-j)!}p_1^ip_2^j(1-p_1-p_2)^{n-i-j}
$$
考虑计算 $cov(X,Y)$。我们发现利用定义计算非常麻烦，但考虑公式
$$
D(X+Y)=D(X)+D(Y)+2cov(X,Y)
$$
这三个方差都很容易求：因为 $X\sim B(n,p_1),Y\sim B(n,p_2),X+Y\sim B(n,p_1+p_2)$，所以
$$
\begin{align}
cov(X,Y)&amp;amp;=\frac{1}{2}(D(X+Y)-D(X)-D(Y))\\
&amp;amp;=\frac{1}{2}(n(p_1+p_2)(1-p_1-p_2)-np_1(1-p_2)-np_2(1-p_2))\\
&amp;amp;=-np_1p_2
\end{align}
$$
$\fbox{Theorem 15.3}$ (Cauchy-Schwarz) 设随机变量 $X,Y$ 的方差都存在，则
$$
(cov(X,Y))^2\leq D(X)D(Y)
$$
其中等号取到当且仅当存在不全为 0 的常数 $a,b$，使得 $P(Y=aX+b)=1$。&lt;/p&gt;
&lt;p&gt;证明：对于任意 $t\in \mathbb R$，令
$$
u(t)=E[(t(X-E[X])-(Y-E[Y]))^2]\geq 0
$$
那么
$$
\begin{align}
u(t)&amp;amp;=t^2E[(X-E[X])^2]-2tE[(X-E[X])(Y-E[Y])]+E[(Y-E[Y])^2]\\
&amp;amp;=t^2D(X)-2tcov(X,Y)+D(Y)
\end{align}
$$
因为 $u(t)\geq 0$，所以二次函数判别式小于等于 0，即 $(cov(X,Y))^2\leq D(X)D(Y)$。&lt;/p&gt;
&lt;p&gt;下面考虑等号成立的条件：&lt;/p&gt;
&lt;p&gt;$\Rightarrow:$ 若 $(cov(X,Y))^2=D(X)D(Y)$，那么存在 $t_0\in \mathbb R$，$u(t_0)=0$。令 $Z=t_0(X-E[X])-(Y-E[Y])$，则
$$
0=u(t_0)=E[Z^2]\geq D(Z)\geq 0
$$
所以 $D(Z)=0$，$P(Z=E[Z])=P(Z=0)=1$。&lt;/p&gt;
&lt;p&gt;$\Leftarrow:$ 若存在不全为 0 的 $a,b\in \mathbb R$ 满足 $P(Y=aX+b)=1$，那么
$$
\begin{align}
cov(X,Y)&amp;amp;=E[XY]-E[X]E[Y]\\
&amp;amp;=E[X(aX+b)]-E[X]E[aX+b]\\
&amp;amp;=aE[X^2]+bE[X]-a(E[X])^2-bE[X]\\
&amp;amp;=aD(X)=\pm \sqrt{a^2D(X)^2}
\end{align}
$$
注意到 $D(Y)=D(aX+b)=a^2D(X)$，所以 $cov(X,Y)=\pm \sqrt{D(X)D(Y)}$。$\blacksquare$&lt;/p&gt;
&lt;h2 id=&#34;correlation-coefficient&#34;&gt;Correlation Coefficient&lt;/h2&gt;
&lt;p&gt;由 Cauchy-Schwarz 不等式可知，$D(X),D(Y)&amp;gt;0$ 时，$\left|\frac{cov(X,Y)}{\sqrt{D(X)D(Y)}}\right|\leq 1$，我们在此基础上定义相关系数。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 15.4}$ 设随机变量 $X,Y$ 方差均存在，$D(X)&amp;gt;0,D(Y)&amp;gt;0$，则令
$$
\rho_{XY}\triangleq \frac{cov(X,Y)}{\sqrt{D(X)D(Y)}}
$$
为 $X,Y$ 的&lt;strong&gt;相关系数&lt;/strong&gt;，也可记作 $corr(X,Y)$。若 $\rho_{XY}&amp;gt;0$，则称 $X,Y$ 正相关；若 $\rho_{XY}&amp;lt;0$，则称 $X,Y$ 负相关。&lt;/p&gt;
&lt;p&gt;注：(1) 考虑 $X,Y$ 的标准化随机变量 $X^*=\frac{X-E[X]}{\sqrt{D(X)}},Y^*=\frac{Y-E[Y]}{\sqrt{D(Y)}}$，那么
$$
cov(X^*,Y^*)=E[(X^*-E[X^*])(Y^*-E[Y^*])]=E\left[\frac{X-E[X]}{\sqrt{D(X)}}\cdot \frac{Y-E[Y]}{\sqrt{D(Y)}}\right]=\frac{cov(X,Y)}{\sqrt{D(X)D(Y)}}=\rho_{XY}
$$
即标准化后的随机变量的协方差等于其原变量的相关系数。&lt;/p&gt;
&lt;p&gt;(2) $\rho_{XY}$ 不依赖 $X,Y$ 数量级的选取。&lt;/p&gt;
&lt;p&gt;(3) 根据 Cauchy-Schwarz 的取等条件，$|\rho_{XY}=1|$ 当且仅当 $|cov(X,Y)|=\sqrt{D(X)D(Y)}$，即存在不全为 0 的常数 $a,b$，$P(Y=aX+b)=1$。$\rho_{XY}$ 刻画了 $X,Y$ 之间线性相关的程度，$|\rho_{XY}|$ 越大，$X,Y$ 的线性关系就越密切。&lt;/p&gt;
&lt;p&gt;$\fbox{Definition 15.5}$ 若 $X,Y$ 满足 $\rho_{XY}=0$，则称 $X,Y$ &lt;strong&gt;线性无关&lt;/strong&gt;或&lt;strong&gt;不相关&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;$\fbox{Proposition 15.6}$ 在 $X,Y$ 方差存在的情况下，下述 4 条等价：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\rho_{XY}=0$。&lt;/li&gt;
&lt;li&gt;$cov(X,Y)=0$。&lt;/li&gt;
&lt;li&gt;$E[XY]=E[X]E[Y]$。&lt;/li&gt;
&lt;li&gt;$D(X\pm Y)=D(X)+D(Y)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据上述命题，我们容易发现如果 $X,Y$ 相互独立，那么 $\rho_{XY}=0$，但反之不成立，即不相关是比相互独立弱的条件，下面是一个例子：&lt;/p&gt;
&lt;p&gt;【例】设 $\theta$ 服从 $[0,2\pi]$ 上的均匀分布，令 $X=\cos \theta,Y=\cos (\theta+a)$，其中 $a$ 为给定常数，考虑 $\rho_{XY}$。
$$
\begin{align}
E[X]&amp;amp;=\int_0^{2\pi}xp(x)dx=\frac{1}{2\pi}\int_0^{2\pi}\cos xdx=0\\
E[Y]&amp;amp;=\frac{1}{2\pi}\int_0^{2\pi}\cos(x+a)dx=0\\
D(X)&amp;amp;=E[(X-0)^2]=\frac{1}{2\pi}\int_0^{2\pi}\cos ^2xdx=\frac{1}{4\pi}\int_0^{2\pi}(1+\cos 2x)dx=\frac{1}{2}\\
D(Y)&amp;amp;=E[(Y-0)^2]=\frac{1}{2\pi}\int_0^{2\pi}\cos ^2(x+a)dx=\frac{1}{4\pi}\int_0^{2\pi}(1+\cos 2(x+a))dx=\frac{1}{2}\\
E[XY]&amp;amp;=E[\cos \theta \cos (\theta +a)]=\frac{1}{2\pi}\int_0^{2\pi}\cos x\cos (x+a)dx\\
&amp;amp;\overset{和差化积}{=}\frac{1}{4\pi}(\int_0^{2\pi}\cos(2x+a)+\cos a)dx=\frac{1}{2}\cos a\\
\rho_{XY}&amp;amp;=\frac{cov(X,Y)}{\sqrt{D(X)D(Y)}}=\frac{\frac{1}{2}cos a}{\sqrt{\frac{1}{2}\cdot \frac{1}{2}}}=\cos a
\end{align}
$$
当 $a=\frac{3\pi}{2}$ 时，$\rho_{XY}=0$，$X,Y$ 不相关。此时 $X=\cos \theta,Y=\cos (\theta+a)=\sin \theta$，有 $X^2+Y^2=1$。&lt;/p&gt;
&lt;p&gt;考虑如下事件的概率：$P(|X|\leq \frac{1}{2},|Y|\leq \frac{1}{2})$，因为 $X^2+Y^2=1$，所以该概率为 0。但 $P(|X|\leq \frac{1}{2})&amp;gt;0$，$P(|Y|\leq \frac{1}{2})&amp;gt;0$，从而 $P(|X|\leq \frac{1}{2},|Y|\leq \frac{1}{2})\neq P(|X|\leq \frac{1}{2})\cdot P(|Y|\leq \frac{1}{2})$，所以 $X,Y$ 不相互独立。&lt;/p&gt;
&lt;p&gt;【例题】设 $(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，求 $cov(X,Y)$ 和 $\rho_{XY}$。&lt;/p&gt;
&lt;p&gt;解：
$$
p(x,y)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2-2\rho \left(\frac{x-\mu_1}{\sigma_1}\cdot \frac{y-\mu_2}{\sigma_2}\right)+\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right]}
$$
我们已知 $E[X]=\mu_1,E[Y]=\mu_2,D(X)=\sigma_1^2,D(Y)=\sigma_2^2$。
$$
\begin{align}
cov(X,Y)&amp;amp;=E[(X-E[X])(Y-E[Y])]=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}(x-\mu_1)(y-\mu_2)p(x,y)dxdy\\
&amp;amp;\overset{u=\frac{x-\mu_1}{\sigma_1},v=\frac{y-\mu_2}{\sigma_2}}{=}\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\sigma_1u\cdot \sigma_2ve^{-\frac{1}{2(1-\rho^2)}(u^2-2\rho uv+v^2)}d(\sigma_1u)d(\sigma_2v)\\
&amp;amp;=\frac{\sigma_1\sigma_2}{2\pi\sqrt{1-\rho^2}}\int_{-\infty}^{+\infty}v\int_{-\infty}^{+\infty}ue^{-\frac{1}{2(1-\rho^2)}(u^2-2\rho vu+\rho^2v^2-\rho^2v^2+v^2)}dudv\\
&amp;amp;=\frac{\sigma_1\sigma_2}{2\pi\sqrt{1-\rho^2}}\int_{-\infty}^{+\infty}ve^{-\frac{v^2}{2}}\int_{-\infty}^{+\infty}ue^{-\frac{1}{2(1-\rho^2)}(u-\rho v)^2}dudv
\end{align}
$$
注意到
$$
\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}}\int_{-\infty}^{+\infty}ue^{-\frac{1}{2(1-\rho^2)}(u-\rho v)^2}du
$$
是正态分布 $N(\rho v, 1-\rho^2)$ 的期望 (自变量为 $u$)，所以
$$
\begin{align}
cov(X,Y)=\frac{\sigma_1\sigma_2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}ve^{-\frac{v^2}{2}}\rho vdv=\sigma_1\sigma_2\rho\cdot  \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}v^2e^{-\frac{v^2}{2}}dv
\end{align}
$$
注意到
$$
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}v^2e^{-\frac{v^2}{2}}dv=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}(v-0)^2e^{-\frac{v^2}{2}}dv
$$
是标准正态分布 $N(0,1)$ 的方差，所以该式为 1，所以 $cov(X,Y)=\sigma_1\sigma_2\rho$。相关系数 $\rho_{XY}=\rho$。&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 15.7}$ 设 $(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，则 $X,Y$ 相互独立当且仅当 $X,Y$ 不相关。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Lecture 16: Moment and Covariance Matrix</title>
      <link>/posts/coursenotes/nju-probability/lec16/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec16/</guid>
      <description>&lt;h2 id=&#34;origin-moment-and-central-moment&#34;&gt;Origin Moment and Central Moment&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 16.1}$ 设 $X$ 是随机变量，对正整数 $k$，若 $E[|X|^k]&amp;lt;+\infty$，则称 $E[X^k]$ 为 $X$ 的 $k$ 阶原点矩或 $k$ 阶矩；若 $E[|X-E[X]|^k]&amp;lt;+\infty$，则称 $E[(X-E[X])^k]$ 为 $X$ 的 $k$ 阶中心矩。&lt;/p&gt;
&lt;p&gt;注：(1) 矩是对期望、方差的推广，1 阶原点矩就是期望，2 阶中心矩就是方差。&lt;/p&gt;
&lt;p&gt;(2) 一般情形下，两个随机变量的任意阶原点矩/中心矩相同不能推出两个随机变量的分布相同。&lt;/p&gt;
&lt;p&gt;【例】考虑正态分布 $X\sim N(\mu,\sigma^2)$ 的 $k$ 阶中心矩。首先写出 $X$ 的密度函数：
$$
p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
所以
$$
\begin{align}
E[(X-E[X])^k]&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}(x-\mu)^ke^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
&amp;amp;\overset{t=\frac{x-\mu}{\sigma}}{=}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}\sigma^kt^ke^{-\frac{t^2}{2}}d(\sigma t)\\
&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}t^ke^{-\frac{t^2}{2}}dt
\end{align}
$$
$k$ 是奇数时，被积函数是奇函数，原式为 0。$k$ 是偶数时，我们考虑归纳地使用分部积分：
$$
\begin{align}
\int_{-\infty}^{+\infty}t^ke^{-\frac{t^2}{2}}dt&amp;amp;=\int_{-\infty}^{+\infty}t^{k-1}e^{-\frac{t^2}{2}}d\frac{t^2}{2}=-\int_{-\infty}^{+\infty}t^{k-1}de^{-\frac{t^2}{2}}\\
&amp;amp;=-\left.t^{k-1}e^{-\frac{t^2}{2}}\right|_{-\infty}^{+\infty}+\int_{-\infty}^{+\infty}e^{-\frac{t^2}{2}}d(t^{k-1})\\
&amp;amp;=(k-1)\int_{-\infty}^{+\infty}t^{k-2}e^{-\frac{t^2}{2}}dt
\end{align}
$$
最后剩下的东西是高斯积分，为 $\sqrt{2\pi}$，因此
$$
E[(X-E[X])^k]=\begin{cases}0&amp;amp;,k是偶数\\\sigma^k(k-1)!!&amp;amp;,k是奇数\end{cases}.
$$
注：标准化随机变量 (此时期望为 0，中心矩和原点矩相同) 的三阶矩称为偏度。偏度为负意味着分布的左尾更长，但大多数取值比期望大 (即有一些概率取到很小的负数，但更大概率取到大于期望的数)；偏度为 0 意味着数值相对均匀地分布在期望的两侧 (但未必完全对称)。&lt;/p&gt;
&lt;p&gt;四阶矩称为峰度。峰度高意味着存在远大于或远小于期望的取值且取到的概率不太小。&lt;/p&gt;
&lt;p&gt;对于中心矩，我们有如下的切比雪夫不等式的推广：&lt;/p&gt;
&lt;p&gt;$\fbox{Theorem 16.2}$ 设 $E[|X-E[X]|^k]&amp;lt;+\infty$，则对于任意 $\varepsilon&amp;gt;0$，
$$
P(|X-E[X||&amp;gt;\varepsilon)\leq \frac{E[|X-E[X]|^k]}{\varepsilon^k}
$$
该不等式在远端可以给出比切比雪夫不等式更好的估计。&lt;/p&gt;
&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 16.3}$ 设 $X=(X_1,\cdots, X_n)^T$ 是一个 $n$ 维随机变量，则
$$
E[X]\triangleq(E[X_1],E[X_2],\cdots, E[X_n])^T
$$
为 $X$ 的期望。记 $C_{ij}=cov(X_i,X_j)$，称矩阵
$$
\Sigma=\begin{bmatrix}C_{11} &amp;amp; C_{12}&amp;amp;\cdots &amp;amp;C_{1n}\\
C_{21}&amp;amp;C_{22}&amp;amp;\cdots&amp;amp;C_{2n}\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
C_{n1}&amp;amp;C_{n2}&amp;amp;\cdots&amp;amp;C_{nn}\end{bmatrix}
$$
为 $X$ 的协方差阵。&lt;/p&gt;
&lt;p&gt;【例】考虑 $(X_1,X_2)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，记 $\Sigma=\begin{bmatrix}\sigma_1^2&amp;amp; \sigma_1\sigma_2\rho\\\sigma_1\sigma_2\rho&amp;amp;\sigma_2^2\end{bmatrix}$，$\mu=(\mu_1,\mu_2)^T,X=(X_1,X_2)^T$，则
$$
p(x_1,x_2)=p(x)=\frac{1}{2\pi\sqrt{\det \Sigma}}e^{-\frac{1}{2}(X-\mu)^T\Sigma^{-1}(X-\mu)}
$$
$\fbox{Theorem 16.4}$ 协方差矩阵是半正定的，即对于任意 $X\in \mathbb R^n$，$X^T\Sigma X\geq 0$。&lt;/p&gt;
&lt;p&gt;证明：对于任意 $T=(t_1,\cdots, t_n)^T\in \mathbb R^n$，考虑如下随机变量的期望：
$$
E\left[\left(\sum_{i=1}^nt_i(X_i-E[X_i])\right)^2\right]\geq 0
$$
我们下面证明这个期望就是协方差矩阵的二次型：
$$
\begin{align}
E\left[\left(\sum_{i=1}^nt_i(X_i-E[X_i])\right)^2\right]&amp;amp;=E\left[\sum_{i=1}^n\sum_{j=1}^nt_it_j(X_i-E[X_i])(X_j-E[X_j])\right]\\
&amp;amp;=\sum_{i=1}^n\sum_{j=1}^nt_it_jE[(X_i-E[X_i])(X_j-E[X_j])]\\
&amp;amp;=\sum_{i=1}^n\sum_{j=1}^nt_it_jC_{ij}\\
&amp;amp;=\sum_{j=1}^n\left(\sum_{i=1}^nt_iC_{ij}\right)t_j\\
&amp;amp;=
\begin{pmatrix}
\sum_{i=1}^nt_iC_{i1}&amp;amp;\sum_{i=1}^nt_iC_{i2}&amp;amp;\cdots&amp;amp;\sum_{i=1}^nt_iC_{in}
\end{pmatrix}
\begin{pmatrix}
t_1\\t_2\\\vdots\\t_n
\end{pmatrix}\\
&amp;amp;=\begin{pmatrix}
\begin{pmatrix}
t_1&amp;amp;\cdots&amp;amp;t_n
\end{pmatrix}
\begin{pmatrix}
C_{11}\\\vdots \\C_{1n}
\end{pmatrix}
&amp;amp;
\cdots
&amp;amp;
\begin{pmatrix}
t_1&amp;amp;\cdots&amp;amp;t_n
\end{pmatrix}
\begin{pmatrix}
C_{n1}\\\vdots \\C_{nn}
\end{pmatrix}
\end{pmatrix}
\begin{pmatrix}
t_1\\\vdots \\t_n
\end{pmatrix}\\
&amp;amp;=
\begin{pmatrix}
t_1&amp;amp;\cdots &amp;amp;t_n
\end{pmatrix}
\begin{bmatrix}C_{11} &amp;amp; C_{12}&amp;amp;\cdots &amp;amp;C_{1n}\\
C_{21}&amp;amp;C_{22}&amp;amp;\cdots&amp;amp;C_{2n}\\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\
C_{n1}&amp;amp;C_{n2}&amp;amp;\cdots&amp;amp;C_{nn}\end{bmatrix}
\begin{pmatrix}
t_1\\\vdots \\t_n
\end{pmatrix}\\
&amp;amp;=T^T\Sigma T
\end{align}
$$
所以 $\Sigma$ 半正定。 $\blacksquare$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 21: Interval Estimation</title>
      <link>/posts/coursenotes/nju-probability/lec21/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec21/</guid>
      <description>&lt;p&gt;$\fbox{Definition 21.1}$ 设 $\theta$ 是总体 $X$ 的未知参数，$X_1,\cdots, X_n$ 是来自 $X$ 的样本，若对于给定的 $0&amp;lt;\alpha&amp;lt;1$，存在两个统计量 $\hat\theta_1(X_1,\cdots, X_n)$ 和 $\hat\theta_2(X_1,\cdots, X_n)$，使得
$$
P(\hat\theta_1&amp;lt;\theta&amp;lt;\hat\theta_2)=1-\alpha
$$
则称 $(\hat\theta_1,\hat\theta_2)$ 是 $\theta$ 的置信度为 $1-\alpha$ 的置信区间，$\theta_1,\theta_2$ 称为置信下限和置信上限，$1-\alpha$ 称为置信度或置信系数。&lt;/p&gt;
&lt;p&gt;注：(1) 上面数学表达式的含义为：在进行了 $m$ 轮 (每轮 $n$ 次) 采样后，我们会得到 $m$ 个区间 $(\hat\theta_{1,1},\hat\theta_{2,1})$ …… $(\hat\theta_{1,m},\hat\theta_{2,m})$。这其中大约有 $(1-\alpha)m$ 个区间包含了 $\theta$。对于一组具体的估计值 $\hat\theta_1(x_1,\cdots,x_n)$ 和 $\hat\theta_2(x_1,\cdots, x_n)$，我们不能说 $\theta$ 有 $1-\alpha$ 的概率落在区间中，因为这时区间的上下界已经是确定的数了，不再具有随机性。&lt;/p&gt;
&lt;p&gt;(2) $\hat\theta_2-\hat\theta_1$ 越大，置信度越高，但精度也会越差。&lt;/p&gt;
&lt;p&gt;有时我们也会只关心未知参数的上限或下限 (如保质期)，因此我们类似地有单侧置信区间和单侧置信上限/下限的概念。&lt;/p&gt;
&lt;h2 id=&#34;pivot-method&#34;&gt;Pivot Method&lt;/h2&gt;
&lt;p&gt;枢轴变量法的主要思想如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构造一个样本函数 $U(X_1,\cdots, X_n;\theta)$，我们要求 $U$ 是有且仅有未知参数 $\theta$ 的函数，且 $U$ 的分布已知。我们称 $U$ 为枢轴变量。&lt;/li&gt;
&lt;li&gt;因为 $U$ 的分布已知，所以对于给定的置信区间 $1-\alpha$，我们可以确定区间 $[a,b]$，使得 $P(a\leq U\leq b)=1-\alpha$ (如果要求单边置信区间，则只需要一个方向的约束)。&lt;/li&gt;
&lt;li&gt;根据 $a\leq U\leq b$ 反推出 $\theta$ 的范围 (范围表达式中仅包含 $X_1,\cdots, X_n$)，这个范围就是我们要求的 $\hat\theta_1$ 和 $\hat\theta_2$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;confidence-interval-of-mu-in-normal-population&#34;&gt;Confidence Interval of $\mu$ in Normal Population&lt;/h2&gt;
&lt;p&gt;【例】给定置信度 $1-\alpha$，样本 $X_1,\cdots, X_n$ 来自总体 $X\sim N(\mu,\sigma^2)$。考虑用枢轴变量法求均值 $\mu$ 的置信区间：因为 $X\sim N(\mu,\sigma^2)$，所以 $\overline{X}\sim N(\mu,\frac{\sigma^2}{n})$ 我们构造 $U=\frac{\overline{X}-\mu}{\sigma/\sqrt n}$，则 $U\sim N(0,1)$。&lt;/p&gt;
&lt;p&gt;记 $u_{\alpha}$ 是 $N(0,1)$ 的上 $\alpha$ 分为数 (即 $P(u&amp;gt;u_\alpha)=\alpha$)，那么
$$
1-\alpha=P(u_{1-\alpha/2}&amp;lt;U&amp;lt;u_{\alpha/2})=P(-u_{\alpha/2}&amp;lt;U&amp;lt;u_{\alpha/2})
$$
因此
$$
-u_{\frac{\alpha}{2}}&amp;lt;\frac{\overline{X}-\mu}{\sigma/\sqrt n}&amp;lt;u_{\frac{\alpha}{2}}\quad \Rightarrow\quad \overline{X}-\frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt n}&amp;lt;\mu&amp;lt;\overline{X}+\frac{\sigma \mu_{\frac{\alpha}{2}}}{\sqrt n}
$$
因此置信区间为 $\left(\overline{X}-\frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt n},\overline{X}+\frac{\sigma \mu_{\frac{\alpha}{2}}}{\sqrt n}\right)$。&lt;/p&gt;
&lt;p&gt;如果我们不知道 $\sigma^2$，我们也可以给出一个置信区间。根据正态总体章节的知识，我们有
$$
T=\frac{\sqrt n(\overline{X}-\mu)}{S}\sim t(n-1)
$$
因此记 $t_{\alpha}(n-1)$ 为 $T$ 的上 $\alpha$ 分为数，有
$$
1-\alpha=P(t_{1-\alpha/2}(n-1)&amp;lt;T&amp;lt;t_{\alpha/2}(n-1))=P(-t_{\alpha/2}(n-1)&amp;lt;T&amp;lt;t_{\alpha/2}(n-1))
$$
因此
$$
-t_{\frac{\alpha}{2}}(n-1)&amp;lt;\frac{\sqrt n(\overline{X}-\mu)}{S}&amp;lt;t_{\frac{\alpha}{2}}(n-1)\quad \Rightarrow \quad 置信区间\left(\overline{X}-\frac{St_{\frac{\alpha}{2}}(n-1)}{\sqrt n},\overline{X}+\frac{St_{\frac{\alpha}{2}}(n-1)}{\sqrt n}\right)
$$
$T$ 分布的尾部比正态分布要重 (正态分布是指数衰减的，T 分布是多项式衰减的)，因此上述置信区间会比知道 $\sigma^2$ 时的置信区间要宽一些。考虑到我们是在知道更少的信息的情况下求出的该区间，这一点是容易理解的。&lt;/p&gt;
&lt;h2 id=&#34;confidence-interval-of-sigma2-in-normal-population&#34;&gt;Confidence Interval of $\sigma^2$ in Normal Population&lt;/h2&gt;
&lt;p&gt;【例】给定置信度 $1-\alpha$，样本 $X_1,\cdots, X_n$ 来自总体 $X\sim N(\mu,\sigma^2)$。考虑用枢轴变量法求方差 $\sigma^2$ 的置信区间：在 $\mu$ 未知的情况下，我们可以这样给出枢轴变量：$\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)$。从而
$$
1-\alpha=P\left(\chi^2_{1-\frac{\alpha}{2}}(n-1)&amp;lt;\frac{(n-1)S^2}{\sigma^2}&amp;lt;\chi^2_{\frac{\alpha}{2}}(n-1)\right)
$$
解得置信区间为
$$
\left(\frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}(n-1)},\frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}}(n-1)}\right)
$$
注：(1) 尽管 $\chi^2$ 分布和 $F$ 分布非对称，但通常仍然沿用 $\alpha/2$ 和 $1-\alpha/2$ 这两个分位点。这样求得的置信区间可能并非最短。&lt;/p&gt;
&lt;p&gt;(2) 若 $\mu$ 已知，我们可以将其运用起来精确化我们的置信区间：取 $U=\left(\frac{\overline{X}-\mu}{\sigma/\sqrt n}\right)^2$，则 $U\sim \chi^2(1)$。&lt;/p&gt;
&lt;h2 id=&#34;confidence-interval-of-mu_1-mu_2-in-normal-population&#34;&gt;Confidence Interval of $\mu_1-\mu_2$ in Normal Population&lt;/h2&gt;
&lt;p&gt;【例】给定置信度 $1-\alpha$，样本 $X_1,\cdots, X_{n_1}$ 来自总体 $X\sim N(\mu_1,\sigma_1^2)$，$Y_1,\cdots, Y_{n_2}$ 来自总体 $Y\sim N(\mu_2,\sigma_2^2)$，考虑它们的均值差 $\mu_1-\mu_2$ 的置信区间。如果 $\sigma_1^2$ 和 $\sigma_2^2$ 均已知，那么 $\overline{X}\sim N(\mu_1,\frac{\sigma_1^2}{n_1})$，$\overline{Y}\sim N(\mu_2,\frac{\sigma_2^2}{n_2})$，$\overline{X}-\overline{Y}\sim N(\mu_1-\mu_2,\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2})$。因此取
$$
U=\frac{\overline{X}-\overline{Y}-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)
$$
从而
$$
-u_{\frac{\alpha}{2}}&amp;lt;\frac{\overline{X}-\overline{Y}-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}&amp;lt;u_{\frac{\alpha}{2}}\Rightarrow置信区间\left(\overline{X}-\overline{Y}-\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}u_{\frac{\alpha}{2}},\overline{X}-\overline{Y}+\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}u_{\frac{\alpha}{2}}\right)
$$
若不知道 $\sigma_1^2$ 和 $\sigma_2^2$，但知道 $\sigma_1^2=\sigma_2^2=\sigma^2$，那么考虑如下公式：
$$
T=\sqrt{\frac{n_1n_2(n_1+n_2-2)}{n_1+n_2}}\frac{(\overline X-\overline Y)-(\mu_1-\mu_2)}{\sqrt{(n_1-1)S_X^2+(n_2-1)S_Y^2}}\sim t(n_1+n_2-2)
$$
于是有
$$
1-\alpha=P(-t_{\alpha/2}(n_1+n_2-2)&amp;lt;T&amp;lt;t_{\alpha/2}(n_1+n_2-2))
$$
令 $C_{n_1,n_2}=\sqrt{\frac{n_1n_2(n_1+n_2-2)}{n_1+n_2}}\frac{1}{\sqrt{(n_1-1)S_X^2+(n_2-1)S_Y^2}}$，则
$$
-t_{\frac{\alpha}{2}}(n_1+n_2-2)&amp;lt;C_{n_1,n_2}[(\overline X-\overline Y)-(\mu_1-\mu_2)]&amp;lt;t_{\frac{\alpha}{2}}(n_1+n_2-2)
$$
解得置信区间为
$$
\left(\overline{X}-\overline{Y}-\frac{t_{\frac{\alpha}{2}}(n_1+n_2-2)}{C_{n_1,n_2}},\overline{X}-\overline{Y}+\frac{t_{\frac{\alpha}{2}}(n_1+n_2-2)}{C_{n_1,n_2}}\right)
$$&lt;/p&gt;
&lt;h2 id=&#34;confidence-interval-of-fracsigma_12sigma_22-in-normal-population&#34;&gt;Confidence Interval of $\frac{\sigma_1^2}{\sigma_2^2}$ in Normal Population&lt;/h2&gt;
&lt;p&gt;【例】给定置信度 $1-\alpha$，样本 $X_1,\cdots, X_{n_1}$ 来自总体 $X\sim N(\mu_1,\sigma_1^2)$，$Y_1,\cdots, Y_{n_2}$ 来自总体 $Y\sim N(\mu_2,\sigma_2^2)$，考虑它们的方差比 $\frac{\sigma_1^2}{\sigma_2^2}$ 的置信区间。若 $\mu_1,\mu_2$ 未知，我们考虑枢轴变量
$$
F=\frac{S_X^2\sigma_2^2}{S_Y^2\sigma_1^2}\sim F(n_1-1,n_2-2)
$$
从而
$$
1-\alpha=P(F_{1-\frac{\alpha}{2}}(n_1-1,n_2-1)&amp;lt;F&amp;lt;F_{\frac{\alpha}{2}}(n_1-1,n_2-1))
$$
解得置信区间为
$$
\left(\frac{S_X^2}{S_Y^2}\frac{1}{F_\frac{\alpha}{2}(n_1-1,n_2-1)},\frac{S_X^2}{S_Y^2}\frac{1}{F_{1-\frac{\alpha}{2}}(n_1-1,n_2-1)}\right)
$$
如果我们知道更多信息，则可以给出更好的估计，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若 $\mu_1$ 已知，$\mu_2$ 未知，那么 $\frac{\overline X-\mu_1}{\sigma_1/\sqrt{n_1}}\sim N(0,1)$，$\frac{(n_2-1)S_Y^2}{\sigma_2^2}\sim \chi^2(n_2-1)$，则
$$
U=\frac{\left(\frac{\overline X-\mu_1}{\sigma_1/\sqrt{n_1}}\right)^2}{S_Y^2/\sigma_2^2}\sim F(1,n_2-1)
$$
(注：分子正态分布的平方，即一个自由度的 $\chi^2$ 分布。)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $\mu_1,\mu_2$ 均已知，则
$$
U=\frac{\left(\frac{\overline X-\mu_1}{\sigma_1/\sqrt{n_1}}\right)^2}{\left(\frac{\overline Y-\mu_2}{\sigma_2/\sqrt{n_2}}\right)^2}\sim F(1,1)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interval-estimation-of-unknown-population&#34;&gt;Interval Estimation of Unknown Population&lt;/h2&gt;
&lt;p&gt;非正态总体均值的区间估计通常采用所为的大样本法：&lt;/p&gt;
&lt;p&gt;设 $X_1,\cdots, X_n$ 来自总体 $X$，且 $E[X]=\mu,D(X)=\sigma$，考虑计算 $\mu$ 的置信度为 $1-\alpha$ 的置信区间。由中心极限定理，有
$$
\frac{\sum_{k=1}^nX_k-n\mu}{\sqrt n\sigma}\to N(0,1)
$$
从而考虑 $U=\frac{\overline{X}-\mu}{\sigma/\sqrt n}\approx N(0,1)$，于是 $1-\alpha\approx P(-u_{\frac{\alpha}{2}}&amp;lt;U&amp;lt;u_{\frac{\alpha}{2}})$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当 $\sigma^2$ 已知时，置信区间为 $(\overline X-\frac{u_{\frac{\alpha}{2}}\sigma}{\sqrt n},\overline X+\frac{u_{\frac{\alpha}{2}}\sigma}{\sqrt n})$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当 $\sigma^2$ 未知时，用 $S$ 来代替 $\sigma$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【例题】设总体服从 0-1 分布，$X_1,\cdots, X_n$ 为样本，求总体期望 $p$ 的置信度为 $1-\alpha$ 的置信区间。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解法一：该题设符合方差未知的情形，所以使用第二个结论，所求区间为
$$
\left(\overline X-\frac{u_{\frac{\alpha}{2}}S}{\sqrt n},\overline X+\frac{u_{\frac{\alpha}{2}}S}{\sqrt n}\right)
$$
这里的 $S$ 是一个二阶的统计量，利用 0-1 分布的性质，我们可以进一步化简它：
$$
\begin{align}
S^2&amp;amp;=\frac{1}{n-1}\sum_{k=1}^n(X_k-\overline{X})^2=\frac{1}{n-1}\left(\sum_{k=1}^nX_k^2-n\overline{X}^2\right)\\
&amp;amp;\overset{\text{0-1分布}}{=}\frac{1}{n-1}\left(\sum_{k=1}^nX_k-n\overline{X}^2\right)\\
&amp;amp;=\frac{n}{n-1}\overline{X}(1-\overline{X})
\end{align}
$$
代入原式得置信区间：
$$
\left(\overline{X}-u_{\frac{\alpha}{2}}\sqrt{\frac{\overline{X}(1-\overline X)}{n-1}},\overline{X}+u_{\frac{\alpha}{2}}\sqrt{\frac{\overline{X}(1-\overline X)}{n-1}}\right)
$$
这样置信区间中只有一阶的统计量，更加准确。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;解法二：因为 $\sum_{k=1}^nX_k\sim B(n,p)$，伯努利试验中方差 $\sigma=\sqrt{p(1-p)}$。根据中心极限定理，
$$
\frac{\sum_{k=1}^nX_k-np}{\sqrt{n}\sqrt{p(1-p)}}\to N(0,1)
$$
因此可以得到近似的置信区间
$$
\left(\overline{X}-u_{\frac{\alpha}{2}}\sqrt{\frac{p(1-p)}{n}},\overline{X}+u_{\frac{\alpha}{2}}\sqrt{\frac{p(1-p)}{n}}\right)
$$
$\overline{X}$ 是对 $p$ 的一个较好的估计，因此用 $\overline{X}$ 代替 $p$，即得到
$$
\left(\overline{X}-u_{\frac{\alpha}{2}}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}},\overline{X}+u_{\frac{\alpha}{2}}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\right)
$$
该做法更好地利用了 0-1 分布 (伯努利试验)，因此给出的区间更加精确一些。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 22: Hypothesis Test</title>
      <link>/posts/coursenotes/nju-probability/lec22/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/lec22/</guid>
      <description>&lt;p&gt;设总体 $X\sim N(\mu,\sigma^2)$，欲判断 $\mu$ 是否为某个给定的常数 $\mu_0$，我们将 $\mu=\mu_0$ ，记为 $H_0:\mu=\mu_0$，称为原假设或零假设。对应的，$H_1:\mu\neq \mu_0$ 称为备择假设或对立假设。
$$
H_0:\mu=\mu_0\qquad H_1:\mu\neq \mu_0
$$
我们也可以考虑判断 $\mu\leq \mu_0$ 是否成立，这时的原假设和对立假设为
$$
H_0:\mu\leq \mu_0\qquad H_1:\mu&amp;gt;\mu_0
$$
第一种假设称为双边检验 (对立假设居于原假设两边)，第二种则是单边检验。上述两种假设只涉及已知总体的未知参数，称为参数假设检验 (像 $H_0:X服从正态分布$ 就是非参数假设检验)。&lt;/p&gt;
&lt;p&gt;以第一种假设为例介绍假设检验的一般方法：$\overline{X}$ 是 $\mu$ 的一个比较好的估计，因此 $|\overline{X}-\mu_0|$ 应该不太大。我们希望找到一个阈值 $k$，当 $|\overline{X}-\mu_0|&amp;lt; k$ 时，我们认为假设成立。问题的关键在于如何选取 $k$。在 $H_0$ 成立的情况下，$\overline{X}\sim N(\mu_0,\frac{\sigma^2}{n})$。考虑 $\sigma^2$ 已知的情形，则 $U=\frac{\overline{X}-\mu_0}{\sigma/\sqrt n}\sim N(0,1)$。$|\overline{X}-\mu_0|\geq k$ 意味着 $|U|\geq \frac{k}{\sigma/\sqrt n}$，故 $k$ 的选取应当使得 $P\left(|U|\geq \frac{k}{\sigma/\sqrt n}\right)$ 足够小。给定一个小概率 $\alpha$ (常用值：0.1, 0.05, 0.01)，令 $P(|U|\geq \frac{k}{\sigma/\sqrt n})=\alpha$，即可确定 $\frac{k}{\sigma/\sqrt n}=u_{\frac{\alpha}{2}}$。&lt;/p&gt;
&lt;p&gt;我们把 $U$ 称为检验统计量，$\alpha$ 称为显著性水平，$u_{\frac{\alpha}{2}}=\frac{k}{\sigma/\sqrt n}$ 称为临界值。$W={|U|\geq u_{\frac{\alpha}{2}}}$ 称为拒绝域，$\overline{W}$ 称为接受域。&lt;/p&gt;
&lt;p&gt;基本步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据问题提出 $H_0$ 和 $H_1$。&lt;/li&gt;
&lt;li&gt;构造一个合适的统计量，在 $H_0$ 成立的条件下求该统计量的分布。&lt;/li&gt;
&lt;li&gt;给出小概率 $\alpha$，确定临界值和拒绝域 $W$。&lt;/li&gt;
&lt;li&gt;由样本算出观察值，若其落入 $W$，则拒绝 $H_0$，否则接受 $H_0$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设检验的两类错误：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;弃真错误：$H_0$ 正确却拒绝了 $H_0$。$P(拒绝H_0|H_0为真)=P(U\in W|H_0为真)=\alpha$。&lt;/li&gt;
&lt;li&gt;存伪错误：$H_0$ 错误却接受了 $H_0$。$P(接受H_0|H_1为真)=P(U\notin W|H_1为真)=\beta$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常来说 $\beta$ 不容易求得。$\alpha$ 和 $\beta$ 不能同时减小，我们通常在控制 $\alpha$ 足够小的情况下尽可能减小 $\beta$ (Neyman-Pearson 原则)，即拒绝 $H_0$ 需要更充分的理由，$H_0$ 地位高于 $H_1$。&lt;/p&gt;
&lt;p&gt;【例题】设 $X\sim N(\mu,1)$，$X_1,\cdots, X_n$ 为样本。在给定显著性水平 $\alpha$ 的情况下求 $H_0:\mu=\mu_0$，$H_1:\mu=\mu_1$ 的第二类错误概率 $\beta$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解：在 $H_0$ 下，我们会构造 $U=\frac{\overline X-\mu_0}{1/\sqrt n}=\sqrt n(\overline X-\mu_0)\sim N(0,1)$。但现在 $H_1$ 成立，因此 $U$ 实际服从的分布为
$$
U=\sqrt n(\overline X-\mu_0)=\sqrt n(\overline X-\mu_1)+\sqrt n(\mu_1-\mu_0)\sim N(\sqrt n(\mu_1-\mu_0),1)
$$
根据 $\beta$ 的定义，
$$
\begin{align}
\beta&amp;amp;=P(U\notin W|H_1为真)=P(|U|&amp;lt;u_{\frac{\alpha}{2}}|\mu=\mu_1)\\
&amp;amp;=P(-u_{\frac{\alpha}{2}}-\sqrt n(\mu_1-\mu_0)&amp;lt;\sqrt n(\overline{X}-\mu_1)&amp;lt;u_{\frac{\alpha}{2}}-\sqrt n(\mu_1-\mu_0)|\mu=\mu_1)\\
&amp;amp;=\Phi(u_{\frac{\alpha}{2}}-\sqrt n(\mu_1-\mu_0))-\Phi(-u_{\frac{\alpha}{2}}-\sqrt n(\mu_1-\mu_0))
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;mean-value-test-of-normal-population&#34;&gt;Mean Value Test of Normal Population&lt;/h2&gt;
&lt;p&gt;总体 $X\sim N(\mu,\sigma^2)$，$X_1,\cdots,X_n$ 为样本，给定 $\alpha$。&lt;/p&gt;
&lt;h3 id=&#34;u-test&#34;&gt;$u$ Test&lt;/h3&gt;
&lt;p&gt;在 $\sigma^2$ 已知的情况下，$H_0:\mu=\mu_0,H_1:\mu\neq \mu_0$。考虑 $U=\frac{\overline{X}-\mu}{\sigma/\sqrt n}\sim N(0,1)$。容易得出临界值 $P(|U|&amp;gt;u_{\frac{\alpha}{2}})=\alpha$，拒绝域 $W=\left\{\left|\frac{\overline{X}-\mu_0}{\sigma/\sqrt n}\right|&amp;gt;u_{\frac{\alpha}{2}}\right\}$。&lt;/p&gt;
&lt;p&gt;如果做单边检验：$H_0:\mu=\mu_0,H_1:\mu&amp;gt;\mu_0$，则临界值为 $u_\alpha$，拒绝域：$W=\left\{\frac{\overline{X}-\mu_0}{\sigma/\sqrt n}&amp;gt;u_\alpha\right\}$。&lt;/p&gt;
&lt;p&gt;若假设为 $H_0:\mu\leq \mu_0,H_1:\mu&amp;gt;\mu_0$。我们的拒绝域 $W$ 应该满足 $P(U\in W|H_0)\leq \alpha$。我们发现 $\mu_0$ 是 $\mu$ 的上界，因此
$$
P\left(\left.\frac{\overline{X}-\mu_0}{\sigma/\sqrt n}&amp;gt;\lambda_\alpha\right|\mu\leq \mu_0\right)\leq P\left(\left.\frac{\overline{X}-\mu}{\sigma/\sqrt n}&amp;gt;\lambda_\alpha\right|\mu\leq \mu_0\right)
$$
令右式等于 $\alpha$，因为 $\frac{\overline{X}-\mu}{\sigma/\sqrt n}\sim N(0,1)$，所以解得 $\lambda_\alpha=u_\alpha$。拒绝域：$W=\left\{\frac{\overline{X}-\mu_0}{\sigma/\sqrt n}&amp;gt;u_\alpha\right\}$。&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;$t$ Test&lt;/h3&gt;
&lt;p&gt;在 $\sigma^2$ 未知的情况下，$H_0:\mu=\mu_0,H_1:\mu\neq \mu_0$，取
$$
T=\frac{\sqrt n(\overline{X}-\mu_0)}{S}\sim t(n-1)
$$
可解出拒绝域 $W=\left\{|T|&amp;gt;t_{\frac{\alpha}{2}}(n-1)\right\}$。&lt;/p&gt;
&lt;h2 id=&#34;test-of-the-difference-of-mean-values-of-two-normal-populations&#34;&gt;Test of the Difference of Mean Values of Two Normal Populations&lt;/h2&gt;
&lt;p&gt;总体 $X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)$。假设 $H_0:\mu_1=\mu_2,H_1:\mu_1\neq \mu_2$。考虑以下几种情况：&lt;/p&gt;
&lt;p&gt;(1) $\sigma_1^2,\sigma_2^2$ 已知，取
$$
U=\frac{\overline X-\overline Y-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\overset{在H_0下}{=}\frac{\overline X-\overline Y}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)
$$
因此拒绝域
$$
W=\left\{\left|\frac{\overline X-\overline Y}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\right|&amp;gt;u_{\frac{\alpha}{2}}\right\}
$$
(2) $\sigma_1^2=\sigma_2^2=\sigma^2$ 但未知，取
$$
T=\sqrt{\frac{n_1n_2(n_1+n_2-2)}{n_1+n_2}}\frac{(\overline X-\overline Y)-(\mu_1-\mu_2)}{\sqrt{(n_1-1)S_X^2+(n_2-1)S_Y^2}}\sim t(n_1+n_2-2)
$$
(类似地，在 $H_0$ 下，$\mu_1-\mu_2$ 可以消去)&lt;/p&gt;
&lt;p&gt;因此拒绝域
$$
W=\left\{|T|&amp;gt;t_{\frac{\alpha}{2}}(n_1+n_2-2)\right\}
$$&lt;/p&gt;
&lt;h3 id=&#34;pairwise-t-test&#34;&gt;Pairwise $t$ Test&lt;/h3&gt;
&lt;p&gt;另外一种情况是，$X,Y$ 分布未知且相关，但 $Z=X-Y\sim N(\mu,\sigma^2)$。我们想要检验这两个分布是否接近。假设 $H_0:\mu=0,H_1:\mu\neq 0$。那么取
$$
T=\frac{\sqrt n(\overline{Z}-\mu)}{S_Z}\overset{在H_0下}{=}\frac{\sqrt n\overline{Z}}{S_Z}\sim t(n-1)
$$
拒绝域
$$
W=\left\{|T|&amp;gt;t_{\frac{\alpha}{2}}(n-1)\right\}
$$&lt;/p&gt;
&lt;h2 id=&#34;variance-test-of-normal-population&#34;&gt;Variance Test of Normal Population&lt;/h2&gt;
&lt;p&gt;设 $X\sim N(\mu,\sigma^2)$，$X_1,\cdots, X_n$ 来自总体 $X$，$\alpha$ 为显著水平。若 $\mu$ 未知，假设 $H_0:\sigma^2=\sigma_0^2,H_1:\sigma^2\neq \sigma_0^2$，考虑变量
$$
\chi^2=\frac{(n-1)S^2}{\sigma_0^2}\sim \chi^2(n-1)
$$
因此拒绝域
$$
W={\chi^2&amp;lt;\chi_{1-\frac{\alpha}{2}}^2(n-1)}\cup{\chi^2&amp;gt;\chi^2_{\frac{\alpha}{2}}(n-1)}
$$
两正态总体方差比值的检验：设 $X\sim N(\mu,\sigma^2)$，$X_1,\cdots, X_n$ 来自总体 $X$，$\alpha$ 为显著水平。这次利用 F 分布
$$
F=\frac{S_1^2\sigma_2^2}{S_2^2\sigma_1^2}\sim F(n_1-1,n_2-2)
$$
因此拒绝域
$$
W={F&amp;lt;F_{1-\frac{\alpha}{2}}(n_1-1,n_2-1)}\cup {F&amp;gt;F_{\frac{\alpha}{2}}(n_1-1,n_2-1)}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Moment Generating Function</title>
      <link>/posts/coursenotes/nju-probability/mgf/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/posts/coursenotes/nju-probability/mgf/</guid>
      <description>&lt;p&gt;矩母函数是快速求解高阶矩的利器。&lt;/p&gt;
&lt;h2 id=&#34;definitions-and-properties&#34;&gt;Definitions and Properties&lt;/h2&gt;
&lt;p&gt;$\fbox{Definition 1}$ ($k$ 阶矩) 设 $X$ 是一个概率密度为 $f$ 的随机变量。若 $X$ 是离散的，则它的 &lt;strong&gt;$k$ 阶矩&lt;/strong&gt; (记作 $\mu_k$) 定义为
$$
\mu_k\triangleq \sum_{m=0}^{\infty}x_m^kf(x_m)
$$
若 $X$ 是连续的，则它的 $k$ 阶矩定义为
$$
\mu_k\triangleq \int_{-\infty}^{+\infty}x^kf(x)\text{d}x
$$
$\fbox{Definition 2}$ (矩母函数) 设 $X$ 是一个概率密度为 $f$ 的随机变量，则其&lt;strong&gt;矩母函数&lt;/strong&gt; $M_X(t)$ 定义为 $\mathbb E[e^{tX}]$。具体地说，若 $X$ 是离散的，则
$$
M_X(t)\triangleq \sum_{m=0}^\infty e^{tx_m}f(x_m)
$$
若 $X$ 是连续的，则
$$
M_X(t)\triangleq \int_{-\infty}^{+\infty}e^{tx}f(x)\text{d}x
$$
矩母函数有如下重要性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$$
M_X(t)=\sum_{k=0}^\infty\mu_k\frac{t^k}{k!}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：此处仅证明连续情形。考虑将矩母函数中的 $e^{tx}$ 泰勒展开，则有
$$
\begin{align}
M_X(t)&amp;amp;=\int_{-\infty}^{+\infty}e^{tx}f(x)\text{d}x\\
&amp;amp;=\int_{-\infty}^{+\infty}\left(\sum_{k=0}^\infty\frac{(tx)^k}{k!}\right)f(x)\text{d}x\\
&amp;amp;=\sum_{k=0}^\infty\frac{t^k}{k!}\int_{-\infty}^{+\infty}x^kf(x)\text{d}x\\
&amp;amp;=\sum_{k=0}^\infty\frac{t^k}{k!}\mu_k\qquad \blacksquare
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因此我们可以通过对矩母函数求导的方式来快速获得一个随机变量的各个 $k$ 阶矩：
$$
\mu_k=\left.\frac{\text{d}^kM_X(t)}{\text{d}x^k}\right|_{t=0}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设 $\alpha$ 和 $\beta$ 为常数，那么
$$
M_{\alpha X+\beta}(t)=e^{\beta t}M_X(\alpha t)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
M_{\alpha X+\beta}(t)=\mathbb E[e^{t(\alpha X+\beta)}]=e^{\beta t}\mathbb E[e^{\alpha t\cdot X}]=e^{\beta t}M_X(\alpha t)\qquad \blacksquare
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;设 $X_1$ 和 $X_2$ 为互相独立的两个随机变量，且 $|t|&amp;lt;\delta$ 时 $M_{X_1}(t)$ 和 $M_{X_2}(t)$ 均收敛，那么
$$
M_{X_1+X_2}(t)=M_{X_1}(t)\cdot M_{X_2}(t)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\begin{align}
M_{X_1+X_2}(t)=\mathbb E[e^{t(X_1+X_2)}]=\mathbb E[e^{tX_1}\cdot e^{tX_2}]=\mathbb E[e^{tX_1}]\cdot \mathbb E[e^{tX_2}]=M_{X_1}(t)\cdot M_{X_2}(t)\qquad \blacksquare
\end{align}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mgf-of-common-distribution&#34;&gt;MGF of Common Distribution&lt;/h2&gt;
&lt;h3 id=&#34;bernoulli-distribution&#34;&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;p&gt;设 $X\sim B(n,p)$，即 $P(X=k)=f(k)=\binom{n}{k}p^k(1-p)^{n-k}$，那么
$$
\begin{align}
M_X(t)&amp;amp;=\sum_{k=0}^\infty e^{tk}f(k)\\
&amp;amp;=\sum_{k=0}^n e^{tk}\binom{n}{k}p^k(1-p)^{n-k}\\
&amp;amp;=\sum_{k=0}^n\binom{n}{k}(e^tp)^k(1-p)^{n-k}\\
&amp;amp;=(e^tp+1-p)^n
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;设 $X\sim p(\lambda)$，即 $P(X=k)=f(k)=\frac{\lambda^ke^{-\lambda}}{k!}$，那么
$$
\begin{align}
M_X(t)&amp;amp;=\sum_{k=0}^\infty e^{tk}f(k)\\
&amp;amp;=\sum_{k=0}^\infty e^{tk}\cdot \frac{\lambda^ke^{-\lambda}}{k!}\\
&amp;amp;=e^{-\lambda}\sum_{k=0}^\infty\frac{(\lambda e^t)^k}{k!}\\
&amp;amp;=e^{-\lambda}\cdot e^{\lambda e^t}=e^{\lambda(e^t-1)}
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;我们首先考虑标准正态分布：设 $Y\sim N(0,1)$，那么 $Y$ 的密度函数 $f(y)=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}$，从而
$$
\begin{align}
M_Y(t)&amp;amp;=\mathbb E[e^{tY}]=\int_{-\infty}^{+\infty}e^{ty}f(y)\text{d}y\\
&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}+ty}\text{d}y\\
&amp;amp;=e^{\frac{1}{2}t^2}\cdot \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}(y-t)^2}\text{d}y
\end{align}
$$
注意到后半部分恰好是 $N(t,1)$ 的概率密度函数在 $\mathbb R$ 上的积分，因此后半部分为 1，从而 $M_Y(t)=e^{\frac{1}{2}t^2}$。&lt;/p&gt;
&lt;p&gt;对于一般的正态分布 $X\sim N(\mu,\sigma^2)$，注意到 $\frac{X-\mu}{\sigma}=Y$，即 $X=\sigma Y+\mu$，因此根据矩母函数的性质，
$$
M_X(t)=M_{\sigma Y+\mu}(t)=e^{\mu t}M_Y(\sigma t)=e^{\mu t+\frac{1}{2}\sigma^2t^2}
$$
正态分布的矩母函数恰好就是对数正态分布的 $k$ 阶矩，推导过程如下：假设 $X$ 服从对数正态分布，那么 $\ln X\sim N(\mu,\sigma)$，从而
$$
\mathbb E[X^k]=\mathbb E[(e^{\ln X})^k]=\mathbb E[e^{k\ln X}]=M_{\ln X}(k)=e^{\mu k+\frac{1}{2}\sigma^2k^2}
$$
注：对数正态分布没有矩母函数。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
